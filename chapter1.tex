\chapter{Introduction}
\label{chapter:intro}

\section{Automatic speech recognition}
\label{section:asr}
\Gls{asr} can be defined as \enquote{\textit{independent, computer-driven transcription of spoken language into readable text in real time}} \cite{stuckless1994developments, jelinek1997statistical}. Although this process is performed almost effortlessly by the human brain, it is extremely difficult to reverse~engineer\footnote{The observation that low-level sensorimotor skills require far more computational resources than high-level reasoning is known as the Moravec's paradox and has been formulated independently by several artificial intelligence researchers in the 1980s \cite{moravec1988mind}.} \Gls{lvcsr} falls into two categories: speech transcription and speech understanding. The former aims to find the exact orthographic transcription of analysed utterance, while the latter aims to find its meaning. The scope of this thesis is limited to speech transcription, as it allows to measure the performance of language models using well-established metrics.

\subsection{Bayesian framework}
\label{subsection:bayesian}
In general, the recogniser tries to determine the word sequence $\hat{W}=w_{1}, \ldots, w_{L}$ out of all possible hypotheses $W$ which is most likely to have generated the sequence of observed acoustic features $Y=y_{1}, \ldots, y_{T}$:
\begin{equation}
\label{equation:recogniser}
  \hat{W}=\argmax_{W}P(W|Y).
\end{equation}
The conditional probability can be rearranged using the Bayes rule:
\begin{equation}
  \label{equation:bayesian}
  \gls{probsequence}=\frac{P(Y|W)P(W)}{P(Y)}\propto{P(Y|W)P(W)}.
\end{equation}
$P(W)$ is estimated by the language model, while $P(Y|W)$ is evaluated using the acoustic model. Typical ASR systems use \glspl{hmm} to represent the sequential structure of speech signal and \glspl{gmm} to model the emission distribution of \glspl{hmm} \cite{baker1975dragon, bourlard1994connectionist}.

\subsection{Feature extraction}
\label{subsection:features}
Speech is a non-stationary process, so to represent it as a succession of discrete states, it is assumed that its properties are constant over a short period of time. Under this assumption, it is possible to extract statistically meaningful acoustic parameters from a sampled speech waveform. One of the most popular types of features are \glspl{mfcc}. The cepstrum is formally defined as the \gls{idft} of the log magnitude of the \gls{dft} of the signal: 

\begin{equation}
	\label{equation:cepstrum}
	c[n]=\sum_{n=0}^{N-1}\log{\left(\abs{\sum_{n=0}^{N-1}x[n]e^{\frac{-2\pi ikn}{N}}}\right)}e^{\frac{2\pi ikn}{N}}.
\end{equation}

In \gls{asr} applications, \gls{dct} is commonly used in place of \gls{idft}. A useful property of the cepstral analysis is that it enables to deconvolve the glottal source waveform of particular fundamental frequency \gls{fundamental} from the vocal tract filter and extract the vocal tract properties, which carry most information about the phone being produced. The algorithm for calculating \glspl{mfcc} consists of the following steps:
\begin{enumerate}
	\item Preemphasis and windowing
	\item \gls{dft}
	\item Conversion to log mel scale 
	\item \gls{dct}
	\item Calculating the delta, double delta, and energy coefficients
\end{enumerate}

First, the speech waveform is subjected to high-frequency preemphasis in order to compensate for lip radiation and the attenuation of high frequencies caused by the sampling process \cite{singh2012preprocessing}. Typically, the signal is passed through a high-pass \gls{fir} filter:
\begin{equation}
H(z)=1-\frac{a}{z},
\end{equation}
where $0.9 \leq a \leq 1.0$. The signal is then segmented into a sequence of frames using 20-30 milisecond windows with about 50\% overlap. The Hamming window is commonly used, as it allows to avoid discontinuities at the boundaries:
\begin{equation}
\label{equation:hamming}
  w[n]=
  \begin{cases}
    0.54-0.46\cos(\frac{2 \pi n}{L}) & 0 \leq n \leq L-1 \\
    0                               & \text{otherwise.}
  \end{cases}
\end{equation}

In the next step, the \gls{dft} is applied. The resulting spectrum carries the information the amounts of energy in $N$ discrete frequency bands:
\begin{equation}
  X[k]=\sum_{n=0}^{N-1}x[n]e^{\frac{-2\pi ikn}{N}}.
\end{equation}

The \gls{dft} bands are evenly-spaced, but many studies confirm that human hearing is not equally sensitive at all frequencies -- above about 500 Hz increasingly large intervals are judged by listeners to produce equal pitch increments \cite{stevens1937scale, fletcher1938loudness}. It was shown that simulating this property of the human brain during the feature extraction improves the ASR performance. The signal is therefore passed through a bank of 26 triangular filters spaced linearly below 1000 Hz and logarithmically above. This corresponds to a following mapping between the raw acoustic frequency $f$ and the mel frequency $m$ \cite{muda2010voice}:
\begin{equation}
  m=1127\ln(1+\frac{f}{700}).
\end{equation}
Another property of the human hearing is that it is less sensitive to variations in signal level at high amplitudes. To model that response, the logarithm of each of the mel spectrum values is taken.

Finally, the \gls{dct} of the log filterbank energies is calculated. Higher values on the cepstrum x-axis represent the glottal pulse, while lower values correspond to the vocal tract characteristic. Generally, the MFCCs are formed from the first 12 cepstral values.

In addition to the 12 cepstral coefficients for each frame, the energy within a frame and the variability between frames are also taken into account, because they provide useful cues for phone identity. For each of the 13 features (12 cepstral features and energy), the delta (velocity) and double delta (acceleration) features are calculated, resulting in a total of 39 \glspl{mfcc} \cite{jurafsky2000speech}.

\subsection{Acoustic model}
\label{subsection:acoustic}
To estimate the posterior probability $P(Y|W)$ in Equation \ref{equation:bayesian}, we use an acoustic model, which represents the relationship between feature vector sequences and some linguistic units. Most acoustic models are based on Hidden Markov Models, although other techniques, such as neural networks, segmental models, and conditional random fields, have also been applied succesfully \cite{yu2009hidden, yu2008maximum, mohamed2012acoustic}. Considering the number of words in a typical language, it is impractical to train a separate HMM for each word. For this reason, sub-word units are used, generally phoneme-sized. Each spoken word $w$ from sequence $W$ is decomposed into a sequence of $K_{w}$ basic sounds, called pronunciation:
\begin{equation}
  \gls{pronunciation}=q_{1}, \ldots, q_{K_{w}}.
\end{equation}
Since a word can have multiple pronunciation, we have to calculate the posterior probability as a sum over all possible pronunciations:
\begin{equation}
  \label{equation:pronunciations}
  P(Y|W) = \sum_{Q}P(Y|Q)P(Q|W),
\end{equation}
where $Q$ is the particular sequence of pronunciations:
\begin{equation}
  P(Q|W)=\prod_{i=1}^{L}P(q^{w_{i}}|w_{i}).
\end{equation}
In practice, the number of alternative pronunciations for each word $w_{i}$ is small, which makes the summation \ref{equation:pronunciations} feasible.

\input{img/hmm.tex}

To calculate $P(Y|W)$, we represent each base phone $q$ by an \gls{hmm} of the form shown in Figure \ref{figure:hmm}, described by the following elements:
\begin{itemize}
\item $N$ -- number of states,
\item $S=\{S_{1}, \ldots, S_{N}\}$ -- individual states,
\item $\theta_{t}$ -- state at time t,
\item $\pi=\{\pi_{i}\}$ -- initial state distribution:
  \begin{equation*}
    \pi_{i}=P(\theta_{1}=S_{i}) \qquad (1 \leqslant i \leqslant N),
  \end{equation*}
\item $A=\{a_{ij}\}$ -- transition matrix describing the probability of transition from state $S_{i}$ to state $S_{j}$:
  \begin{equation*}
    a_{ij}=P(\theta_{t+1}=S_{j}|\theta_{t}=S_{i}) \qquad (1 \leqslant i, j \leqslant N), 
  \end{equation*}
\item $B=\{b_{i}(y)\}$ -- observation distribution associated with state $S_{i}$, usually modeled by by mixture of $M$ Gaussians:
    \begin{gather}
      b_{i}(y)=\sum_{k=1}^{M}w_{ik}\mathcal{N}(y, \mu_{ik}, \Sigma_{ik}) \\
      (1 \leqslant i \leqslant N, \quad w_{ik} \geqslant 0, \quad\mathlarger{\sum_{k=1}^{M}}w_{ik}=1)\nonumber
    \end{gather}
where $\mu_{ik}$ and $\Sigma_{ik}$ are the mean vector and covariance matrix associated with state $S_{j}$ and mixture $k$, while $w_{ik}$ is the mixture weight \cite{juang1985mixture}.
\end{itemize}
Every time step, an \gls{hmm} makes a transition from its current state to one of its connected states, generating a feature vector according to the probability density function $b_{j}(y)$. Two important assumptions follow:
\begin{enumerate}
\item \textit{The first-order Markov assumption --} the probability of a state $\theta_{t}$ at time $t$ is only dependent on the previous state $\theta_{t-1}$:
  \begin{equation}
    P(\theta_{t}|\theta_{t-1}, \ldots, \theta_{1})=P(\theta_{t}|\theta_{t-1}).
  \end{equation}
\item \textit{The output independence assumption --} every observation is conditionally independent of all other observations: 
  \begin{equation}
    P(y_{t}|y_{t-1}, \ldots, y_{1})=P(y_{t}|y_{t-1}).
  \end{equation}
\end{enumerate}
These assumptions significantly simplify the calculation of posterior probability $P(Y|W)$ from Equation \ref{equation:pronunciations}. We can now define it in terms of acoustic model parameters \cite{lu2013subspace}:
\begin{equation}
  P(Y|Q)=\sum_{\Theta}P(\Theta, Y|Q),
\end{equation}
where $\Theta=\theta_{0}, \ldots, \theta_{T+1}$ is a state sequence and
\begin{equation}
  \label{equation:states}
  P(\Theta, Y|Q)=a_{\theta_{0}\theta_{1}}\prod_{t=1}^{T}b_{\theta_{t}}(y_{t})a_{\theta_{t}\theta{t+1}}.
\end{equation}
In the above equation, $\theta_{0}$ and $\theta_{T+1}$ are non-emitting states (entry and exit). The model parameters $\lambda=\{a_{ij}, b_{j}(y)\}$ can be estimated from a corpus of training data using the forward-backward algorithm \cite{baum1970maximization}. Generally, it iteratively updates the model parameters by maximizing the lower bound of the log-likelihood function $log(P(Y|W))$. For a detailed explanation of the algorithm, see \cite{rabiner1986introduction}.

The described technique has an important shortcoming -- decomposing each vocabulary word into a sequence of independent units (monophones) completely ignores the context-dependent variation that exist in real speech. A common solution is to use a unique model for every triphone, i.e. phoneme in left and right context. This approach results in a data sparsity problem -- for $N$ base phones, there are $N^{3}$ potential triphones, which can be avoided by mapping the complete set of logical triphones $L$ to a reduced set of physical models $P$ by clustering and tying together the parameters in each cluster. The clustering is usually done at state-level using phonetically driven decision trees \cite{gales2008application}.

\Glspl{hmm} remained a \textit{de facto} standard for acoustic modelling until the advent of \gls{dnn}, first successfully used for acoustic modelling in 2012, as a result of cooperation between Google, Microsoft, IBM, and the University of Toronto. According to Richard Rashid, former head of Microsoft Research, it was ``the most dramatic change in accuracy since 1979 \cite{markoff2012scientists}''. In \cite{hinton2012deep}, \glspl{dnn} with many hidden layers have been shown to outperform \glspl{gmm} on a variety of challenging speech recognition tasks, sometimes by a large margin. The unprecedented gains in error reduction were further confirmed in \cite{pan2012investigation}, where it is reported that \glspl{dnn} can consistently achieved about 25-30\% relative error reduction over the best discriminatively trained \glspl{gmm}.

\subsection{Language models}
\label{subsection:lm}
Language modelling lies at the core of many natural language processing tasks, such as speech recognition and synthesis, document classification, grammar and spelling checking, parsing, machine translation, and information retrieval. In general, the task of \glspl{lm} is to estimate the likelihood of a sequence of words. This information can then be used to reject invalid sentences or resolve ambiguities. In case of automatic speech recognition the language model guides and constrains the search among alternative word hypotheses \cite{glass2013automatic}.

In a Bayesian framework presented in equation \ref{equation:bayesian}, the language model estimates the a priori likelihood by assigning probability $P(W)$ to each word sequence $W=w_{1}, \ldots, w_{n}$ such that $\sum_{W}P(W)=1$. Since the search is usually performed unidirectionally, $P(W)$ can be formulated as a chain rule:
\begin{equation}
\label{equation:chain}
  P(W)=\prod^{n}_{i=1}P(w_{i}|h_{i}),
\end{equation}
where $\gls{wordhistory}=w_{1}, \ldots, w_{i-1}$ is the word history for $w_{i}$, often reduced to equivalence class \gls{equivalenceclass}:
\begin{equation}
	P(w_{i}|h_{i})\approx P(w_{i}|\Phi(h_{i}))=P(w_{i}|\varphi_{i}).
\end{equation}
Good equivalence classes maximise information about the next word $w_{i}$ given its history, but usually require a vast quantity of example sequences. The development of effective statistical language models is therefore limited by the availability of representative and machine readable text corpora \cite{rosenfeld2000two}.

\section{Polish Language}
\label{section:polish}
\subsection{Introduction}
Although \gls{asr} has been an active field of research for several decades, for a long time most of the effort has been focused on English.  Many important techniques were developed as a result of research programmes financed by the American Defense Advanced Research Projects Agency and involving companies and institutions like IBM, AT\&T Bell Labs, Institute for Defense Analysis, Princeton University, and CMU. In the nineties numerous ASR systems originally developed for English had been succesfully ported to other languages, such as French, German, Japanese, and Mandarin Chinese \cite{besacier2014automatic}. Also the recently introduced \gls{dnn} models were first trained and benchmarked on English corpora, and later used for other languages \cite{hinton2012deep}. Although this suggest that similar modelling assumptions can hold across languages, Polish and other inflected languages still pose a formidable challenge for speech technology researchers, due to their complex grammar, rich inflection, and a large set of phonetically similar prefixes and suffixes. This section describes the features of Polish in the context of \gls{asr} and explains where the main problems arise and how can they can be tackled.
\subsection{Slavic Languages}
Polish is an Indo-European language used by about 40 million speakers in Poland and around the world. It belongs to the Slavic group, containing about 20 languages and dialects spoken by over 400 million people in Central, Southern, and Eastern Europe, as well as in the Asian part of Russia \cite{karpov2012speech}. This family of languages is traditionally divided into three main branches:
\begin{itemize}
\item West Slavic -- Polish, Czech, and Slovak,
\item South Slavic -- Serbian, Croatian, Bulgarian, Slovene, and Macedonian,
\item East Slavic -- Russian, Ukrainian, and Bellarusian.
\end{itemize}
In the field of speech technology, research and development effort has been focused primarily on Czech \cite{nouza2010adapting, oparin2008morphological}, Polish \cite{zelasko2015linguistically, ziolko2011automatic}, Russian {\cite{karpov2012speech}}, and Slovak \cite{lojka2009finite}.
\subsection{Inflection}
\label{subsection:inflection}
Polish, like other Slavic languages, exhibits a large degree of inflection. This means that a lexical unit (lexeme) modifies its basic form (lemma) depending on grammatical, morphological, and contextual relations. Grammatical classes, e.g. nouns, adjectives, adverbs, have an associated set of grammatical categories, such as number, case, or aspect. Major grammatical categories and their values are:

\begin{itemize}
\item number -- singular, plural,
\item case -- nominative, genitive, dative, accusative, instrumental, locative, vocative,
\item gender -- human masculine, animate masculine, inanimate masculine, feminine, neuter,
\item person -- first, second, third,
\item degree -- positive, comparative, superlative,
\item aspect -- imperfective, perfective,
\item negation -- affirmative, negative.
\end{itemize}
For a given class, a grammatical category can be morphological (all lexemes from that class are subject to inflection with respect to that category), or lexical (for each lexeme belonging to that class, all forms of that lexeme have the same value of that category, although that value differs across lexemes). For example, a noun changes its orthographic and phonetic form with respect to number and case, but its gender does not change, as shown in Table \ref{table:declination}. Similarly, Table \ref{table:conjugation} shows that a verb in past tense is subject to conjugation with respect to person, gender, and number, but its aspect is a fixed property.

\begin{table}[h!]
  \begin{center}
    \caption{Inflection of the feminine noun \textit{mowa} (speech) with respect to case and number.}
    \label{table:declination}
    \begin{tabular*}{.6\linewidth}{@{\extracolsep{\fill}}llll}
      & singular & plural \\
      \midrule
      nominative & mowa & mowy \\
      genitive & mowy & mów \\
      dative & mowie & mowom \\
      accusative & mowę & mowy \\
      instrumental & mową & mowami \\
      locative & mowie & mowach \\
      vocative & mowo & mowy \\
    \end{tabular*}
  \end{center}
\end{table}

\begin{table}[h!]
  \caption{Conjugation of the verb \textit{rozpoznawać} (recognize) in past tense and perfective aspect, with respect to number (singular, plural), person (first, second, third), and gender (masculine, feminine, neuter)}
  \label{table:conjugation}
    \centering    \subcaption{singular}
    \begin{tabular*}{.8\linewidth}{@{\extracolsep{\fill}}llll}
      & first & second & third \\
      \midrule
      masculine & rozpoznałem & rozpoznałeś & rozpoznał \\
      feminine & rozpoznałam & rozpoznałaś & rozpoznała \\
      neuter & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & rozpoznało \\
    \end{tabular*}
    \bigskip  
    \centering
    \subcaption{plural}
    \begin{tabular*}{.8\linewidth}{@{\extracolsep{\fill}}llll}
      & first & second & third \\
      \midrule
      masculine & rozpoznaliśmy & rozpoznaliście & rozpoznali \\
      feminine & rozpoznałyśmy & rozpoznałyście & rozpoznały \\
      neuter & rozpoznałyśmy & rozpoznałyście & rozpoznały \\
    \end{tabular*}
\end{table}
Examples presented in Tables \ref{table:declination} and \ref{table:conjugation} already hint at the most important problem in \gls{asr} of inflected language: the size of the lexicon. While an English noun usually has just two forms (singular and plural), its Polish equivalent can have fourteen distinct forms. In case of verbs or adjectives, the difference is even more pronounced (see Table \ref{table:verb}). New words can be created not only by changing the endings, but also by adding multiple prefixes and suffixes, as well as modifying the stem itself. Even negative, comparative, or superlative forms of an adjective are distinct lexical items (in English, they would be created with auxilliary words, such as \textit{not}, \textit{more}, \textit{the most}). This is a serious challenge in building any \gls{asr} system, as the number of distinct word-forms can easilly exceed one million. In case of English, 135 vocabulary items are needed to account for half of the Brown Corpus \cite{fagan2011introduction}, and an inventory of 50000 words is enough to achieve 99\% coverage \footnote{In \cite{michel2011quantitative} the size of the English lexicon in 2000 has been statistically estimated as $1022000$, but the distribution of words in a natural language is governed by the Zipf's law, stating that the frequency of any word is inversely proportional to its rank in the frequency table \cite{li1992random}.}. In case of inflected language, the wordbuilding mechanisms allow for producing virtually unlimited number of words, while even a vocabulary of one million items is too large to be processed in real time \cite{nouza2010challenges}. The most straightforward strategy of dealing with that problem is to limit the lexicon to words that occured at least $N$ times in the training corpus. There is obviously a tradeoff between the size of the model and recognition accuracy.

\subsection{Word order}
From the \gls{asr} perspective, the most problematic grammatical feature of Polish and other Slavic language, is the relatively free word order in sentences.Thanks to rich morphology, the subject, verb, and adjectives may appear at almost arbitrary places. The role of a certain word in the sentence can be inferred from its inflection, as there is strong grammatical agreement between the parts of a sentence. The subject of a sentence must agree in gender, person, number, and case with all its modifiers and the verb \footnote{Although verbs are not subject to conjugation with respect to cases, a verb constrains the case of the noun, forming a predicate expression called verbonominal collocation\cite{vetulani2007towards}.}. Therefore, the agreement relation often does not fit the left-to-right direction of language processing and frequently binds words which are not immediate neighbours in the sentence. Because of this, n-gram models for Slavic languages will never be as efficient as in case of English, which imposes strict constraints on the relative order of words in a sentence.

The mechanisms of grammatical agreement are too complex to be formalised into a set of machine-processable rules. A more common approach is using class-based n-gram models instead of word-based models. First, a list of classes is defined, usually based on grammatical classes and categories. Each word from the training corpus is then replaced by the coresponding class, and an n-gram model is trained. The number of classes is significantly lower than the size of the lexicon, so these kind of models are less prone to data sparsity problems and usually do not require smoothing (see Section \ref{subsection:smoothing}).

\subsection{Phonology}
The vowel system consists of six oral monophthongs and two nasal diphthongs, presented in Table \ref{table:vowel}. There are several inconsistencies that make the orthographic to phonemic transcription challenging. Note for example that the nasal vowels, denoted by letters <ą> and <ę>, are pronounced either as a mid-vowel followed by a nasalised labio-velar or palatal glide, or as a combination of an oral vowel and a nasal consonant. Moreover, letter <i> can denote a vowel /i/, as in \textit{igła} /\textipa{igwa}/ (needle), have a purely orthographic function of marking the palatalization of preceding consonant when followed by a vowel, as in \textit{cieplo} /\textipa{\t{tC}EpwO}/(heat), or do both at the same time, for example in \textit{cichy} /\textipa{\t{tC}ix1}/ (silent). It can also denote a consonant /j/, as in \textit{hiena} /\textipa{xjEna}/, or a combination of /j/ and /i/, for example in \textit{naiwny} /\textipa{najivn1}/ (naïve). In contrast, the vowel /u/ can be denoted by two different letters: <ó> and <u>. Although there is no difference in articulation, they cannot be used interchangeably. It is a relic of the vowel length system, which is still present in Czech and Slovak, but has disappeared from Polish.

the polish consonant system is far more complex, at least from the \gls{asr} perspective, mostly due to the distinction between the "rustling" laminal retroflex sounds~(sz,~ż,~cz,~dz), the corresponding "humming" alveolo-palatals~(ś,~ź,~ć,~dź), and "hissing" alveolars~(s,~z,~c,~dz). table \ref{table:consonants} presents polish consonants -- their orthographic transcription, corresponding \gls{ipa} symbols, examples, and english approximations \cite{gussmann2007phonology}. 

\section{Motivation and outline}
\label{section:outline}
This thesis presents a comparative analysis of morphosyntactic and semantic language models in the context of automatic speech recognition of Polish. It focuses mostly on $n$-grams, skip-grams, and neural probabilistic models. The goal of the experimental part is to find a model that is optimal in terms of \gls{werr}, size, and speed. The analysis of existing literature on the subject shows that language modelling of Polish is still in its incipient stages when compared to English. Most of the available works focus solely on different variations of the $n$-gram model \cite{majewski2008syllable, ziolko2011n}. Until recently, there were no studies on using neural networks for language modelling of Polish \cite{gajecki2013modelowanie, brocki2012connectionist}. A comparative review of existing approaches would allow to select the most promising directions of research and in the long-term perspective contribute to the development of efficient \gls{lvcsr} systems for Polish. Moreover, obtained results could be directly translatable to other Slavic languages.

The introduction has outlined the process of automatic speech recognition and the role of acoustic and language models in a Bayesian framework. Section \ref{section:polish} discusses some features of Polish and the challenges they pose in speech technology, in particular the rich morphology resulting in large lexicons and the relatively free word order, hindering the performance of standard $n$-gram models. 
Chapter \ref{chapter:lm} is a comprehensive overview of language models used in the experimental part: word $n$-grams, class $n$-grams, skip grams, and neural network models. Section \ref{section:evaluation} presents the evaluation metrics: perplexity, \gls{werr}, and average rating based on the Shannon visualisation method.
Chapter \ref{chapter:tools} describes the tools and resources used in the experiments with special emphasis on the National Corpus of Polish (\textit{Narodowy Korpus Języka Polskiego}, NKJP), described in Section \ref{section:nkjp}, the most comprehensive annotated collection of Polish texts, used to train all the models in the experimental part. Other tools include a morphosyntactic tagger and two language modelling frameworks.
Chapter \ref{chapter:results} presents the results of the experiments -- a comparison of all the models with respect to performance metrics described in Section \ref{section:evaluation}.
Chapter \ref{chapter:conclusion} contains a short summary of the results and discusses the implications of the conducted experiments, while at the same time pointing at some limitations of the methodology. It also proposes new directions for studies on language modeling of Polish and other similar languages.
