\chapter{Introduction}
\label{chapter:intro}

\section{Automatic speech recognition}
\label{section:asr}
\Gls{asr} can be defined as independent, computer-driven transcription of spoken language into readable text in real time \cite{stuckless1994developments, jelinek1997statistical}. Although this process can be performed almost effortlessly by the human brain, it is extremely difficult to reverse-engineer\footnote{The observation that low-level sensorimotor skills require far more computational resources than high-level reasoning is known as the Moravec's paradox and has been formulated independently by several artificial intelligence researchers in the 1980s. As Moravec wrote: ``it is comparatively easy to make computers exhibit adult level performance on intelligence tests or playing checkers, and difficult or impossible to give them the skills of a one-year-old when it comes to perception and mobility''\cite{moravec1988mind}.}. \Gls{lvcsr} falls into two distinct categories: speech transcription and speech understanding. The former aims to find the exact orthographic transcription of analysed utterance, while the latter aims to find its meaning. In this thesis we focus on speech transcription, because its performance can be reliably measured in terms of word recognition errors.

\subsection{Bayesian framework}
In general, the recogniser tries to determine the word sequence $\hat{W}=w_{1}, \ldots, w_{L}$ out of all possible hypotheses $W$ which is most likely to have generated the sequence of observed acoustic features $Y=y_{1}, \ldots, y_{T}$:
\begin{equation}
\label{equation:recogniser}
  \hat{W}=\max_{W}P(W|Y).
\end{equation}
We can rearrange the conditional probability using the Bayes rule:
\begin{equation}
  \label{equation:bayesian}
  \gls{probsequence}=\frac{P(Y|W)P(W)}{P(Y)}\propto{P(Y|W)P(W)}.
\end{equation}
$P(W)$ is estimated by the language model (see subsection \ref{subsection:lm}), while $P(Y|W)$ is computed using the acoustic model (see subsection \ref{subsection:acoustic}). Typical ASR systems use \glspl{hmm} to model the sequential structure of speech signal and \glspl{gmm} for modelling the emission distribution of \glspl{hmm} \cite{baker1975dragon, bourlard1994connectionist}.

\subsection{Feature extraction}
\label{subsection:features}
Speech is a non-stationary process, so to represent it as a succession of discrete states, it is assumed that its statistical properties are constant over a short period of time. Under this assumption it is possible to extract statistically meaningful acoustic parameters (feature vectors) from a sampled speech waveform. The most popular method of spectral analysis are \glspl{mfcc} and \gls{lpc}.
\begin{figure}[!ht]
  \centering
    \begin{tikzpicture}[block/.style = {minimum width = 4cm, minimum height = 1cm, font = \small}]
      \node [block] (preemphasis) {Preemphasis};nn
      \node [block, right = of preemphasis] (windowing) {Windowing};
      \node [block, right = of windowing] (dft) {DFT};
      \node [block, below = of dft] (melfilter) {Mel Filters};
      \node [block, below = of melfilter] (log) {Logarithm};
      \node [block, left = of log] (dct) {DCT};
      \node [block, left = of dct] (delta) {Delta Coefficients};
      \path[draw,->]
      (preemphasis) edge (windowing)
      (windowing) edge (dft)
      (dft) edge (melfilter)
      (melfilter) edge (log)
      (log) edge (dct)
      (dct) edge (delta);
    \end{tikzpicture}
    \caption{Calculating \gls{mfcc}}
    \label{figure:mfcc}
\end{figure}

The algorithm for calculating \glspl{mfcc} is shown in figure \ref{figure:mfcc}. First, the speech waveform is subjected to high-frequency preemphasis in order to compensate for lip radiation and attenuation of high frequencies caused by the sampling process \cite{singh2012preprocessing}. Typically, the signal is passed through a high-pass \gls{fir} filter:
\begin{equation}
H(z)=1-\frac{a}{z},
\end{equation}
where $0.9 \leq a \leq 1.0$ (for a different approach see: \cite{nossair1995signal}). The signal is then divided into a sequence of frames using 20-30 milisecond windows with about 50\% overlap. The extraction takes place simply by multiplying the value of the signal by the value of the window for every time point $n$:
\begin{equation}
  y[n]=s[n]w[n].
\end{equation}
Hamming window is commonly used, as it allows to avoid discontinuities at the boundaries:
\begin{equation}
\label{equation:hamming}
  w[n]=
  \begin{cases}
    0.54-0.46\cos(\frac{2 \pi n}{L}) & 0 \leq n \leq L-1 \\
    0                               & \text{otherwise.}
  \end{cases}
\end{equation}

The next step is to extract spectral information from the windowed signal using \gls{dft}. The result of Fourier analysis is the information about the amounts of energy in $N$ evenly-spaced discrete frequency bands:
\begin{equation}
  X[k]=\sum_{n=0}^{N-1}x[n]e^{\frac{-2\pi ikn}{N}}.
\end{equation}
However, human hearing is not equally sensitive at all frequency bands. Many studies confirm that above about 500 Hz increasingly large intervals are judged by listeners to produce equal pitch increments \cite{stevens1937scale, fletcher1938loudness}. It was shown that simulating this property of human brain during feature extraction improves ASR performance. The signal is therefore passed through a bank of triangular filters spaced linearly below 1000 Hz and logarithmically above. This corresponds to the mapping between raw acoustic frequency $f$ and mel frequency $m$ \cite{muda2010voice}:
\begin{equation}
  m=1127\ln(1+\frac{f}{700}).
\end{equation}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=\textwidth]{img/mel.png}
  \caption{Plot of pitch mel scale versus Hertz scale \cite{vedala2008mel}.}
  \label{figure:mel}
\end{figure}

Another property of human hearing is that we are less sensitive to variation in signal level at high amplitudes than at low amplitudes. To model that logarithmic response, we take the log of each of the mel spectrum values.

Finally, cepstrum of the signal is calculated. Formally, it can be defined as the inverse \gls{dft} of the log magnitude of the \gls{dft} of the signal, although \gls{dct} is also frequently used instead of inverse \gls{dft}:
\begin{equation}
  \label{equation:cepstrum}
  c[n]=\sum_{n=0}^{N-1}\log{\left(\abs{\sum_{n=0}^{N-1}x[n]e^{\frac{-2\pi ikn}{N}}}\right)}e^{\frac{2\pi ikn}{N}}.
\end{equation}

In general, speech can be represented with the source-filter model. The original glottal source waveform of particular fundamental frequency \gls{fundamental} is passed trough the vocal tract, which acts as a filter. However, glottal source features are irrelevant for distinguishing phones. Cepstral analysis enables to deconvolve the source from the filter and extract the vocal tract properties, which carry most information about the phone being produced. Higher values on the cepstrum x-axis represent the glottal pulse, while lower values correspond to vocal tract characteristic. Generally, MFCCs are formed from the first 12 cepstral values, which carry information solely about the vocal tract  \cite{jurafsky2000speech}. Another useful property of the cepstral coefficients is that their variance tends to be uncorrelated, which is not true in case of raw mel-frequency spectrum. This means that the GMMs don't have to represent the covariance between MFCCs, which significantly reduces the number of parameters (see subsection \ref{subsection:acoustic}).

Having 12 cepstral coefficients for each frame is not sufficient, as we need inormation about one more feature that is useful in phone recognition: energy. The energy of a signal $x$ from time sample $t_{1}$ to time sample $t_{2}$ can be calculated as the sum over time of the power of the samples:
\begin{equation}
  \text{Energy}=\sum_{t=t_{1}}^{t_{2}}x^{2}[t].
\end{equation}

We also have to take into account the variability of the signal. Changes from frame to frame, such as the slope of a formant at its transitions, provide a useful cue for phone identity. For each of the 13 features (12 cepstral features and energy), we calculate the delta (velocity) and double delta (acceleration) features. This can be done simply by computing differences between cepstral values, although usually more sophisticated estimates are used:

\begin{equation}
  d(t)=\frac{c(t+1)-c(t-1)}{2}.
\end{equation}

After adding energy coefficients and time derivatives, we end up with a total of 39 \glspl{mfcc}:
\begin{itemize}
\itemsep0em
\item 12 cepstral coefficients,
\item 12 delta cepstral coefficients,
\item 12 double delta cepstral coefficients,
\item 1 energy coefficient,
\item 1 delta energy coefficient,
\item 1 double delta energy coefficient.
\end{itemize}

\subsection{Acoustic model}
\label{subsection:acoustic}
To estimate the posterior probability $P(Y|W)$ in Equation \ref{equation:bayesian}, we use an acoustic model, which represents the relationship between feature vector sequences and some linguistic units. Most acoustic models are based on Hidden Markov Models, although other techniques, such as neural networks, segmental models, and conditional random fields, have also been applied succesfully \cite{yu2009hidden, yu2008maximum, mohamed2012acoustic}. Considering the number of words in a typical language, it is impractical to train a separate HMM for each word. For this reason, sub-word units are used, generally phoneme-sized. Each spoken word $w$ from sequence $W$ is decomposed into a sequence of $K_{w}$ basic sounds, called pronunciation:
\begin{equation}
  \gls{pronunciation}=q_{1}, \ldots, q_{K_{w}}.
\end{equation}
Since a word can have multiple pronunciation, we have to calculate the posterior probability as a sum over all possible pronunciations:
\begin{equation}
  \label{equation:pronunciations}
  P(Y|W) = \sum_{Q}P(Y|Q)P(Q|W),
\end{equation}
where $Q$ is the particular sequence of pronunciations:
\begin{equation}
  P(Q|W)=\prod_{i=1}^{L}P(q^{w_{i}}|w_{i}).
\end{equation}
In practice, the number of alternative pronunciations for each word $w_{i}$ is small, which makes the summation \ref{equation:pronunciations} feasible.

\input{img/hmm.tex}

To calculate $P(Y|W)$, we represent each base phone $q$ by an \gls{hmm} of the form shown in Figure \ref{figure:hmm}, described by the following elements:
\begin{itemize}
\item $N$ -- number of states,
\item $S=\{S_{1}, \ldots, S_{N}\}$ -- individual states,
\item $\theta_{t}$ -- state at time t,
\item $\pi=\{\pi_{i}\}$ -- initial state distribution:
  \begin{equation*}
    \pi_{i}=P(\theta_{1}=S_{i}) \qquad (1 \leqslant i \leqslant N),
  \end{equation*}
\item $A=\{a_{ij}\}$ -- transition matrix describing the probability of transition from state $S_{i}$ to state $S_{j}$:
  \begin{equation*}
    a_{ij}=P(\theta_{t+1}=S_{j}|\theta_{t}=S_{i}) \qquad (1 \leqslant i, j \leqslant N), 
  \end{equation*}
\item $B=\{b_{i}(y)\}$ -- observation distribution associated with state $S_{i}$, usually modeled by by mixture of $M$ Gaussians:
    \begin{gather}
      b_{i}(y)=\sum_{k=1}^{M}w_{ik}\mathcal{N}(y, \mu_{ik}, \Sigma_{ik}) \\
      (1 \leqslant i \leqslant N, \quad w_{ik} \geqslant 0, \quad\mathlarger{\sum_{k=1}^{M}}w_{ik}=1)\nonumber
    \end{gather}
where $\mu_{ik}$ and $\Sigma_{ik}$ are the mean vector and covariance matrix associated with state $S_{j}$ and mixture $k$, while $w_{ik}$ is the mixture weight \cite{juang1985mixture}.
\end{itemize}
Every time step, an \gls{hmm} makes a transition from its current state to one of its connected states, generating a feature vector according to the probability density function $b_{j}(y)$. Two important assumptions follow:
\begin{enumerate}
\item \textit{The first-order Markov assumption --} the probability of a state $\theta_{t}$ at time $t$ is only dependent on the previous state $\theta_{t-1}$:
  \begin{equation}
    P(\theta_{t}|\theta_{t-1}, \ldots, \theta_{1})=P(\theta_{t}|\theta_{t-1}).
  \end{equation}
\item \textit{The output independence assumption --} every observation is conditionally independent of all other observations: 
  \begin{equation}
    P(y_{t}|y_{t-1}, \ldots, y_{1})=P(y_{t}|y_{t-1}).
  \end{equation}
\end{enumerate}
These assumptions significantly simplify the calculation of posterior probability $P(Y|W)$ from Equation \ref{equation:pronunciations}. We can now define it in terms of acoustic model parameters \cite{lu2013subspace}:
\begin{equation}
  P(Y|Q)=\sum_{\Theta}P(\Theta, Y|Q),
\end{equation}
where $\Theta=\theta_{0}, \ldots, \theta_{T+1}$ is a state sequence and
\begin{equation}
  \label{equation:states}
  P(\Theta, Y|Q)=a_{\theta_{0}\theta_{1}}\prod_{t=1}^{T}b_{\theta_{t}}(y_{t})a_{\theta_{t}\theta{t+1}}.
\end{equation}
In the above equation, $\theta_{0}$ and $\theta_{T+1}$ are non-emitting states (entry and exit). The model parameters $\lambda=\{a_{ij}, b_{j}(y)\}$ can be estimated from a corpus of training data using the forward-backward algorithm \cite{baum1970maximization}. Generally, it iteratively updates the model parameters by maximizing the lower bound of the log-likelihood function $log(P(Y|W))$. For a detailed explanation of the algorithm, see \cite{rabiner1986introduction}.

The described technique has an important shortcoming -- decomposing each vocabulary word into a sequence of independent units (monophones) completely ignores the context-dependent variation that exist in real speech. A common solution is to use a unique model for every triphone, i.e. phoneme in left and right context. This approach results in a data sparsity problem -- for $N$ base phones, there are $N^{3}$ potential triphones, which can be avoided by mapping the complete set of logical triphones $L$ to a reduced set of physical models $P$ by clustering and tying together the parameters in each cluster. The clustering is usually done at state-level using phonetically driven decision trees \cite{gales2008application}.

\Glspl{hmm} remained a \textit{de facto} standard for acoustic modelling until the advent of \gls{dnn}, first successfully used for acoustic modelling in 2012, as a result of cooperation between Google, Microsoft, IBM, and the University of Toronto. According to Richard Rashid, former head of Microsoft Research, it was ``the most dramatic change in accuracy since 1979 \cite{markoff2012scientists}''. In \cite{hinton2012deep}, \glspl{dnn} with many hidden layers have been shown to outperform \glspl{gmm} on a variety of challenging speech recognition tasks, sometimes by a large margin. The unprecedented gains in error reduction were further confirmed in \cite{pan2012investigation}, where it is reported that \glspl{dnn} can consistently achieved about 25-30\% relative error reduction over the best discriminatively trained \glspl{gmm}.

\subsection{Language models}
\label{subsection:lm}
 In a Bayesian framework presented in equation \ref{equation:bayesian}, the language model estimates the a priori likelihood by assigning probability $P(W)$ to each word sequence $W=\{w_{1}, \ldots, w_{n}\}$ such that $\sum_{W}P(W)=1$. Since the search is usually performed unidirectionally, $P(W)$ can be formulated as a chain rule:
\begin{equation}
  P(W)=\prod^{n}_{i=1}P(w_{i}|h_{i}),
\end{equation}
where $\gls{wordhistory}=\{w_{1}, \ldots, w_{i-1}\}$ is the word history for $w_{i}$, often reduced to equivalence class \gls{equivalenceclass}:
\begin{equation}
  P(w_{i}|h_{i})\approx P(w_{i}|\phi(h_{i})).
\end{equation}
Good equivalence classes maximise information about the next word $w_{i}$ given its history, but also require a vast quantity of example sequences. The development of effective statistical language models is therefore limited by the availability of representative and machine readable text corpora \cite{rosenfeld2000two}.

\section{Polish Language}
\label{section:polish}
\subsection{Introduction}
Although \gls{asr} has been an active field of research for several decades, for a long time most of the effort has been focused on English.  Many important techniques were developed as a result of research programmes financed by the American \gls{darpa} and involving companies and institutions like IBM, AT\&T Bell Labs, Institute for Defense Analysis, Princeton University, and CMU. In the nineties numerous ASR systems originally developed for English had been succesfully ported to other languages, such as French, German, Japanese, and Mandarin Chinese \cite{besacier2014automatic}. Also the recently introduced \gls{dnn} models were first trained and benchmarked on English corpora, and later used for other languages \cite{hinton2012deep}. Although this suggest that similar modelling assumptions can hold across languages, Polish and other inflected languages still pose a formidable challenge for speech technology researchers, due to their complex grammar, rich inflection, and a large set of phonetically similar prefixes and suffixes. This section describes the features of Polish in the context of \gls{asr} and explains where the main problems arise and how can they can be tackled.
\subsection{Slavic Languages}
Polish is an Indo-European language used by about 40 million speakers in Poland and around the world. It belongs to the Slavic group, containing about 20 languages and dialects spoken by over 400 million people in Central, Southern, and Eastern Europe, as well as in the Asian part of Russia \cite{karpov2012speech}. This family of languages is traditionally divided into three main branches:
\begin{itemize}
\item West Slavic -- Polish, Czech, and Slovak,
\item South Slavic -- Serbian, Croatian, Bulgarian, Slovene, and Macedonian,
\item East Slavic -- Russian, Ukrainian, and Bellarusian.
\end{itemize}
In the field of speech technology, research and development effort has been focused primarily on Czech \cite{nouza2010adapting, oparin2008morphological}, Polish \cite{zelasko2015linguistically, ziolko2011automatic}, Russian {\cite{karpov2012speech}}, and Slovak \cite{lojka2009finite}.
\subsection{Inflection}
Polish, like other Slavic languages, exhibits a large degree of inflection. This means that a lexical unit (lexeme) modifies its basic form (lemma) depending on grammatical, morphological, and contextual relations. Grammatical classes, e.g. nouns, adjectives, adverbs, have an associated set of grammatical categories, such as number, case, or aspect. Major grammatical categories and their values are:

\begin{itemize}
\item number -- singular, plural,
\item case -- nominative, genitive, dative, accusative, instrumental, locative, vocative,
\item gender -- human masculine, animate masculine, inanimate masculine, feminine, neuter,
\item person -- first, second, third,
\item degree -- positive, comparative, superlative,
\item aspect -- imperfective, perfective,
\item negation -- affirmative, negative.
\end{itemize}
For a given class, a grammatical category can be morphological (all lexemes from that class are subject to inflection with respect to that category), or lexical (for each lexeme belonging to that class, all forms of that lexeme have the same value of that category, although that value differs across lexemes). For example, a noun changes its orthographic and phonetic form with respect to number and case, but its gender does not change, as shown in Table \ref{table:declination}. Similarly, Table \ref{table:conjugation} shows that a verb in past tense is subject to conjugation with respect to person, gender, and number, but its aspect is a fixed property.

\begin{table}[h!]
  \begin{center}
    \caption{Inflection of the feminine noun \textit{mowa} (speech) with respect to case and number.}
    \label{table:declination}
    \begin{tabular*}{.6\linewidth}{@{\extracolsep{\fill}}llll}
      & singular & plural \\
      \midrule
      nominative & mowa & mowy \\
      genitive & mowy & mów \\
      dative & mowie & mowom \\
      accusative & mowę & mowy \\
      instrumental & mową & mowami \\
      locative & mowie & mowach \\
      vocative & mowo & mowy \\
    \end{tabular*}
  \end{center}
\end{table}

\begin{table}[h!]
  \caption{Conjugation of the verb \textit{rozpoznawać} (recognize) in past tense and perfective aspect, with respect to number (singular, plural), person (first, second, third), and gender (masculine, feminine, neuter)}
  \label{table:conjugation}
    \centering    \subcaption{singular}
    \begin{tabular*}{.8\linewidth}{@{\extracolsep{\fill}}llll}
      & first & second & third \\
      \midrule
      masculine & rozpoznałem & rozpoznałeś & rozpoznał \\
      feminine & rozpoznałam & rozpoznałaś & rozpoznała \\
      neuter & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & rozpoznało \\
    \end{tabular*}
    \bigskip  
    \centering
    \subcaption{plural}
    \begin{tabular*}{.8\linewidth}{@{\extracolsep{\fill}}llll}
      & first & second & third \\
      \midrule
      masculine & rozpoznaliśmy & rozpoznaliście & rozpoznali \\
      feminine & rozpoznałyśmy & rozpoznałyście & rozpoznały \\
      neuter & rozpoznałyśmy & rozpoznałyście & rozpoznały \\
    \end{tabular*}
\end{table}
Examples presented in Tables \ref{table:declination} and \ref{table:conjugation} already hint at the most important problem in \gls{asr} of inflected language: the size of the lexicon. While an English noun usually has just two forms (singular and plural), its Polish equivalent can have more than twenty distinct forms (see Table \ref{table:noun}). In case of verbs or adjectives, the difference is even more pronounced (see Table \ref{table:verb}). New words can be created not only by changing the endings, but also by adding multiple prefixes and suffixes, as well as modifying the stem itself. Even negative, comparative, or superlative forms of an adjective are distinct lexical items (in English, they would be created with auxilliary words, such as \textit{not}, \textit{more}, \textit{the most}). This is a serious challenge in building any \gls{asr} system, as the number of distinct word-forms can easilly exceed one million. In case of English, 135 vocabulary items are needed to account for half of the Brown Corpus \cite{fagan2011introduction}, and an inventory of 50000 words is enough to achieve 99\% coverage \footnote{In \cite{michel2011quantitative} the size of the English lexicon in 2000 has been statistically estimated as $1022000$, but the distribution of words in a natural language is governed by the Zipf's law, stating that the frequency of any word is inversely proportional to its rank in the frequency table \cite{li1992random}.}. In case of inflected language, the wordbuilding mechanisms allow for producing virtually unlimited number of words, while even a vocabulary of one million items is too large to be processed in real time \cite{nouza2010challenges}. There are several ways of dealing with the tradeoff between the lexicon size and recognition quality.

The most straighforward strategy of reducing the \gls{oov} rate is to limit the lexicon to a set of most frequent word forms. It can be done by simply creating the word lexicon from words that occured at least $N$ times in the training corpus. The size of the lexicon can be controlled by the value of $N$.
\begin{table}[!ht]
  \caption{The noun \textit{student} (student) in English and its inflected forms in Polish}
    \label{table:noun}
    \centering
    \begin{tabularx}{.8\linewidth}{@{}p{.2\textwidth}Xp{.6\textwidth}X@{}}
      English & Polish\\
      \hline
      \raggedright student, students & \textbf{masculine}: student, studenta, studentowi, studentem, studencie, studenci, studentów, studentom, studentami, studentach\\
                        & \textbf{feminine}: studentka, studentki, studentce, studentkę, studentką, studentko, studentki, studentek, studentkom, studentkami, studentkach\\
    \end{tabularx}
\end{table}

\begin{table}[!ht]
  \caption{The verb \textit{być} (to be) in English and its inflected forms in Polish}
    \label{table:verb}
    \centering
    \begin{tabularx}{.8\linewidth}{@{}p{.2\textwidth}Xp{.6\textwidth}X@{}}
      English & Polish\\
      \hline
      \raggedright be, being, been, am, are, is, was, were & jestem, jesteś, jest, jesteśmy, jesteście, są, byłem, byłam, byłeś, byłaś, był, była, było, byliśmy, byłyśmy, byliście, byłyście, byli, były, będę, będziesz, będzie, będziemy, będziecie, będą, byłbym, byłabym, byłbyś, byłabyś, byłby, byłaby, byłoby, bylibyśmy, byłybyśmy, bylibyście, byłybyście, byliby, byłyby, bądź, bądźmy, bądźcie, będący, będąc\\
    \end{tabularx}
\end{table}

\FloatBarrier
\subsection{Word order}
From the \gls{asr} perspective, the most problematic grammatical feature of Polish and other Slavic language, is the relatively free word order in sentences.Thanks to rich morphology, the subject, verb, and adjectives may appear at almost arbitrary places. The role of a certain word in the sentence can be inferred from its inflection, as there is strong grammatical agreement between the parts of a sentence. The subject of a sentence must agree in gender, person, number, and case with all its modifiers and the verb \footnote{Although verbs are not subject to conjugation with respect to cases, a verb constrains the case of the subject, forming a predicate expression called verbonominal collocation\cite{vetulani2007towards}. See Table \ref{table:agreement}.}. Therefore, the agreement relation often does not fit the left-to-right direction of language processing and frequently binds words which are not immediate neighbours in the sentence. Because of this, n-gram models for Slavic languages will never be as efficient as in case of English, which imposes strict constraints on the relative order of words in a sentence.

\begin{table}[!ht]
  \caption{An example of grammatical agreement}
    \label{table:agreement}
    \centering
    \begin{tabularx}{.8\linewidth}{@{}p{.4\textwidth}Xp{.4\textwidth}X@{}}
      English & Polish\\
      \hline
      I see an intelligent woman & Widzę inteligentn\textbf{ą} kobiet\textbf{ę}\\
      I don't see an intelligent woman & Nie widzę inteligentn\textbf{ej} kobiet\textbf{y}\\
    \end{tabularx}
\end{table}

The mechanisms of grammatical agreement are too complex to be formalised into a set of machine-processable rules. A more common approach is using class-based n-gram models instead of word-based models. First, a list of classes is defined, usually based on grammatical classes and categories. Each word from the training corpus is then replaced by the coresponding class, and an n-gram model is trained. The number of classes is significantly lower than the size of the lexicon, so these kind of models are less prone to data sparsity problems and usually do not require smoothing (see Section \ref{subsection:class}).

\subsection{Phonology}
The vowel system consists of six oral monophthongs and two nasal diphthongs, presented in Table \ref{table:vowel} (see Appendix \ref{appendix:articulation} for details). Several commenta are in place here:
\begin{itemize}
\item The nasal vowels, denoted by letters <ą> and <ę>, are pronounced either as a mid-vowel followed by a nasalised labio-velar or palatal glide (/\textipa{E\super{\~ w}}/, /\textipa{E \super{\~ j}}/, /\textipa{O \super{\~ w}}/), or as a combination of an oral vowel and a nasal consonant.
\item Letter <i> can denote a vowel /i/, as in \textit{igła} /\textipa{igwa}/ (needle), have a purely orthographic function of marking the palatalization of preceding consonant when followed by a vowel, as in \textit{cieplo} /\textipa{\t{tC}EpwO}/(heat), or do both at the same time, for example in \textit{cichy} /\textipa{\t{tC}ix1}/ (silent). It can also denote a consonant /j/, as in \textit{hiena} /\textipa{xjEna}/, or a combination of /j/ and /i/, for example in \textit{naiwny} /\textipa{najivn1}/ (naïve).
\item The vowel /u/ has two spellings: <ó> and <u>. Although there is no difference in articulation, they cannot be used interchangeably. It is a relic of the vowel length system, which is still present in Czech and Slovak, but has disappeared from Polish. Letter <ó> used to denote prolonged /o/ and is still distinguished in script, despite the vowel shift from long /o/ to short /u/. Similarly, <á> and <é> used to denote prolonged /\textipa{a:}/ and /\textipa{E:}/, but became disused.
\end{itemize}

\begin{table}[h!]
  \begin{center}
    \caption{Polish vowels.}
    \label{table:vowel}
    \begin{tabular*}{.8\linewidth}{@{\extracolsep{\fill}}lll}
      Polish & IPA & example\\
      \midrule
      a & \textipa{a}  & \textit{atak} /\textipa{atak}/ (attack)\\
      e & \textipa{E}  & \textit{era} /\textipa{Era}/ (era)\\
      i & \textipa{i}  & \textit{igła} /\textipa{igwa}/ (needle)\\
      o & \textipa{O}  & \textit{oko} /\textipa{OkO}/ (eye)\\
      u/ó & \textipa{u} & \textit{ucho} /\textipa{uxO}/ (ear), \textit{ósmy} /\textipa{usm1}/ (eighth)\\
      y & \textipa{1}   & \textit{rym} /\textipa{r1m}/ (rhyme)\\
      ę & \textipa{E \super{\~w}} & gęsty /\textipa{gE\super{\~w}st1}/ (thick)\\
        &  \textipa{E \super{\~j}} & gęś /\textipa{gE\super{\~j}C}/ (goose)\\
        &  \textipa{En} & wędka /\textipa{vEntka}/ (rod)\\
        &  \textipa{E\textltailn} & sędzia /\textipa{sE\textltailn d\textctz a}/ (judge)\\
        &  \textipa{Em} & tępy /\textipa{tEmp1}/ (blunt)\\
      ą & \textipa{O \super{\~w}} & wąs /\textipa{wO\super{\~w}s}/ (moustache)\\
        &  \textipa{ON} & bąk /\textipa{bONk}/ (bumblebee)\\
        &  \textipa{O\textltailn} & wziąć /\textipa{v\textctz O\textltailn tC}/ (to take)\\
        &  \textipa{On} & wątroba /\textipa{vOntrOba}/ (liver)\\
        &  \textipa{Om} & kąpiel /\textipa{kOmp\super ijel}/ (bath)\\
    \end{tabular*}
  \end{center}
\end{table}

\FloatBarrier
The Polish consonant system is far more complex, at least from the \gls{asr} perspective, mostly due to the distinction between the "rustling" laminal retroflex sounds~(sz,~ż,~cz,~dz), the corresponding "humming" alveolo-palatals~(ś,~ź,~ć,~dź), and "hissing" alveolars~(s,~z,~c,~dz). Table \ref{table:consonants} presents Polish consonants -- their orthographic transcription, corresponding \gls{ipa} symbols, examples, and English approximations \cite{gussmann2007phonology}. Several comments should be made:

\begin{itemize}
\item The sound /\textipa{\:z}/ can be written either as <ż>, as in \textit{może} /\textipa{mO\:zE}/ (he/she can) or <rz>, as in \textit{morze} /\textipa{mO\:zE}/ (sea). However, <rz> can also denote a combination of /\textipa{r}/ and /\textipa{z}/, e.g. \textit{zamarzać} /\textipa{zamarza\t{tC}}/ (to freeze).

\item The sound /\textipa{x}/ is rendered orthographically either as <ch> or <h>. It has strongest friction before consonants, weaker friction before vowels and weakest friction intervocalically, where it may be realized as glottal /\textipa{h}/. It also has a voiced allophone /\textipa{G}/ that occurs whenever /\textipa{x}/ is followed by a voiced obstruent, as in \textit{niechby} (may it).

\item The phoneme /\textipa{n}/ has a velar allophone \textipa{N}, which occurs before velar consonants, as in \textit{bank} /\textipa{baNk}/ (bank).

\item The  affricatives /\textipa{\t{t\:s} \t{d\:z}}/ are written with digraphs <cz~dż>. In contrast, transitions /t\:s d\:z/ are usually denoted by trigraphs <trz> and <drz>, e.g. \textit{trzoda} /\textipa{t\:sOda}/ (flock), \textit{drzewo} /\textipa{d\:zEvO}/ (tree), with some rare exceptions, like \textit{nadżerka} /\textipa{nad\:zErka}/ (laceration).

\item The alveolo-palatals /\textipa{C~\textctz~\t{tC}~\t{d\textctz}~\textltailn}/ are rendered <ś~ź~ć~dź~ń> pre-consonantally and word-finally. However, they are spelled with multigraphs <si~zi~ci~dzi~ni> before a vowel. There are some exceptions, especially in loanwords, e.g. \textit{sinus} /\textipa{sinus}/ (sine), \textit{siwert} /\textipa{sivert}/ (sievert).

\item The retroflex fricatives and affricates /\textipa{\:s~\:z~\t{t\:s}~\t{t\:z}}/ are sometimes transcribed as palato-alveolar consonants /\textipa{S~Z~\t{tS}~\t{tZ}}/ \cite{jassem2003polish}.

\item The phonemes /\textipa{k\super{i}}/ and /\textipa{g\super{i}}/ are less commonly transcribed as /\textipa{c}/ and /\textipa{\textbardotlessj}/.

  
\end{itemize}

\begin{table}[h!]
  \centering
  \caption{Polish consonants}
  \label{table:consonants}
  \begin{threeparttable}
  \begin{tabular}{@{} llll @{}}
      Polish & IPA & example & English approximation\\
      \midrule
      b & \textipa{b} & \textit{bar} /\textipa{bar}/ (bar) & \textbf{b}ar\\
      c & \textipa{\t{ts}} & \textit{co} /\textipa{\t{ts}O}/ (what) & ca\textbf{ts}\\
      ć/c(i) & \textipa{\t{tC}} & \textit{ćma} /\textipa{\t{tC}ma}/ (moth), \textit{ciało} /\textipa{\t{tC}awO}/ (body) & \textbf{ch}eer\tnote{*}\\
      cz & \textipa{\t{t\:s}} & \textit{czar} /\textipa{\t{t\:s}ar}/ (spell) & \textbf{ch}ild\tnote{*}\\
      d & \textipa{d} & \textit{dom} /\textipa{dOm}/ (home) & \textbf{d}og\\
      dz & \textipa{\t{dz}} & \textit{dzwon} /\textipa{\t{dz}vOn}/ (bell) & be\textbf{ds}\\
      dź/dz(i) & \textipa{\t{d\textctz}} & \textit{dźwig} /\textipa{\t{d\textctz}vik}/ (lift), \textit{dzień} /\textipa{\t{d\textctz}e\textltailn}/ (day) & \textbf{j}eep\tnote{*}\\
      dż & \textipa{\t{d\:z}} & \textit{dżem} /\textipa{\t{d\:z}Em}/ (jam) & \textbf{j}ug\tnote{*}\\
      f & \textipa{f} & \textit{fan} /\textipa{fan}/ (fan) & \textbf{f}an\\
      g & \textipa{g} & \textit{gol} /\textipa{gOl}/ (goal) & \textbf{g}irl\\
      g(i) & \textipa{g\super{i}} & \textit{magia} /\textipa{mag\super{i}a}/ (magic) & ar\textbf{g}ue\\
      h/ch & \textipa{x} & \textit{hak} /\textipa{xak}/ (hook), \textit{cham} /xam/ (boor) & \textbf{h}ope\\
      h(i)/ch(i) & \textipa{x\super{i}} & \textit{hit} /\textipa{x\super{i}it}/ (hit), \textit{chichot} /\textipa{x\super{i}ixOt}/ (giggle) & \textbf{h}ue\\
      j & \textipa{j} & \textit{jak} /\textipa{jak}/ (how) & \textbf{y}es\\
      k & \textipa{k} & \textit{kat} /\textipa{kat}/ (executioner) & s\textbf{c}am\\
      k(i) & \textipa{k\super{i}} & \textit{kiedy} /\textipa{k\super{i}jEd1}/ (when) & s\textbf{k}ew\\
      l & \textipa{l} & \textit{lot} /\textipa{lOt}/ (flight) & \textbf{l}ion\\
      ł & \textipa{w} & \textit{łoś} /\textipa{wOC}/ (moose) & \textbf{w}ay\\
      m & \textipa{m} & \textit{mapa} /\textipa{mapa}/ (map) & \textbf{m}ile\\
      m(i) & \textipa{m\super{i}} & \textit{miód} /\textipa{m\super{i}jut}/ (honey) & \textbf{m}ute\\
      n & \textipa{n} & \textit{nos} /\textipa{nOs}/ (nose) & \textbf{n}o\\
      ń/n(i) & \textipa{\textltailn} & \textit{koń} /\textipa{kO\textltailn}/ (horse), \textit{nie} /\textipa{\textltailn E}/ (no) & ca\textbf{ny}on\\
      p & \textipa{p} & \textit{park} /\textipa{park}/ (park) & \textbf{p}ut\\
      r & \textipa{r} & \textit{rap} /\textipa{rap}/ (rap) & --\\
      s & \textipa{s} & \textit{suma} /\textipa{suma}/ (sum) & \textbf{s}ong\\
      ś/s(i) & \textipa{C} & \textit{śruba} /\textipa{Cruba}/ (screw), \textit{siano} /\textipa{Cano}/ (hay) & \textbf{sh}e\tnote{*}\\
      sz & \textipa{\:s} & \textit{szum} /\textipa{\:sum}/ (noise) & \textbf{sh}ore\tnote{*}\\
      t & \textipa{t} & \textit{tak} /\textipa{tak}/ (yes) & \textbf{t}op\\
      w & \textipa{v} & \textit{wór} /\textipa{vur}/ (bag) & \textbf{v}an\\
      z & \textipa{z} & \textit{zero} /\textipa{zErO}/ (zero) & \textbf{z}ebra\\
      ź/z(i) & \textipa{\textctz} & \textit{źrebak} /\textipa{\textctz rEbak}/ (foal), \textit{zima} /\textipa{\textctz ima}/ (winter) & vi\textbf{si}on\tnote{*}\\
      ż/rz & \textipa{\:z} & \textit{żal} /\textipa{\:zal}/ (sorrow), \textit{rząd} /\textipa{\:zOnt}/ (government) & a\textbf{z}ure\tnote{*}\\
  \end{tabular}
  \begin{tablenotes}
  \item[*] Both the laminal retroflex sounds /\textipa{\:s~\:z~\t{t\:s}~\t{t\:z}}/ and corresponding alveolo-palatals /\textipa{C~\textctz~\t{tC}~\t{t\textctz}}/ sound similar to the English palato-alveolar consonants /\textipa{S~Z~\t{tS}~\t{tZ}}/, but are distinct sounds in Polish.
  \end{tablenotes}
  \end{threeparttable}
\end{table}

\FloatBarrier
\section{Motivation and outline}
\label{section:outline}
This dissertation presents a comparative analysis of morphosyntactic and semantic language models in the context of automatic speech recognition of Polish. It focuses mostly on $n$-grams, skip-grams, and neural probabilistic models. The goal of the experimental part is to find a model that is optimal in terms of \gls{werr}, size, and speed. The analysis of existing literature on the subject shows that language modelling of Polish is still in its incipient stages when compared to English. Most of the available works focus solely on different variations of the $n$-gram model \cite{majewski2008syllable, ziolko2011n}. Until recently, there were no studies on using neural networks for language modelling of Polish \cite{gajecki2013modelowanie, brocki2012connectionist}. A comparative review of existing approaches would allow to select the most promising directions of research and in the long-term perspective contribute to the development of efficient \gls{lvcsr} systems for Polish. Moreover, obtained results could be directly translatable to other Slavic languages.

The introduction has outlined the process of automatic speech recognition and the role of acoustic and language models in a Bayesian framework. Section \ref{section:polish} discusses some features of Polish and the challenges they pose in speech technology, in particular the rich morphology resulting in large lexicons and the relatively free word order, hindering the performance of standard $n$-gram models. 
Chapter \ref{chapter:lm} is a comprehensive overview of language models used in the experimental part: word $n$-grams, class $n$-grams, skip grams, and neural network models. Section \ref{section:evaluation} presents the evaluation metrics: perplexity, \gls{werr}, and average rating based on the Shannon visualisation method.
Chapter \ref{chapter:tools} describes the tools and resources used in the experiments with special emphasis on the National Corpus of Polish (\textit{Narodowy Korpus Języka Polskiego}, NKJP), described in Section \ref{section:nkjp}, the most comprehensive annotated collection of Polish texts, used to train all the models in the experimental part. Other tools include a morphosyntactic tagger and two language modelling frameworks.
Chapter \ref{chapter:results} presents the results of the experiments -- a comparison of all the models with respect to performance metrics described in Section \ref{section:evaluation}.
Chapter \ref{chapter:conclusion} contains a short summary of the results and discusses the implications of the conducted experiments, while at the same time pointing at some limitations of the methodology. It also proposes new directions for studies on language modeling of Polish and other similar languages.
