\chapter{Introduction}
\label{chapter:intro}

\section{Automatic speech recognition}
\label{section:asr}
\Gls{asr} can be defined as independent, computer-driven transcription of spoken language into readable text in real time \cite{stuckless1994developments, jelinek1997statistical}. Although this process can be performed almost effortlessly by the human brain, it is extremely difficult to reverse-engineer\footnote{The observation that low-level sensorimotor skills require far more computational resources than high-level reasoning is known as the Moravec's paradox and has been formulated independently by several artificial intelligence researchers in the 1980s. As Moravec wrote: ``it is comparatively easy to make computers exhibit adult level performance on intelligence tests or playing checkers, and difficult or impossible to give them the skills of a one-year-old when it comes to perception and mobility''\cite{moravec1988mind}.}. \Gls{lvcsr} falls into two distinct categories: speech transcription and speech understanding. The former aims to find the exact orthographic transcription of analysed utterance, while the latter aims to find its meaning. In this thesis we focus on speech transcription, because its performance can be reliably measured in terms of word recognition errors.

\subsection{Bayesian framework}
In general, the recogniser tries to determine the word sequence $\hat{W}=w_{1}, \ldots, w_{L}$ out of all possible hypotheses $W$ which is most likely to have generated the sequence of observed acoustic features $Y=y_{1}, \ldots, y_{T}$:
\begin{equation}
\label{equation:recogniser}
  \hat{W}=\max_{W}P(W|Y).
\end{equation}
We can rearrange the conditional probability using the Bayes rule:
\begin{equation}
  \label{equation:bayesian}
  \gls{probsequence}=\frac{P(Y|W)P(W)}{P(Y)}\propto{P(Y|W)P(W)}.
\end{equation}
$P(W)$ is estimated by the language model (see subsection \ref{subsection:lm}), while $P(Y|W)$ is computed using the acoustic model (see subsection \ref{subsection:acoustic}). Typical ASR systems use \glspl{hmm} to model the sequential structure of speech signal and \glspl{gmm} for modelling the emission distribution of \glspl{hmm} \cite{baker1975dragon, bourlard1994connectionist}.

\subsection{Feature extraction}
\label{subsection:features}
Speech is a non-stationary process, so to represent it as a succession of discrete states, it is assumed that its statistical properties are constant over a short period of time. Under this assumption it is possible to extract statistically meaningful acoustic parameters (feature vectors) from a sampled speech waveform. The most popular method of spectral analysis are \glspl{mfcc} and \gls{lpc}.
\begin{figure}[!ht]
  \centering
    \begin{tikzpicture}[block/.style = {draw, rectangle, minimum width = 4cm, minimum height = 1cm, font = \small}]
      \node [block] (preemphasis) {Preemphasis};nn
      \node [block, right = of preemphasis] (windowing) {Windowing};
      \node [block, right = of windowing] (dft) {DFT};
      \node [block, below = of dft] (melfilter) {Mel Filters};
      \node [block, below = of melfilter] (log) {Logarithm};
      \node [block, left = of log] (dct) {DCT};
      \node [block, left = of dct] (delta) {Delta Coefficients};
      \path[draw,->]
      (preemphasis) edge (windowing)
      (windowing) edge (dft)
      (dft) edge (melfilter)
      (melfilter) edge (log)
      (log) edge (dct)
      (dct) edge (delta);
    \end{tikzpicture}
    \caption{Calculating \gls{mfcc}}
    \label{figure:mfcc}
\end{figure}

The algorithm for calculating \glspl{mfcc} is shown in figure \ref{figure:mfcc}. First, the speech waveform is subjected to high-frequency preemphasis in order to compensate for lip radiation and attenuation of high frequencies caused by the sampling process \cite{singh2012preprocessing}. Typically, the signal is passed through a high-pass \gls{fir} filter:
\begin{equation}
H(z)=1-\frac{a}{z},
\end{equation}
where $0.9 \leq a \leq 1.0$ (for a different approach see: \cite{nossair1995signal}). The signal is then divided into a sequence of frames using 20-30 milisecond windows with about 50\% overlap. The extraction takes place simply by multiplying the value of the signal by the value of the window for every time point $n$:
\begin{equation}
  y[n]=s[n]w[n].
\end{equation}
Hamming window is commonly used, as it allows to avoid discontinuities at the boundaries:
\begin{equation}
\label{equation:hamming}
  w[n]=
  \begin{cases}
    0.54-0.46\cos(\frac{2 \pi n}{L}) & 0 \leq n \leq L-1 \\
    0                               & \text{otherwise.}
  \end{cases}
\end{equation}

The next step is to extract spectral information from the windowed signal using \gls{dft}. The result of Fourier analysis is the information about the amounts of energy in $N$ evenly-spaced discrete frequency bands:
\begin{equation}
  X[k]=\sum_{n=0}^{N-1}x[n]e^{\frac{-2\pi ikn}{N}}.
\end{equation}
However, human hearing is not equally sensitive at all frequency bands. Many studies confirm that above about 500 Hz increasingly large intervals are judged by listeners to produce equal pitch increments \cite{stevens1937scale, fletcher1938loudness}. It was shown that simulating this property of human brain during feature extraction improves ASR performance. The signal is therefore passed through a bank of triangular filters spaced linearly below 1000 Hz and logarithmically above. This corresponds to the mapping between raw acoustic frequency $f$ and mel frequency $m$ \cite{muda2010voice}:
\begin{equation}
  m=1127\ln(1+\frac{f}{700}).
\end{equation}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=\textwidth]{img/mel.png}
  \caption{Plot of pitch mel scale versus Hertz scale \cite{vedala2008mel}.}
  \label{figure:mel}
\end{figure}

Another property of human hearing is that we are less sensitive to variation in signal level at high amplitudes than at low amplitudes. To model that logarithmic response, we take the log of each of the mel spectrum values.

Finally, cepstrum of the signal is calculated. Formally, it can be defined as the inverse \gls{dft} of the log magnitude of the \gls{dft} of the signal, although \gls{dct} is also frequently used instead of inverse \gls{dft}:
\begin{equation}
  \label{equation:cepstrum}
  c[n]=\sum_{n=0}^{N-1}\log{\left(\abs{\sum_{n=0}^{N-1}x[n]e^{\frac{-2\pi ikn}{N}}}\right)}e^{\frac{2\pi ikn}{N}}.
\end{equation}

In general, speech can be represented with the source-filter model. The original glottal source waveform of particular fundamental frequency \gls{fundamental} is passed trough the vocal tract, which acts as a filter. However, glottal source features are irrelevant for distinguishing phones. Cepstral analysis enables to deconvolve the source from the filter and extract the vocal tract properties, which carry most information about the phone being produced. Higher values on the cepstrum x-axis represent the glottal pulse, while lower values correspond to vocal tract characteristic. Generally, MFCCs are formed from the first 12 cepstral values, which carry information solely about the vocal tract  \cite{jurafsky2000speech}. Another useful property of the cepstral coefficients is that their variance tends to be uncorrelated, which is not true in case of raw mel-frequency spectrum. This means that the GMMs don't have to represent the covariance between MFCCs, which significantly reduces the number of parameters (see subsection \ref{subsection:acoustic}).

Having 12 cepstral coefficients for each frame is not sufficient, as we need inormation about one more feature that is useful in phone recognition: energy. The energy of a signal $x$ from time sample $t_{1}$ to time sample $t_{2}$ can be calculated as the sum over time of the power of the samples:
\begin{equation}
  \text{Energy}=\sum_{t=t_{1}}^{t_{2}}x^{2}[t].
\end{equation}

We also have to take into account the variability of the signal. Changes from frame to frame, such as the slope of a formant at its transitions, provide a useful cue for phone identity. For each of the 13 features (12 cepstral features and energy), we calculate the delta (velocity) and double delta (acceleration) features. This can be done simply by computing differences between cepstral values, although usually more sophisticated estimates are used:

\begin{equation}
  d(t)=\frac{c(t+1)-c(t-1)}{2}.
\end{equation}

After adding energy coefficients and time derivatives, we end up with a total of 39 \glspl{mfcc}:
\begin{itemize}
\itemsep0em
\item 12 cepstral coefficients,
\item 12 delta cepstral coefficients,
\item 12 double delta cepstral coefficients,
\item 1 energy coefficient,
\item 1 delta energy coefficient,
\item 1 double delta energy coefficient.
\end{itemize}

\subsection{Acoustic model}
\label{subsection:acoustic}
To estimate the posterior probability $P(Y|W)$ in Equation \ref{equation:bayesian}, we use an acoustic model, which represents the relationship between feature vector sequences and some linguistic units. Most acoustic models are based on Hidden Markov Models, although other techniques, such as neural networks, segmental models, and conditional random fields, have also been applied succesfully \cite{yu2009hidden, yu2008maximum, mohamed2012acoustic}. Considering the number of words in a typical language \footnote{In \cite{michel2011quantitative} the size of the English lexicon in 2000 is estimated as $1022000$.}, it is impractical to train a separate HMM for each word. For this reason, sub-word units are used, generally phoneme-sized. Each spoken word $w$ from sequence $W$ is decomposed into a sequence of $K_{w}$ basic sounds, called pronunciation:
\begin{equation}
  \gls{pronunciation}=q_{1}, \ldots, q_{K_{w}}.
\end{equation}
Since a word can have multiple pronunciation, we have to calculate the posterior probability as a sum over all possible pronunciations:
\begin{equation}
  \label{equation:pronunciations}
  P(Y|W) = \sum_{Q}P(Y|Q)P(Q|W),
\end{equation}
where $Q$ is the particular sequence of pronunciations:
\begin{equation}
  P(Q|W)=\prod_{i=1}^{L}P(q^{w_{i}}|w_{i}).
\end{equation}
In practice, the number of alternative pronunciations for each word $w_{i}$ is small, which makes the summation \ref{equation:pronunciations} feasible. To calculate $P(Y|W)$, we represent each base phone $q$ by an \gls{hmm} of the form shown in Figure \ref{figure:hmm}, described by the following elements:
\begin{itemize}
\item $N$ -- number of states,
\item $S=\{S_{1}, \ldots, S_{N}\}$ -- individual states,
\item $\theta_{t}$ -- state at time t,
\item $\pi=\{\pi_{i}\}$ -- initial state distribution:
  \begin{equation*}
    \pi_{i}=P(\theta_{1}=S_{i}) \qquad (1 \leqslant i \leqslant N),
  \end{equation*}
\item $A=\{a_{ij}\}$ -- transition matrix describing the probability of transition from state $i$ to state $j$:
  \begin{equation*}
    a_{ij}=P(\theta_{t+1}=S_{j}|\theta_{t}=S_{i}) \qquad (1 \leqslant i, j \leqslant N), 
  \end{equation*}
\item $B=\{b_{i}(y)\}$ -- observation distribution associated with state $S_{i}$.
\end{itemize}
Every time step, an \gls{hmm} makes a transition from its current state to one of its connected states, generating a feature vector according to the probability density function $b_{j}(y)$. In the majority of \gls{hmm}-based speech recognisers, the emitting density is modeled by mixture of $M$ Gaussians \cite{juang1985mixture}:
\begin{equation}
  b_{i}(y)=\sum_{k=1}^{M}w_{ik}\mathcal{N}(y, \mu_{ik}, \Sigma_{ik}) \qquad (1 \leqslant i \leqslant N),
\end{equation}
where $\mu_{ik}$ and $\Sigma_{ik}$ are the mean vector and covariance matrix associated with state $S_{j}$ and mixture $k$, while $w_{ik}$ is the mixture weight:
\begin{equation}
w_{ik} \geqslant 0 \quad\text{and}\quad \mathlarger{\sum_{k=1}^{M}}w_{ik}=1.
\end{equation}
The model is based on two important assumptions:
\begin{enumerate}
\item \emph{The first-order Markov assumption --} the probability of a state $\theta_{t}$ at time $t$ is only dependent on the previous state $\theta_{t-1}$:
  \begin{equation}
    P(\theta_{t}|\theta_{t-1}, \ldots, \theta_{1})=P(\theta_{t}|\theta_{t-1}).
  \end{equation}
\item \emph{The output independence assumption --} every observation is conditionally independent of all other observations: 
  \begin{equation}
    P(y_{t}|y_{t-1}, \ldots, y_{1})=P(y_{t}|y_{t-1}).
  \end{equation}
\end{enumerate}
These assumption significantly simplify the calculation of posterior probability $P(Y|W)$ from Equation \ref{equation:pronunciations}. We can now define it in terms of acoustic model parameters \cite{lu2013subspace}:
\begin{equation}
  P(Y|Q)=\sum_{\Theta}P(\Theta, Y|Q),
\end{equation}
where $\Theta=\theta_{0}, \ldots, \theta_{T+1}$ is a state sequence and
\begin{equation}
  \label{equation:states}
  P(\Theta, Y|Q)=a_{\theta_{0}\theta_{1}}\prod_{t=1}^{T}b_{\theta_{t}}(y_{t})a_{\theta_{t}\theta{t+1}}.
\end{equation}
In the above equation, $\theta_{0}$ and $\theta_{T+1}$ are non-emitting states (entry and exit). The model parameters $\lambda=\{a_{ij}, b_{j}(y)\}$ can be estimated from a corpus of training data using the forward-backward algorithm \cite{baum1970maximization}. Generally, it iteratively updates the model parameters by maximizing the lower bound of the log-likelihood function $log(P(Y|W))$. For a detailed explanation of the algorithm, see \cite{gales2008application}.

\subsection{Language models}
\label{subsection:lm}
 In a Bayesian framework presented in equation \ref{equation:bayesian}, the language model estimates the a priori likelihood by assigning probability $P(W)$ to each word sequence $W=\{w_{1}, \ldots, w_{n}\}$ such that $\sum_{W}P(W)=1$ \cite{rosenfeld2000two}. Since the search is usually performed unidirectionally, $P(W)$ can be formulated as a chain rule:
\begin{equation}
  P(W)=\prod^{n}_{i=1}P(w_{i}|h_{i}),
\end{equation}
where $\gls{wordhistory}=\{w_{1}, \ldots, w_{i-1}\}$ is the word history for $w_{i}$, often reduced to equivalence class \gls{equivalenceclass}:
\begin{equation}
  P(w_{i}|h_{i})\approx P(w_{i}|\phi(h_{i})).
\end{equation}
Good equivalence classes maximise information about the next word $w_{i}$ given its history, but also require a vast quantity of example sequences. The development of effective statistical language models is therefore limited by the availability of representative and machine readable text corpora.

\section{Polish vs. English}
\label{section:polish}

\section{Outline}
\label{section:outline}
This dissertation presents the comparative analysis of morphosyntactic and semantic language models in the context of automatic speech recognition of Polish. It focuses mostly on $N$-gram models capturing language dependencies which occur in $N$-tuples of some particular modelling unit.
