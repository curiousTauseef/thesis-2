\chapter{Building the models}
\label{chapter:tools}
\section{Tools and resources}
This section describes the resources that served as a source of training data, and the tools that were used for gathering, filtering, and transforming the data, including \mbox{XML-parsing,} \mbox{POS-tagging,} lemmatisation, and morphological disambiguation. It also presents two language modelling frameworks that enabled building large-scale \mbox{$n$-gram} and neural language models on consumer hardware.
\subsection{NKJP}
\label{section:nkjp}
Collecting high quality language data is a difficult task, as large and representative collections of modern texts are generally hard to obtain, if only for copyright reasons. One potential source of modern texts is obviously the Internet. However, web data can be extremely noisy and scraping, cleaning, and normalising it would be a cumbersome process. It should also be considered that the language of the Internet is different from that used in speech. Indeed, transcripts of spoken language appear to be a more valuable source of training data in \gls{asr} applications, than written texts \cite{dziadzio2015comparison}. Moreover, using raw text data to build POS-based models would require a very accurate and robust morphological tagger. For these reasons, linguistic corpora have become essential in advanced language technology. Fortunately, there exists an extensive, publicly available reference corpus of Polish language.

\gls{nkjp} is a shared initiative of four institutions: the Institute of Computer Science at the Polish Academy of Sciences, the Institute of Polish Language at the Polish Academy of Sciences, the Polish Scientific Publisher PWN, and the Department of Computational and Corpus Linguistics at the University of Łódź. It has been carried out as a research-development project of the Ministry of Science and Higher Education. The full corpus contains over one and a half billion words. The list of sources for the corpora consists of literature, scientific journals, magazines, daily newspapers, and a variety of Internet texts. Most importantly, it contains transcripts of parliamentary proceedings and conversations. Moreover, the creators of the corpus claim that the conversations were chosen so that they represent both male and female speakers, in various age groups, coming from various regions in Poland \cite{lewandowska2012narodowy}.

Another important feature of the \gls{nkjp} is that every text is accompanied by several layers of annotation. The metadata contain information about the text, such as title, author, and source, or~--~in case of speech transcripts~--~the topic of the conversation, the level of formality, background information about the speakers, and so on \cite{przepiorkowski2009xml}. More importantly, every lexical unit is described by several tags carrying information about its grammatical class and category. This enabled to easily filter the data by rejecting foreign words, incomplete or corrupted segments, punctuation, and \mbox{non-alphanumeric} characters. 

For the purpose of the experimental part of this thesis, the redistributable subcorpus was used as the main source of training data. It consists of all text of the full \gls{nkjp} that are free from intellectual property constraints. The sources include mostly texts of legal documents and transcripts of parliamentary and investigation commission proceedings. For detailed information about the contents of the final training set, see Section \ref{subsection:trainingset}.

\begin{table}[h!]
  \begin{center}
	  \caption[Contents of the redistributable subcorpus of the NKJP]{Contents of the redistributable subcorpus of the NKJP.}
	    \label{table:freenkjp}
	    \begin{tabular*}{.6\linewidth}{@{\extracolsep{\fill}}lr}
      source & word count \\
      \cmidrule{1-2}
      books & 67 000\\
      proceedings of investigation commissions & 4 623 000\\
      legal texts & 6 970 000\\
      parliamentary proceedings & 87 621 000\\
    \end{tabular*}
  \end{center}
\end{table}

\subsection{Concraft-pl}
Concraft-pl is a morphosyntactic tagger for Polish. It combines Maca -- a morphosyntactic segmentation and analysis tool and Concraft -- a morphosyntactic disambiguation library based on constrained conditional random fields \cite{waszczuk2012harnessing}. It was trained on the manually annotated subcorpus of the \gls{nkjp}, so that the tagsets are compatible. The tagger was used to tag plain text data in cases where the manual or semi-automatic annotation was unavailable. Although the output of the tagger is a list of most probable morphosyntactic tags with corresponding probabilities, only the ones chosen by the disambiguation library were taken into consideration. The class mapping is therefore deterministic, which simplifies the language model and enables to use existing \gls{nkjp} annotation when possible. As for lemmatisation, Concraft-pl does not disambiguate between lemmas when their morphosyntactic tag is the same, so the NKJP annotation was used in almost every case and a random lemma from the Concraft-pl output was chosen otherwise.

\subsection{SRILM}
\gls{srilm} is a toolkit for building and applying statistical language models, primarily for use in speech recognition, statistical tagging, and machine translation. It has been under development in the SRI Speech Technology and Research Laboratory since 1995. It consists of a set of C++ libraries implementing language models, data structures and miscellaneous utility functions, a set of executable programs built on top of these libraries to perform standard tasks such as training and testing language models, and a collection of miscellaneous scripts facilitating minor related tasks \cite{stolcke2011srilm}. It mainly supports \mbox{$n$-gram} modelling, so it was used in the experimental part to train and evaluate \mbox{word-based} and \mbox{class-based} \mbox{n-gram} models. To automate certain tasks, the toolkit was accessed via a basic Python wrapper. SRILM was used under the SRI's Research Community License and the Python binding by Nathaniel Smith was used under the MIT Licence.
\subsection{RNNLM}
The \gls{rnnlm} is a simple language modelling toolkit built by Mikolov et al. \cite{mikolov2011extensions}. It consists of a single C++ library for training language models based on recurrent neural networks. It also facilitates model evaluation by providing functionalities for perplexity calculation and n-best list rescoring. The training process is controlled by a set of parameters, such as the size of the hidden layer, the number of clustering classes, and the number of time steps to use in \gls{bptt} (see \ref{section:rnn} for details). The learning rate is adjusted automatically based on an entropy score calculated on a validation file.
\section{Method}
\label{section:method}
Building language models can be roughly divided into two stages~--~first, the training data is gathered and pre-processed, and then the models are trained and evaluated. This section contains a description of the process conducted in the experimental part. The first subsection presents in detail the subsequent stages of the data preprocessing pipeline, while the second is devoted to the training process.
\subsection{Building the training sets}
\label{subsection:trainingset}
The training data was extracted from the \gls{nkjp}, more precisely from the redistributable subcorpus and the one million manually annotated subcorpus. First, the files were divided into two categories~--~written texts and transcripts of speech. This process was automated using the metadata. The speech training corpus contains mostly the transcripted proceedings of the Polish parliament and investigation commissions, while the text corpus consists mainly of legal documents. The testing set was built solely from parliamentary speeches and contains 20 000 sentences, each at least five words long. Testing the models on transcripts of spoken language seems reasonable if one wants to simulate the use of language models in an \gls{asr} system. However, the strong bias towards formal speech and writing is a serious issue. The training and testing corpora come from a specific domain and therefore cannot be treated a comprehensive representation of Polish (see Figures \ref{figure:commonunigrams}-\ref{figure:commontrigrams} presenting the most common \mbox{$n$-grams} from the training set). However, the one million subcorpus, although more representative, is far too small for the experiments to yield statistically significant results. Given this trade-off, it seemed logical to sacrifice representativeness for the sake of meaningfulness.

\begin{table}[!htbp]
	\centering
	\caption[Contents of the text corpus]{Contents of the text corpus.}
	\begin{tabular*}{.6\linewidth}{@{\extracolsep{\fill}}lr}
		source & word count \\
		\midrule
                books & 68 465 \\
                newspapers & 813 318 \\
                legal texts & 4 608 863 \\
                total  & 5 490 619 \\
	\end{tabular*}
\end{table}

\begin{table}[!htbp]
	\centering
	\caption[Contents of the speech corpus]{Contents of the speech corpus.}
	\begin{tabular*}{.6\linewidth}{@{\extracolsep{\fill}}lr}
		source & word count \\
		\midrule
                conversational speech  & 80 629 \\
                investigation commissions  & 4 198 129 \\
                parliamentary proceedings  & 13 895 902 \\
                total  & 18 174 660 \\
	\end{tabular*}
\end{table}

The XML files that constitute the \gls{nkjp} were parsed using a Python implementation of the ElementTree XML API. The XML files contain several layers of annotation, so it was possible to build all corpora in a single run. Four types of modelling tokens were used:

\begin{enumerate}
	\item words -- plain text,
	\item lemmas -- lemmatised text, 
	\item POS tags -- tags containing only information about the grammatical class (part of speech),
	\item GNC tags -- tags containing information about the grammatical class and selected grammatical categories that were shown in \cite{pohl2013using} to offer significant improvements over the \mbox{POS-only} model (gender, number, case).
\end{enumerate}

Each corpus was split into speech and text. The twelve final training datasets are listed in Table \ref{table:datasets}. This nomenclature will be used consistently throughout the following chapters. The contents of the corpora were filtered during extraction. All corrupted and incomplete segments were discarded, all numerals expressed with digits were substituted with a common tag, and all sentences were converted to lowercase, stripped of punctuation, non-alphanumeric symbols, foreign words and abbreviations. Only segments longer than three words were selected. 

\begin{table}[!htbp]
	\centering
	\caption{List of the training corpora.}
	\begin{tabular*}{.6\linewidth}{@{\extracolsep{\fill}}llll}
	\label{table:datasets}
		word full   & lemma full   & POS full   & GNC full \\ 
		word text   & lemma text   & POS text   & GNC text \\
		word speech & lemma speech & POS speech & GNC speech \\ 
	\end{tabular*}
\end{table}

\subsection{Training the n-gram models}
The \mbox{$n$-gram} models were built using the SRILM toolkit. First, the \mbox{$n$-gram} counts were computed for each corpus. Only unigrams, bigrams, and trigrams were taken under consideration. 

\begin{table}[!htbp]
	\centering
	\caption[Number of unique tokens in the word corpus]{Number of unique tokens in the word corpus.}
	\begin{tabular*}{.6\linewidth}{@{\extracolsep{\fill}}l*3r}
	\label{table:countsword}
		{}        & \multicolumn{1}{c}{text} & \multicolumn{1}{c}{speech} & \multicolumn{1}{c}{full}  \\
		\midrule
                unigrams  &  157 866   & 184 236    & 244 346\\
	        bigrams   &  1 540 010 & 4 083 052  & 5 111 189\\
		trigrams  &  4 669 098 & 14 549 205 & 12 843 843\\
	\end{tabular*}
\end{table}

\begin{table}[!htbp]
	\centering
	\caption[Number of unique tokens in the lemma corpus]{Number of unique tokens in the lemma corpus.}
	\begin{tabular*}{.6\linewidth}{@{\extracolsep{\fill}}l*3r}
	\label{table:countslemma}
		{}        & \multicolumn{1}{c}{text} & \multicolumn{1}{c}{speech} & \multicolumn{1}{c}{full}  \\
		\midrule
                unigrams  &  54 522    & 41 666    & 65 420     \\
	        bigrams   &  1 012 669 & 2 241 798 & 2 835 191  \\
		trigrams  &  2 555 053 & 8 314 572 & 10 380 493 \\
	\end{tabular*}
\end{table}

\begin{table}[!htbp]
	\centering
	\caption[Number of unique tokens in the POS corpus]{Number of unique tokens in the POS corpus.}
	\begin{tabular*}{.6\linewidth}{@{\extracolsep{\fill}}l*3r}
	\label{table:countspos}
		{}        & \multicolumn{1}{c}{text} & \multicolumn{1}{c}{speech} & \multicolumn{1}{c}{full}  \\
		\midrule
                unigrams  &  34    & 34     & 34   \\
	        bigrams   &  851   & 905    & 935  \\
		trigrams  &  10238 & 14030  & 14689\\
	\end{tabular*}
\end{table}

\begin{table}[!htbp]
	\centering
	\caption[Number of unique tokens in the GNC corpus]{Number of unique tokens in the GNC corpus.}
	\begin{tabular*}{.6\linewidth}{@{\extracolsep{\fill}}l*3r}
	\label{table:countsgnc}
		{}        & \multicolumn{1}{c}{text} & \multicolumn{1}{c}{speech} & \multicolumn{1}{c}{full}  \\
		\midrule
                unigrams  & 462      & 411     & 475     \\
	        bigrams   & 34 874   & 38 107  & 45 689  \\
		trigrams  & 389 709  & 686 461 & 803 012 \\
	\end{tabular*}
\end{table}

The results for the four types of corpora are presented in Tables \ref{table:countsword}-\ref{table:countsgnc}. The counts include the start-of-sentence and end-of-sentence tokens. The statistics confirm the issues described in Subsection \ref{subsection:inflection}. The first evident problem is the size of the lexicon. Even though the corpus is limited to one domain, there are 244 346 distinct unigrams and 148 567 of them occur more than once. To achieve 99\% unigram coverage, a lexicon of more than 92 000 words is required (see Figure \ref{figure:coverage}). Note also the data sparsity~--~even for the POS corpus, with only 34 unigrams, less than 40\% of potential trigrams occur in the corpus. In case of the word corpus, less than 0.0009\% of potential bigrams are present in the training data. Another interesting observation is the relationship between the word and lemma corpus. There are almost four times more inflected unigrams than distinct lemmas~--~a word in Polish has, on average, four inflected forms. A vocabulary of 33 000 lemmas is enough to achieve 99\% coverage. However, important information about grammatical agreement is lost in the lemmatisation process.

\begin{figure}[!htbp]
	  \centering
	  \includegraphics[height=10cm, width=15cm]{coverage.png}
      \caption[Unigram coverage of the full word corpus]{Unigram coverage of the full word corpus as a function of the vocabulary size.}
      \label{figure:coverage}
\end{figure}
\FloatBarrier
The \mbox{$n$-gram} counts were then smoothed. Several smoothing methods were tested~--~Good-Turing, Kneser-Ney, and Chen-Goodman's modified Kneser-Ney, both in the back-off and interpolated variant. Perplexity of the word text model calculated on the testing set is presented in Table \ref{table:perplexitysmoothing}. The model discounted using the interpolated modified Kneser-Ney method has the lowest perplexity. This smoothing technique was used for all language models except for the POS and GNC models, where the size of the vocabulary is too small to apply it. However, in case of models with a small vocabulary, smoothing is less of an issue.

\begin{table}[h!]
  \begin{center}
	  \caption[Perplexity of a language model with different smoothing methods]{Perplexity of a language model trained on the full word corpus with different smoothing methods~--~Good-Turing, Kneser-Ney (back-off), Kneser-Ney (interpolated), Chen-Goodman (back-off), Chen-Goodman (interpolated).}
	    \label{table:perplexitysmoothing}
	    \begin{tabular*}{.8\linewidth}{@{\extracolsep{\fill}}l*6r}
		    {} & \multicolumn{1}{c}{GT} & \multicolumn{1}{c}{KNB} & \multicolumn{1}{c}{KNI} & \multicolumn{1}{c}{CGB} & \multicolumn{1}{c}{CGI}\\
		    \midrule
      unigrams & 2604.09 & 3735.08 & 3735.08 & 3767.08 & 3767.37\\
      bigrams  & 387.11  & 471.07  & 452.72  & 484.31  & 458.40\\
      trigrams & 277.19  & 250.40  & 244.44  & 254.08  & 242.31\\
    \end{tabular*}
  \end{center}
\end{table}

\begin{figure}[!htbp]
	  \centering
	  \includegraphics[height=10cm, width=15cm]{ngrams.png}
      \caption{N-gram distribution in the full word corpus.}
      \label{figure:ngramdistribution}
\end{figure}

Figure \ref{figure:ngramdistribution} presents the distribution of \mbox{n-grams} in the full word corpus on a double logarithmic plot. Zipf's law states that the \mbox{rank-frequency} distribution of words in a natural language corpus is an inverse relation. More formally, the probability mass function of the term frequencies is of the form:
\begin{equation}
	p=\alpha k^{-\beta},
	\label{equation:zipf}
\end{equation}
where $k$ is the rank of the term from the most to the least frequent one, and $(\alpha, \beta)$ are distribution parameters. Except for the very common and very rare terms, \mbox{$n$-grams} in the full word corpus follow a Zipfian distribution. The exponential decay factor $\beta$ decreases with \mbox{$n$-gram} order and so does $\alpha$, the frequency of the most common term.

Figures \ref{figure:commonunigrams}-\ref{figure:commontrigrams} present the most common terms of the full word corpus. Although the popular unigrams and bigrams are mostly stop words and general phrases, trigrams reveal a strong bias towards parliamentary jargon. For example, the phrase \textit{radiofonii i telewizji} refers to a bill which was the subject of the so called Rywin affair, a large corruption scandal investigated by a parliamentary committee, proceedings of which consist a large portion of the training corpus. This lack of representativeness will probably be reflected during the model evaluation.

\begin{figure}[!htbp]
	  \includegraphics[height=8cm, width=15cm]{common_unigrams.png}
	      \caption{Most common unigrams of the full word corpus.}
	      \label{figure:commonunigrams}
\end{figure}

\begin{figure}[!htbp]
	  \includegraphics[height=8cm, width=15cm]{common_bigrams.png}
	      \caption{Most common bigrams of the full word corpus.}
	      \label{figure:commonbigrams}
\end{figure}

\begin{figure}[!htbp]
	  \includegraphics[height=8cm, width=15cm]{common_trigrams.png}
	      \caption{Most common trigrams of the full word corpus.}
	      \label{figure:commontrigrams}
\end{figure}

\FloatBarrier
\subsection{Training the neural network models}
The neural network models were trained on the same datasets as the \mbox{$n$-gram} models. However, as the computation times for \gls{rnn} models were extremely large, only the full corpora were used, resulting in four \gls{rnn} models. Twenty percent of phrases from each training corpus were randomly assigned to a validation set, which was used for controlling the learning rate and early stopping. All models were trained using 300 neurons in the hidden layer and two-step \gls{bptt}. For word and lemma corpora, 500 word classes were used. For POS and GNC models, no additional clustering was necessary.
