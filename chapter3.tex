\chapter{Building the models}
\label{chapter:tools}
\section{Tools and resources}
This section describes the resources that served as a source of training data, and the tools that were used for gathering, filtering, and transforming the data, including XML-parsing, POS-tagging, lemmatization, and morphological disambiguation. It also presents two language modelling frameworks that enabled building large-scale n-gram and neural language models on consumer hardware.
\subsection{NKJP}
\label{section:nkjp}
Collecting high quality language data is a difficult task, as large and representative collections of modern texts are generally hard to obtain, if only for copyright reasons. One potential source of modern texts is obviously the Internet. However, web data can be extremely noisy and scraping, cleaning, and normalizing it would be a cumbersome process. It should also be considered that the language of the Internet is different from that used in speech. Indeed, transcripts of spoken language appear to be a more valuable source of training data in \gls{asr} applications, than written texts \cite{dziadzio2015comparison}. Moreover, using raw text data to build POS-based models would require a very accurate and robust morphological tagger. For these reasons, linguistic corpora have become essential in advanced language technology. Fortunately, there exists an extensive, publicly available reference corpus of Polish language.

The \gls{nkjp} is a shared initiative of four institutions: Institute of Computer Science at the Polish Academy of Sciences, Institute of Polish Language at the Polish Academy of Sciences, Polish Scientific Publishers PWN, and the Department of Computational and Corpus Linguistics at the University of Łódź. It has been carried out as a research-development project of the Ministry of Science and Higher Education. The full corpus contains over one and a half billion words. The list of sources for the corpora consists of literature, scientific journals, magazines, daily newspapers, and a variety of Internet texts. Most importantly, it contains transcripts of parliamentary proceedings and conversations. Moreover, the creators of the corpus claim that the conversations were chosen so that they represent both male and female speakers, in various age groups, coming from various regions in Poland \cite{lewandowska2012narodowy}. For that reason alone, \gls{nkjp} seemed like an excellent choice for the source of training data.

Another important feature of the \gls{nkjp} is that every text is accompanied by several layers of annotation. The metadata contain information about the text, such as title, author, and source, or~--~in case of speech transcripts~--~the topic of the conversation, the level of formality, background information about the speakers, and so on \cite{przepiorkowski2009xml}. More importantly, every lexical unit is described by several tags carrying information about its grammatical class and category. This enabled to easily filter the data by rejecting foreign words, incomplete or corrupted segments, punctuation, and non alpha-numeric characters. 

For the purpose of the experimental part of this thesis, the redistributable subcorpus was used. It consists of all text of the full \gls{nkjp} that are free from intellectual property constraints. The sources include mostly texts of legal documents and transcripts of parliamentary and investigation commision proceedings (see Table \ref{table:freenkjp}). For detailed information about the contents of the final training set, see Section \ref{subsection:trainingset}.

\begin{table}[h!]
  \begin{center}
	  \caption{Contents of the redistributable subcorpus of the NKJP}
	    \label{table:freenkjp}
	    \begin{tabular*}{.8\linewidth}{@{\extracolsep{\fill}}ll}
      Source & Number of words (thousands) \\
      \midrule
      Books & 67\\
      Laws & 6970\\
      Proceedings of Investigation Commisions & 4623\\
      Parliamentary proceedings & 87621\\
    \end{tabular*}
  \end{center}
\end{table}

\subsection{Concraft-pl}
\subsection{SRILM}
\subsection{RNNLM}
\section{Method}
The process of building a language model can be roughly divided into two stages~--~data gathering and pre-processing and training the model.
\subsection{Building the training set}
\label{subsection:trainingset}

\begin{table}[!htbp]
	\centering
	\caption{Number of unique tokens in the plain text and lemma training corpora (thousands)}
	\begin{tabular}{*5c}
		{}        &  \multicolumn{2}{c}{plain} & \multicolumn{2}{c}{lemma}\\
		\cmidrule{2-5}
		{}        &  text & speech & text & speech \\
		unigrams  &  158  & 184    & 55   & 42    \\
	        bigrams   &  1540 & 4083   & 1013 & 2242   \\
                trigrams  &  639  & 1480   & 679  & 1609   \\
	\end{tabular}
\end{table}

\begin{table}[!htbp]
	\centering
	\caption{Number of unique tokens in the pos and gnc training corpora}
	\begin{tabular}{*5c}
		{}        &  \multicolumn{2}{c}{pos} & \multicolumn{2}{c}{gnc}\\
		\cmidrule{2-5}
		{}        &  text & speech & text    & speech \\
		unigrams  &  35   & 35     & 463     & 412    \\
	        bigrams   &  851  & 905    & 34873   & 38107  \\
                trigrams  &  8172 & 11956  & 212840  & 408868 \\
	\end{tabular}
\end{table}

\subsection{Training the models}
