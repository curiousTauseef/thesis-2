\chapter{Conclusions}
\label{chapter:conclusion}
The purpose of this thesis was to conduct a comparative analysis of morphosyntactic and semantic language models in the context of automatic speech recognition of Polish. Sixteen n-gram, class n-gram and neural network models were built and evaluated using several performance metrics: perplexity, size, and word error rate reduction evaluated on two n-best list rescoring tasks~--~one using the output of a real \gls{asr} system, and the other involving artificially generated recognition hypothesis.

Perplexity evaluation, although often criticized as a metric of language model quality, revealed some trends which were further confirmed in the \gls{werr} experiments. First of all, there is a noticeable difference between language models trained on speech transcripts and those trained on written texts, regardless of the modelling unit used. The comparison of models with identical vocabularies showed that models trained on speech transcripts achieve lower perplexity than their text-based counterparts. Moreover, in case of words and lemmas, the speech-based trigram models outperform those trained on the full corporas.
Although more cumbersome, the \gls{werr} evaluation on a real \gls{asr} system is undoubtedly a more reliable evaluation method than perplexity. The results confirm what has been reported in \cite{dziadzio2015comparison}, this time on a much larger training set~--~n-gram models trained on speech transcripts indeed perform better than text-based models, even when morphosyntactic tags are used as the modelling units. Figure \ref{figure:total} presents a summary of results.

\begin{figure}[!htbp]
	  \centering
	  \includegraphics[height=10cm, width=15cm]{total_werr.png}
	  \caption{The absolute WERR achieved by respective models on n-best list rescoring}
	      \label{figure:total}
\end{figure}

The quality of the acoustic model may have been an important factor in the \gls{werr} calculation pipeline. Because a newer version of the recognition engine was unavailable, an alternative approach was proposed to simulate the operation of an advanced acoustic model. The mock n-best list rescoring is a compromise between proxy metrics like perplexity and using a real recognition system, as the generation of mock n-best lists is fully automated. It allows to simulate the rescoring task using any testing set without an actual \gls{asr} system.

\begin{figure}[!htbp]
	  \centering
	  \includegraphics[height=10cm, width=15cm]{total_simulated.png}
	  \caption{The absolute WERR achieved by respective models on mock n-best list rescoring}
	      \label{figure:total_mock}
\end{figure}
