\chapter{Conclusions}
\label{chapter:conclusion}
The purpose of this thesis was to conduct a comparative analysis of morphosyntactic and semantic language models in the context of automatic speech recognition of Polish. Sixteen n-gram, class n-gram and neural network models were built and evaluated using several performance metrics: perplexity, size, and word error rate reduction on two n-best list rescoring tasks~--~one using the output of a real \gls{asr} system, and the other involving artificially generated recognition hypothesis.

Perplexity evaluation, although often criticized as a metric of the language model quality, revealed some trends which were further confirmed in the \gls{werr} experiments. First of all, there is a noticeable difference between language models trained on speech transcripts and those trained on written texts, regardless of the modelling unit used. The comparison of models with identical vocabularies showed that models trained on speech transcripts achieve lower perplexity than their text-based counterparts. Moreover, in case of words and lemmas, the speech-based trigram models outperform those trained on the full corporas.
Although more cumbersome, the \gls{werr} evaluation on a real \gls{asr} system is undoubtedly a more reliable evaluation method than perplexity. The results confirm what has been reported in \cite{dziadzio2015comparison}, this time on a much larger training set~--~n-gram models trained on speech transcripts indeed perform better than text-based models, even when morphosyntactic tags are used as the modelling units. Figure \ref{figure:total} presents a summary of results.

\begin{figure}[!htbp]
	  \centering
	  \includegraphics[height=10cm, width=15cm]{total_werr.png}
	  \caption[The absolute WERR achieved by respective models on n-best list rescoring]{The absolute WERR achieved by respective models on n-best list rescoring.}
	      \label{figure:total}
\end{figure}

The quality of the acoustic model may have been an important factor in the \gls{werr} calculation pipeline. Because a newer version of the recognition engine was unavailable, an alternative approach was proposed to simulate the operation of an advanced acoustic model. The mock n-best list rescoring is a compromise between proxy metrics like perplexity and using a real recognition system, as the generation of mock n-best lists is fully automated. It allows to simulate the rescoring task using any testing set without an actual \gls{asr} system. The results of the mock n-best list rescoring evaluation are presented in Figure \ref{figure:total_mock}. Note the very good score of the neural and \mbox{$n$-gram} GNC models, which both outperformed the word text model, despite a vocabulary order of magnitudes smaller.

\begin{figure}[!htbp]
	  \centering
	  \includegraphics[height=10cm, width=15cm]{total_simulated.png}
	  \caption[The absolute WERR achieved by respective models on mock n-best list rescoring]{The absolute WERR achieved by respective models on mock n-best list rescoring.}
	      \label{figure:total_mock}
\end{figure}

To sum up, the experimental part provided some valuable insights into how to optimise the language modelling of highly inflected language for speech recognition. First of all, the speech data once again proved to be a more valuable source of information than written texts \cite{dziadzio2015comparison}. Unfortunately, the conversation corpus in the \gls{nkjp} is too small to repeat the experiments on general spoken language. 

The second important observation is that a morphosyntactic model based on grammatical classes and three grammatical categories (gender, number, case) turned out to be a very effective way of modelling Polish for speech recognition. The GNC model performs similarly to a \mbox{lemma-based} model and does so at a fraction of the computational cost. It is likely that it is possible to find a similarly informative set of grammatical categories for other inflected languages. 

The last conclusion that can be drawn from the experimental part is that although the neural network models in most cases outperform their \mbox{$n$-gram} counterparts, the differences are rather slight. This can be attributed to numerous factors. Firstly, the neural models were trained on full corpora instead of just the speech transcript parts. Moreover, they were not optimised, as the values of parameters (the number of \gls{bptt} steps, the size of the hidden layer, the number of classes) were chosen so that the training part would be feasible on a consumer hardware. Additional experiments could definitely shed some light on the matter. It would be especially interesting to build and test a neural GNC model trained entirely on speech data, this time with a much larger hidden layer and a lot more \gls{bptt} steps, as it has the potential of being a viable alternative to \mbox{word-based} \mbox{$n$-grams}.

