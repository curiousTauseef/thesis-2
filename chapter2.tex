\chapter{Statistical language models}
\label{chapter:lm}
Language modelling lies at the core of many natural language processing tasks, such as speech recognition and synthesis, document classification, grammar and spelling checking, parsing, machine translation, and information retrieval. In general, the task of statistical language models is to estimate the likelihood of a sequence of words. This information can then be used to reject invalid sentences or resolve ambiguities. In case of automatic speech recognition the language model guides and constrains the search among alternative word hypotheses \cite{glass2013automatic}.

In this chapter, a thorough introduction to statistical language modelling is given. The first section describes the word $n$-gram model, which is still one of the most important tools in speech and language processing. It also presents several smoothing techniques. Section \ref{section:ann} is dedicated to the connectionist models based on artificial neural networks. The final section contains an overview of evaluation methods for language models.
\section{N-gram models}
\label{section:ngrams}
In Section \ref{subsection:lm} we defined the chain rule of probability, which lets us decompose the joint probability of a sequence of $N$ words into a product of conditional probabilities:
\begin{equation}
	P(w_{1}, \dots, w_{n})=\prod_{i=1}^{n}P(w_{i}|w_{1},\dots,w_{i-1})=\prod_{i=1}^{n}P(w_{i}|h_{i})
\end{equation}
However, there is no way of computing the probability of a word given a long history of preceding words. Estimating it from relative frequency counts is infeasible, if only for data sparsity reason. We can deal with this problem by using an $n$-gram model. An $n$-gram is simply a sequence of $n$ words (or other modelling units) -- an $n$-gram of size 1 is called a unigram, size 2 is a bigram, and size 3 is called a trigram. The $n$-gram model approximates the probability of a word given all the previous words with the probability of the word given $n-1$ preceding words:
\begin{equation}
	P(w_{i}|w_{1},\dots,w_{i-1})\approx P(w_{i}|w_{i-(n-1)},\dots,w_{i-1})
\end{equation}
The $n$-gram can be interpreted as a left-to-right Markov chain of order $n-1$, because the probability of the current state depends only on $n-1$ previous states. To put it differently, the $n$-gram models satisfies the $n-1$ order Markov assumption (see Section \ref{subsection:acoustic}). In contrast to \glspl{hmm}, the states of Markov chains are directly visible.

Another way to look at the $n$-gram model is through the notion of history equivalence class, introduced in Section \ref{subsection:lm}. As indicated previously, it is impossible to obtain a probability estimate for every imaginable word history, but the number of parameters can be reduced by classifying word histories into equivalence classes. Indeed, in \cite{jelinek1997statistical}, language modelling is defined as the task of finding the best history classification function:
\begin{equation}
	\Phi:h_{i}\mapsto\varphi_{i}=\Phi(h_{i})=\Phi(w_{1}, \dots, w_{i-1})
\end{equation}
Finding an optimal history classification function is the matter of striking a fine balance between its predictive power and complexity. To be a useful predictor, an equivalence class has to maximise the information about next word, while minimising the dimensionality of the resulting model. In case of $n$-gram models, the history classification function $\Phi$ simply limits the word history to the last $n-1$ elements:
\begin{equation}
	\varphi_{i}=\Phi(h_{i})=\Phi(w_{1}, \dots, w_{i-1})=w_{i-(n-1)}, \dots, w_{i-1}
\end{equation}
Two word histories are therefore equivalent if their last $n-1$ elements are identical. The tradeoff between efficiency and complexity quickly becomes apparent when the value of $n$ is increased~--~higher order $n$-grams typically exhibit lower perplexity (see Section \ref{subsection:perplexity}) and better performance, but are more expensive in terms of time and memory.
\subsection{Maximum Likelihood Estimation}
\label{subsection:mle}
The most common technique of estimating the $n$-gram probabilities from a text corpus is \gls{mle}. The procedure is very simple -- the $n$-gram probability of a word $w_{i}$ given the truncated history $\varphi_{i}=w_{i-(n-1)}, \dots, w_{i-1}$ is the observed frequency of the $n$-gram $w_{i-(n-1)}, \dots, w_{i-1}, w_{i}$ normalised by the sum of observed frequencies of all $n$-grams sharing the same history:
\begin{equation}
	\label{equation:mle}
	p(w_{i}|w_{i-(n-1)}, \dots, w_{i-1})=\frac{C(w_{i-(n-1)}, \dots, w_{i-1}, w_{i})}{\sum_{w}C(w_{i-(n-1)}, \dots, w_{i-1}, w)}.
\end{equation}
Note that the sum in the denominator is equal simply to $C(\varphi_{i})$, so the formula in Equation \ref{equation:mle} can be more intuitively understood as the proportion of cases in which a particular equivalent word history $\varphi_{i}$ is followed by the word $w_{i}$:
\begin{equation}
	\label{equation:mle2}
	P(w_{i}|\varphi_{i})=\frac{C(\varphi_{i}, w_{i})}{C(\varphi_{i})}.
\end{equation}
This ratio is called a relative frequency. In the special case of unigram probability it can be calculated as the count of the particular word $w_{i}$ normalised by the size of the corpus:
\begin{equation}
	\label{equation:mle2}
	P(w_{i})=\frac{C(w_{i})}{\sum_{w}C(w)}.
\end{equation}
Table \ref{table:mle} shows an example of applying \gls{mle} on a corpus of four sentences from Monty Python's notorious spam sketch.
\begin{table}[h!]
	\caption{Calculating selected bigram and trigram probabilities using maximum likelihood estimation \cite{jurafsky2000speech}}
	\label{table:mle}
	\texttt{%
		\begin{tabular}{c}
			<s> egg bacon and spam </s> \\
			<s> egg bacon sausage and spam </s> \\
			<s> spam bacon sausage and spam </s> \\
			<s> spam egg spam spam bacon and spam </s> \\
			\\
		\end{tabular}}
		\centering
		\begin{tabular*}{.8\linewidth}{@{\extracolsep{\fill}}lll}
			$P(\texttt{bacon}|\texttt{spam})=\frac{2}{8}$ & $P(\texttt{spam}|\texttt{spam})=\frac{1}{8}$ & $P(\texttt{and}|\texttt{egg bacon})=\frac{1}{2}$  \\
			$P(\texttt{bacon}|\texttt{egg})=\frac{2}{3}$  & $P(\texttt{spam}|\texttt{<s>})=\frac{2}{4}$ & $P(\texttt{egg}|\texttt{bacon and})=0$ \\
			$P(\texttt{bacon}|\texttt{<s>})=0$            & $P(\texttt{spam}|\texttt{and})=1$ & $P(\texttt{</s>}|\texttt{and spam})=1$ \\
		\end{tabular*}
\end{table}

\subsection{Word n-grams}
So far, while describing the $n$-gram models, we used the terms ``word'' and ``modeling unit'' interchangeably. However, depending on the application, $n$-grams can use sub-lexical units, such as phonemes, letters, and syllables, or supra-lexical units, such as \gls{pos} tags. In case of \gls{asr}, words are the most popular choice for the modeling units, because word-based models can be easily incorporated into the search process described in section \ref{subsection:lm}. For example, consider this famous quote:
\begin{center}
\texttt{open the pod bay doors}  
\end{center}
We would like to estimate the probability of next word being \texttt{hal}. Using a bigram model, we get:
\begin{equation}
	P(\texttt{hal}|\texttt{open the pod bay doors}) \approx P(\texttt{hal}|\texttt{doors})
\end{equation}
Truncating the history to $n-1$ units drastically reduces the number of free parameters. Although this number is still enormous - for a bigram model and a vocabulary of 50000 words, there are potentially two and a half billion parameters -- it is at least feasible to estimate the probability of the entire sequence:
\begin{multline}
	P(\texttt{open the pod bay door hal}) \approx \\
	\approx P(\texttt{open}|\texttt{<s>})P(\texttt{the}|\texttt{open})P(\texttt{pod}|\texttt{the})P(\texttt{bay}|\texttt{pod})P(\texttt{door}|\texttt{bay})P(\texttt{hal}|\texttt{door})P(\texttt{</s>}|\texttt{hal}),
\end{multline}
where <s> and </s> are start-of sentence and end-of-sentence markers respectively.
There are several disadvantages of word-based $n$-gram models. The first one is the obviously unrealistic Markov's assumption. Language dependencies often span across far more than just three or four words, especially in inflected languages. Consequently, low-order word-based $n$-grams may do a poor job at disambiguating grammatically invalid sentences, because an incorrect sentence can be constructed from a sequence of valid short $n$-grams. Take this quote from Yoda as an example:
\begin{center}
	\texttt{look I so old to young eyes.}
\end{center}
A bigram model would likely assign a relatively high probability to this phrase, because each pair of consecutive words is entirely plausible. 
In the above example, using a trigram model would probably yield better results, but at a considerable cost - even for a lexicon of 50 000 words (far too few for modeling any inflected language), switching from bigram to trigram introduces more than one hundred trillions potential parameters! Several techniques of dealing with the \gls{oov} words are available (see Section \ref{subsection:smoothing}), but there is always a tradeof between the quality of predictions and the amount of data required to calculate the probability estimates. Training and storing higher-order word $n$-grams can be simply infeasible due to data sparsity and memory constraints.
Another issue with word-based $n$-grams is that the truncation of word history may lead to unintuitive or inconsistent behaviour. This problem is especially pronounced in case of languages with weak constraints on word order -- permutations of the same sequence may be assigned different probability estimates despite being grammatically and semantically equivalent. Furthermore, very similar word histories may lead to different predictions. Consider these two sequences:
\begin{center}
	\texttt{a brilliant american mathematician who graduated from} \\
	\texttt{a brilliant american mathematician who graduated recently from}.
\end{center}
From the point of view of predicting the next word, these two histories are very similar. However, they would be considered completely different even by a trigram word model \cite{whittaker2000statistical}. Furthermore, in the second case, we would need at least a $5$-gram model to capture the intuition that the next word is probably a name of a university.

Although word-based $n$-gram language models have been outperformed by deep neural networks in terms of perplexity and word error reduction rate, they are still commonly used in \gls{asr} systems, as they strike a fine balance between effectiveness and simplicity. They are straightforward to train and can be easily used to guide the search through the lattice of word hypotheses. However, in case of inflected languages, the problem of data sparsity is significantly amplified because and therefore using words as the modeling unit is not necesarily the best choice.
\subsection{Class-based $n$-gram models}
\section{Neural network models}
\label{section:ann}
\section{Evaluation of language models}
\label{section:evaluation}
\section{Evaluation of language models}
\label{section:evaluation}
\subsection{Perplexity}
\label{subsection:perplexity}
\subsection{WERR}
\label{subsection:werr}
\subsection{Shannon Method}
\label{subsection:perplexity}
