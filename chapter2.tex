\chapter{Statistical language models}
\label{chapter:lm}
Language modelling lies at the core of many natural language processing tasks, such as speech recognition and synthesis, document classification, grammar and spelling checking, parsing, machine translation, and information retrieval. In general, the task of statistical language models is to estimate the likelihood of a sequence of words. This information can then be used to reject invalid sentences or resolve ambiguities. In case of automatic speech recognition the language model guides and constrains the search among alternative word hypotheses \cite{glass2013automatic}.

In this chapter, a thorough introduction to statistical language modelling is given. The first section describes the word $n$-gram model, which is still one of the most important tools in speech and language processing. It also presents several smoothing techniques. Section \ref{section:ann} is dedicated to the connectionist models based on artificial neural networks. The final section contains an overview of evaluation methods for language models.
\section{N-gram models}
\label{section:ngrams}
In Section \ref{subsection:lm} we defined the chain rule of probability, which lets us decompose the joint probability of a sequence of $N$ words into a product of conditional probabilities:
\begin{equation}
	P(w_{1}, \dots, w_{n})=\prod_{i=1}^{n}P(w_{i}|w_{1},\dots,w_{i-1})=\prod_{i=1}^{n}P(w_{i}|h_{i})
\end{equation}
However, there is no way of computing the probability of a word given a long history of preceding words. Estimating it from relative frequency counts is infeasible, if only for data sparsity reason. We can deal with this problem by using an $n$-gram model. An $n$-gram is simply a sequence of $n$ words (or other modelling units) -- an $n$-gram of size 1 is called a unigram, size 2 is a bigram, and size 3 is called a trigram. The $n$-gram model approximates the probability of a word given all the previous words with the probability of the word given $n-1$ preceding words:
\begin{equation}
	P(w_{i}|w_{1},\dots,w_{i-1})\approx P(w_{i}|w_{i-n+1},\dots,w_{i-1})
\end{equation}
The $n$-gram can be interpreted as a left-to-right Markov chain of order $n-1$, because the probability of the current state depends only on $n-1$ previous states. To put it differently, the $n$-gram models satisfies the $n-1$ order Markov assumption (see Section \ref{subsection:acoustic}). In contrast to \glspl{hmm}, the states of Markov chains are directly visible.
\subsection{Maximum likelihood estimation}
\label{subsection:mle}
The most common technique of estimating the $n$-gram probabilities from a text corpus is \gls{mle}. The procedure is very simple - the probability of a particular $n$-gram is estimated by dividing its observed frequency by the frequency of all $n$-grams that share the same prefix of size $n-1$:
\begin{equation}
	P(w_{i}|w_{i-n+1}, \dots, w_{i-1})=\frac{C(w_{i-n+1}, \dots, w_{i-1}, w_{i})}{\sum_{w}C(w_{i-n+1}, \dots, w_{i-1}, w)}
\end{equation}


\subsection{Word n-grams}
So far, while describing the $n$-gram models, we used the terms ``word'' and ``modeling unit'' interchangeably. However, depending on the application, $n$-grams can use sub-lexical units, such as phonemes, letters, and syllables, or supra-lexical units, such as \gls{pos} tags. In case of \gls{asr}, words are the most popular choice for the modeling units, because word-based models can be easily incorporated into the search process described in section \ref{subsection:lm}. For example, consider the following sequence:
\begin{center}
\texttt{open the pod bay doors}  
\end{center}
We would like to estimate the probability of next word being \texttt{hal}. Using a bigram model, we get:
\begin{equation}
	P(\texttt{hal}|\texttt{open the pod bay doors}) \approx P(\texttt{hal}|\texttt{doors})
\end{equation}
Truncating the history to $n-1$ units drastically reduces the number of free parameters. Although this number is still enormous - for a bigram model and a vocabulary of 50000 words, there are potentially two and a half billion parameters -- it is at least feasible to estimate the probability of the entire sequence:
\begin{multline}
	P(\texttt{open the pod bay door hal}) \approx \\
	\approx P(\texttt{open}|\texttt{<s>})P(\texttt{the}|\texttt{open})P(\texttt{pod}|\texttt{the})P(\texttt{bay}|\texttt{pod})P(\texttt{door}|\texttt{bay})P(\texttt{hal}|\texttt{door})P(\texttt{</s>}|\texttt{hal}),
\end{multline}
where <s> and </s> are start-of sentence and end-of-sentence markers respectively.
There are several disadvantages of word-based $n$-gram models. The first one is the obviously unrealistic Markov's assumption. Language dependencies often span across far more than just three or four words, especially in inflected languages. Consequently, low-order word-based $n$-grams may do a poor job at disambiguating grammatically invalid sentences, because an incorrect sentence can be constructed from a sequence of valid $n$-grams, when the $n$ is small. Take this quote from Yoda as an example:
\begin{center}
	\texttt{look I so old to young eyes.}
\end{center}
A bigram model would likely assign a relatively high probability to this phrase, because each pair of consecutive words is entirely plausible. 
In the above example, using a trigram model would probably yield better results, but at a considerable cost - even for a lexicon of 50 000 words (far too few for modeling any inflected language), switching from bigram to trigram introduces more than one hundred trillions potential parameters! Several techniques of dealing with the \gls{oov} words are available (see Section \ref{subsection:smoothing}), but there is always a tradeof between the quality of predictions and the amount of data required to calculate the probability estimates. Training and storing higher-order word $n$-grams can be simply infeasible due to data sparsity and memory constraints.
Another issue with word-based $n$-grams is that the truncation of word history may lead to unintuitive or inconsistent behaviour. This problem is especially pronounced in case of languages with weak constraints on word order -- permutations of the same sequence may be assigned different probability estimates despite being grammatically and semantically equivalent. Furthermore, very similar word histories may lead to different predictions. Consider these two sequences:
\begin{center}
	\texttt{a brilliant american mathematician who graduated from} \\
	\texttt{a brilliant american mathematician who graduated quite recently from}.
\end{center}
From the point of view of predicting the next word, these two histories are very similar. However, they would be considered completely different, even by a trigram word model \cite{whittaker2000statistical}. Furthermore, in the second case, we would need at least a $5$-gram model to capture the intuition that the next word is probably a name of a university.

Although word-based $n$-gram language models have been outperformed by deep neural networks in terms of perplexity and word error reduction rate, they are still commonly used in \gls{asr} systems, as they strike a fine balance between effectiveness and simplicity. They are straightforward to train and can be easily used to guide the search through the lattice of word hypotheses. However, in case of inflected languages, the problem of data sparsity is significantly amplified because and therefore using words as the modeling unit is not necesarily the best choice.
\subsection{Class-based $n$-gram models}
\section{Neural network models}
\label{section:ann}
\section{Evaluation of language models}
\label{section:evaluation}
