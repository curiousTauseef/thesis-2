\chapter{Statistical language models}
\label{chapter:lm}

\section{N-gram models}
\label{section:ngrams}
In Section \ref{subsection:lm} we defined the chain rule of probability, which lets us decompose the joint probability of a sequence of $N$ words into a product of conditional probabilities. Let $w_{1}^{n}$ denote $w_{1}, \dots, w_{n}$. The chain rule can be formulated as
\begin{equation}
	\label{equation:chain}
	P(w_{1}^{n})=\prod_{i=1}^{n}P(w_{i}|w_{1}^{i-1})=\prod_{i=1}^{n}P(w_{i}|h_{i}).
\end{equation}
However, there is no way of computing the probability of a word given a long history of preceding words. Estimating it from relative frequency counts is infeasible, if only for data sparsity reason. We can deal with this problem by using an \mbox{$n$-gram} model. An \mbox{$n$-gram} is simply a sequence of $n$ words (or other modelling units)~--~an \mbox{$n$-gram} of size 1 is called a unigram, size 2 is a bigram, and size 3 is called a trigram. The \mbox{$n$-gram} model approximates the probability of a word given all the previous words with the probability of the word given $n-1$ preceding words:
\begin{equation}
	P(w_{i}|w_{1}^{i-1})\approx P(w_{i}|w_{i-n+1}^{i-1}).
\end{equation}
The \mbox{$n$-gram} can be interpreted as a left-to-right Markov chain of order $n-1$, because the probability of the current state depends only on $n-1$ previous states. To put it differently, the \mbox{$n$-gram} models satisfies the $n-1$ order Markov assumption (see Section \ref{subsection:acoustic}). In contrast to \glspl{hmm}, the states of Markov chains are directly visible.

Another way to look at the \mbox{$n$-gram} model is through the notion of history equivalence class, introduced in Section \ref{subsection:lm}. As indicated previously, it is impossible to obtain a probability estimate for every imaginable word history, but the number of parameters can be reduced by classifying word histories into equivalence classes. Indeed, in \cite{jelinek1997statistical}, language modelling is defined as the task of finding the best history classification function:
\begin{equation}
	\Phi:h_{i}\mapsto\varphi_{i}=\Phi(h_{i})=\Phi(w_{1}^{i-1})
\end{equation}
Finding an optimal history classification function is the matter of striking a fine balance between its predictive power and complexity. To be a useful predictor, an equivalence class has to maximise the information about next word, while minimising the dimensionality of the resulting model. In case of \mbox{$n$-gram} models, the history classification function $\Phi$ simply limits the word history to the last $n-1$ elements:
\begin{equation}
	\Phi(w_{1}^{i-1})=w_{i-n+1}^{i-1}.
\end{equation}
Two word histories are therefore equivalent if their last $n-1$ elements are identical. The trade-off between efficiency and complexity quickly becomes apparent when the value of $n$ is \mbox{increased~--~higher} order \mbox{$n$-grams} typically exhibit lower perplexity (see Section \ref{subsection:perplexity}) and better performance, but are more expensive in terms of time and memory.
\subsection{Maximum Likelihood Estimation}
\label{subsection:mle}
The most common technique of estimating the \mbox{$n$-gram} probabilities from a text corpus is \gls{mle}. The procedure is very simple~--~the \mbox{$n$-gram} probability of a word $w_{i}$ given the truncated history $\varphi_{i}=w_{i-n+1}^{i-1}$ is the observed frequency of the \mbox{$n$-gram} $w_{i-n+1}^{i}$ normalised by the sum of observed frequencies of all \mbox{$n$-grams} sharing the same history:
\begin{equation}
	\label{equation:mle}
	P(w_{i}|\varphi)=\frac{C(\varphi, w_{i})}{\sum_{w_{j}}C(\varphi, w_{j})}.
\end{equation}
Note that the sum in the denominator is equal simply to $C(\varphi_{i})$, so the formula in Equation \ref{equation:mle} can be more intuitively understood as the proportion of cases in which a particular equivalent word history $\varphi_{i}$ is followed by the word $w_{i}$:
\begin{equation}
	\label{equation:mle2}
	P(w_{i}|\varphi_{i})=\frac{C(\varphi_{i}, w_{i})}{C(\varphi_{i})}.
\end{equation}
This ratio is called the relative frequency. In the special case of unigram probability it can be calculated as the count of the particular word $w_{i}$ normalised by the size of the corpus:
\begin{equation}
	\label{equation:mle2}
	P(w_{i})=\frac{C(w_{i})}{\sum_{w}C(w)}.
\end{equation}
\begin{table}[h!]
	\caption[Calculating selected bigram and trigram probabilities using MLE]{Calculating selected bigram and trigram probabilities using maximum likelihood estimation \cite{jurafsky2000speech}. The \texttt{<s>} and \texttt{</s>} tags denote the beginning and the end of the sentence.}
	\label{table:mle}
	\texttt{%
		\begin{tabular}{c}
			<s> egg bacon and spam </s> \\
			<s> egg bacon sausage and spam </s> \\
			<s> spam bacon sausage and spam </s> \\
			<s> spam egg spam spam bacon and spam </s> \\
			\\
		\end{tabular}}
		\centering
		\begin{tabular*}{.8\linewidth}{@{\extracolsep{\fill}}lll}
			$P(\texttt{bacon}|\texttt{spam})=\frac{2}{8}$ & $P(\texttt{spam}|\texttt{spam})=\frac{1}{8}$ & $P(\texttt{egg}|\texttt{<s> <s>})=\frac{2}{4}$  \\
			$P(\texttt{bacon}|\texttt{egg})=\frac{2}{3}$  & $P(\texttt{spam}|\texttt{<s>})=\frac{2}{4}$ & $P(\texttt{egg}|\texttt{bacon and})=0$ \\
			$P(\texttt{bacon}|\texttt{and})=0$            & $P(\texttt{spam}|\texttt{and})=1$ & $P(\texttt{</s>}|\texttt{and spam})=1$ \\
		\end{tabular*}
	\end{table}
Table \ref{table:mle} shows an example of calculating \mbox{$n$-gram} probabilities using a corpus of four phrases from Monty Python's notorious spam sketch. Inspecting it reveals a major problem with the \gls{mle} approach. Note that some probability estimates are equal to one, so using the model to generate random sentences would in many cases result in phrases taken verbatim from the corpus. Even worse, there are also probability estimates equal to zero. This means that because of the chain rule, the model will assign a zero probability to any sequence containing a known word in a new context. This includes one of the previous lines of the sketch:
	\begin{equation}
		P(\texttt{bacon}|\texttt{and})=0 \Rightarrow P(\texttt{<s> egg and bacon </s>})=0
	\end{equation}
	Both these artifacts are a consequence of data sparsity~--~the corpus is simply too small for the model to be an accurate representation of the language. The probability of observed items is overestimated, while the probability of unobserved items is underestimated. Although the example is obviously exaggerated, the problem persists in case of full-sized corpora. There are many techniques of correcting this bias by shifting the probability mass from frequent to previously unseen items. This process, called smoothing or discounting, is described in Section \ref{subsection:smoothing}. 

	Zero probability is one problem, but there is also an issue of words that do not appear in the corpus at all. In speech recognition, \gls{oov} tokens are inevitable and they need to be identified, because they contribute to recognition errors in surrounding words. More importantly, they are often information-rich nouns, such as proper names, domain-specific notions, or foreign words. Language models can be used to facilitate the process of \gls{oov} token detection by incorporating the information about unknown words into the training process. All words that appear in the \gls{lm} training data, but do not appear in the \gls{asr} vocabulary, are simply substituted by the unknown word token \texttt{<ign>}. These pseudo-words are then treated like any other regular word in the corpus, similarly to \texttt{<s>} and \texttt{</s>}. 

	Another conclusion that can be drawn from the example in Table \ref{table:mle} is that the model is only as representative as the corpus it is trained on. This is especially important in \gls{lvcsr}, where the model not only has to represent non-domain-specific vocabulary, but also focus on spoken language, which is often very different from writing. \gls{asr} systems are often trained on written texts, because this kind of data is usually readily available, but using speech transcripts can lead to better results with less training data \cite{dziadzio2015comparison}. This idea is further explored in the experimental part of the thesis.
	\subsection{Smoothing}
	\label{subsection:smoothing}
	Smoothing is a way of dealing with the zero probability problem. In principle, it is the process of adjusting the \gls{mle} estimates to produce more accurate probabilities \cite{chen1996empirical}. High probabilities are adjusted downwards and low probabilities are adjusted upwards, so the resulting distribution is more uniform. The simplest method of smoothing is additive smoothing. The idea is to add a constant $\delta$ to every \mbox{$n$-gram} count:
	\begin{equation}
		P_{\textsc{add}}(w_{i}|\varphi{i})=\frac{\delta+C(\varphi_{i}w_{i})}{\delta|V|+C(\varphi_{i})},
	\end{equation}
where typically $0 < \delta \leq 1$. This method is easy to implement, but has been shown in \cite{Gale94what'swrong} to generally perform poorly.
	\subsubsection*{Katz back-off}
	Katz back-off builds on the Good-Turing estimate, based on the \textit{symmetry requirement} which states that two events which occur the same number of times in the sample must have equal probabilities \cite{whittaker2000statistical}. The idea is to adjust the count of \mbox{$n$-grams} that occur $c$ times using the counts of \mbox{$n$-grams} that occur $c+1$ times~--~in particular, to estimate the probability of unseen \mbox{$n$-grams} using the singleton counts. Let $n_{c}$ denote the number of \mbox{$n$-grams} that occur $c$ times in the sample. For each count $c$, an adjusted count $\hat{c}$ is computed:
	\begin{equation}
		\hat{c}=(c+1)\frac{n_{c+1}}{n_{c}}.
		\label{equation:gt}
	\end{equation}
	From Equation \ref{equation:gt} it follows that the total number of counts that will be assigned to \mbox{$n$-grams} with zero counts is equal to the number of singletons. The updated probability estimate can be calculated by normalising the updated count by the total number of tokens. For an \mbox{$n$-gram} $\alpha$ with $c_{\alpha}$ counts:
	\begin{equation}
		P_{\textsc{good}}(\alpha:C(\alpha)=c_{\alpha})=\frac{\hat{c}_{\alpha}}{\sum_{c}cn_{c}}.
	\end{equation}

	The problem with using plain Good-Turing estimates for discounting is that $n_{c+1}$ quite often equals zero for high $c$. Katz back-off extends the idea behind Good-Turing estimation by using lower-order distributions to better reallocate the count mass subtracted from nonzero counts. For example, while adjusting the counts of bigrams, the unigram distribution is used: 
	\begin{equation}
		C_{\textsc{katz}}(w_{i-1}, w_{i})=
		\begin{cases}
			d_{c}c & \text{if } c>0 \\
			\alpha(w_{i-1})P_{\textsc{mle}}(w_{i}) & \text{if } c=0.
		\end{cases}
	\end{equation}
	The discount coefficient $d_{c}$ depends on the the \mbox{$n$-gram} count $c$. Counts larger than some arbitrary threshold $k$ are not discounted ($d_{c}=1$). The value of $d_{c}$ when $c\leq k$ is chosen so that the resulting discount is proportional to the Good-Turing discount: 
	\begin{equation}
		1-d_{c}=\mu(1-\frac{\hat{c}}{c}).
		\label{equation:constraint1}
	\end{equation}
	Furthermore, the total number of counts discounted in the \mbox{$n$-gram} distribution should be equal to the total number of counts assigned to zero-count \mbox{$n$-grams} according to the Good Turing estimate (see Equation \ref{equation:gt}):
	\begin{equation}
		\sum_{c=1}^{k}n_{c}(1-d_{c})c=n_{1}.
		\label{equation:constraint2}
	\end{equation}
	The only solution that satisfies the constraints formulated in Equation \ref{equation:constraint1} and \ref{equation:constraint2} is given by:
	\begin{equation}
		d_{c}=\frac{\frac{\hat{c}}{c}-\frac{(k+1)n_{k+1}}{n_{1}}}{1-\frac{(k+1)n_{k+1}}{n_{1}}}.
		\label{equation:dc}
	\end{equation}
	Katz back-off for higher-order \mbox{$n$-gram} is defined recursively, where the unigram model is taken to be the \gls{mle} unigram model to end the recursion \cite{whittaker2000statistical}. The updated probability $P_{\textsc{katz}}$ of word~$w_{i}$ given truncated history $w_{i-n+1}^{i-1}$ is given by:
        \begin{equation}
        P_{\textsc{katz}}(w_{i}|w_{i-n+1}^{i-1})=
        \begin{cases}
           \frac{C(w_{i-n+1}^{i})}{C(w_{i-n+1}^{i-1})}& \text{if }C(w_{i-n+1}^{i}) > k\\
           d_{C(w_{i-n+1}^{i})}\cdot \frac{C(w_{i-n+1}^{i})}{C(w_{i-n+1}^{i-1})}& \text{if } 1\leq C(w_{i-n+1}^{i})\leq k\\
           \alpha(w_{i-n+1}^{i-1})\cdot P_{\textsc{katz}}(w_{i}|w_{i-n+2}^{i-1})& \text{if } C(w_{i-n+1}^{i})=0,
        \end{cases}
        \end{equation}
	where:
        \begin{equation}
            \alpha(w_{i-n+1}^{i-1})=\frac{1-\sum_{w_{i}:C(w_{i-n+1}^{i})>0}P_{\textsc{katz}}(w|w_{i-n+1}^{i-1})}{\sum_{w_{i}:C(w_{i-n+1}^{i})=0}P_{\textsc{katz}}(w|w_{i-n+2}^{i})}.
        \end{equation}
        Note that the value of $\alpha$ is chosen so that the total number of counts in the distribution is unchanged. For details on the derivation of the back-off weight~$\alpha$ and the discount coefficient~$d_{c}$, see \cite{chen1996empirical}.
	\subsubsection*{Kneser-Ney smoothing}
	The Kneser-Ney smoothing is an extension of absolute discounting, which involves subtracting a fixed discount $\delta \in (0, 1)$ from each nonzero count:
	\begin{equation}
		P_{\textsc{abs}}(w_{i}|w_{i-n+1}^{i-1})= \frac{max\{C(w_{i-n+1}^{i})-\delta, 0\}}{\sum_{w_{i}}C(w_{i-n+1}^{i})}+(1-\lambda_{w_{i-n+1}^{i}})P_{\textsc{abs}}(w_{i}|w_{i-n+2}^{i-1}).
	\end{equation}
	The parameter $\lambda$ is taken so that the resulting distribution sums to one:
	\begin{equation}
		1-\lambda_{w_{i-n+1}^{i-1}}=\frac{\delta}{\sum_{w_{i}}C(w_{i-n+1}^{i})}N_{1+}(w_{i-n+1}^{i-1}~\bullet).
	\end{equation}
	The expression $N_{1+}(\varphi~\bullet)$, denotes the number of unique words that follow the history $\varphi$:
	\begin{equation}
		N_{1+}(w_{i-n+1}^{i-1}~\bullet)=|\{w_{i}:C(w_{i-n+1}^{i}) > 0\}.
	\end{equation}
	The parameter $\delta$ is chosen using held-out estimation. In \cite{ney1994structuring}, it is estimated as 
	\begin{equation}
		\delta=\frac{n_{1}}{n_{1}+2n_{2}}.
	\end{equation}
	The idea behind Kneser-Ney smoothing is that since the lower-order model is only necessary when the count is small or zero in the higher-order model, it should be optimised for that purpose. In most other algorithms, the lower-order distribution is just a smoothed version of the MLE distribution. In Kneser-Ney smoothing, the unigram probability is not proportional to the number of occurrences of the word, but the number of words it follows:
	\begin{equation}
		P_{\textsc{kn}}(w_{i})=\frac{N_{1+}(\bullet~w_{i})}{N_{1+}(\bullet~\bullet)},
		\label{equation:kneser-ney}
	\end{equation}
	where $N_{1+}(\bullet~w_{i})$ is the number of different words that precede $w_{i}$ in the training data and $N_{1+}(\bullet~\bullet)$ is the number of unique bigrams with nonzero count:
	\begin{equation}
		N_{1+}(\bullet~w_{i})=|{w_{i-1}:C(w_{i-1}, w_{i})>0}|,
	\end{equation}
	\begin{equation}
		N_{1+}(\bullet~\bullet)=|{(w_{i-1}, w_{i}):C(w_{i-1}, w_{i})>0}|.
	\end{equation}
	The general formula for the unmodified Kneser-Ney smoothing is
	\begin{equation}
		P_{\textsc{kn}}(w_{i}|w_{i-n+1}^{i-1})= \frac{max\{C(w_{i-n+1}^{i})-\delta, 0\}}{\sum_{w_{i}}C(w_{i-n+1}^{i})}+(1-\lambda_{w_{i-n+1}^{i}})\frac{N_{1+}(\bullet~w_{i-n+2}^{i})}{N_{1+}(\bullet~w_{i-n+2}^{i-1}~\bullet)},
	\end{equation}
	where:
	\begin{equation}
		N_{1+}(\bullet~w_{i-n+2}^{i})=|{w_{i-n+1}:C(w_{i-n+1}^{i})>0}|,
	\end{equation}
	\begin{equation}
		N_{1+}(\bullet~w_{i-n+2}^{i-1}~\bullet)=|{w_{i-n+1, w_{i}}:C(w_{i-n+1}^{i})>0}|.
	\end{equation}
	The Chen and Goodman's modification of the original Kneser-Ney algorithm uses three different discount values, $\delta_{1}$, $\delta_{2}$, $\delta_{3}$, that are applied to \mbox{$n$-grams} with one, two, and three or more counts, respectively \cite{chen1996empirical}. While estimating $P_{\textsc{kn}}(w_{i}|w_{i-n+1}^{i-1})$, the parameter $\delta$ simply becomes a function of $C(w_{i-n+1}^{i-1})$: 
	\begin{equation}
		D(c)=	
		\begin{cases}
			0 & \text{if } c=0\\
			\delta_{1} & \text{if } c=1\\
			\delta_{2} & \text{if } c=2\\
			\delta_{3+} & \text{if } c\geq3.\\
		\end{cases}
	\end{equation}
	The distribution must still sum to one, so the new value of $\alpha$ is:
	\begin{equation}
		\alpha(w_{i-n+1}^{i-1})=\frac{\delta_{1}N_{1}(w_{i-n+1}^{i-1}~\bullet)+\delta_{2}N_{2}(w_{i-n+1}^{i-1}~\bullet)+\delta_{3+}N_{3+}(w_{i-n+1}^{i-1}~\bullet)}{\sum_{w_{i}}C(w_{i-n+1}^{i})}.
	\end{equation}
	The modified version has been shown to significantly outperform the original Kneser-Ney algorithm, because the optimal average discount for \mbox{$n$-grams} with one or two counts is different from the optimal average discount for \mbox{$n$-grams} with higher counts. Chen and Goodman provide estimates of the optimal values for $\delta_{1}$, $\delta_{2}$, and $\delta_{3}$:
	\begin{align}
		Y&=\frac{n_{1}}{n_{1}+2n_{2}} \nonumber\\ 
		\delta_{1}&=1-2Y\frac{n_{2}}{n_{1}} \nonumber\\
		\delta_{2}&=2-3Y\frac{n_{3}}{n_{2}} \nonumber\\
		\delta_{3+}&=3-4Y\frac{n_{4}}{n_{3}}.
	\end{align}
	\subsection{Word n-grams}
	So far, while describing the \mbox{$n$-gram} models, we used the terms ``word'' and ``modeling unit'' interchangeably. However, depending on the application, \mbox{$n$-grams} can use \mbox{sub-lexical} units, such as phonemes, letters, and syllables, or \mbox{supra-lexical} units, such as \gls{pos} tags. In case of \gls{asr}, words are the most popular choice for the modeling units, because \mbox{word-based} models can be easily incorporated into the search process described in section \ref{subsection:lm}. For example, consider this famous quote:
	\begin{center}
		\texttt{open the pod bay doors}.
	\end{center}
	We would like to estimate the probability of next word being \texttt{hal}. Using a bigram model, we get:
	\begin{equation}
		P(\texttt{hal}|\texttt{open the pod bay doors}) \approx P(\texttt{hal}|\texttt{doors}).
	\end{equation}
	Truncating the history to $n-1$ units drastically reduces the number of free parameters. Although this number is still enormous~--~for a bigram model and a vocabulary of 50000 words, there are potentially two and a half billion parameters~--~it is at least feasible to estimate the probability of the entire sequence using the chain rule from Equation \ref{equation:chain}:
	\begin{multline}
		P(\texttt{<s> open the pod bay door hal </s>}) \approx \\
		\approx P(\texttt{open}|\texttt{<s>})P(\texttt{the}|\texttt{open})P(\texttt{pod}|\texttt{the})P(\texttt{bay}|\texttt{pod})P(\texttt{door}|\texttt{bay})P(\texttt{hal}|\texttt{door})P(\texttt{</s>}|\texttt{hal}).
	\end{multline}
	There are several disadvantages of \mbox{word-based} \mbox{$n$-gram} models. The first one is the obviously unrealistic Markov's assumption. Language dependencies often span across far more than just three or four words, especially in inflected languages. Consequently, \mbox{low-order} \mbox{word-based} \mbox{$n$-grams} may do a poor job at disambiguating grammatically invalid sentences, because an incorrect sentence can be constructed from a sequence of valid short \mbox{$n$-grams}. Take this quote from Yoda as an example:
	\begin{center}
		\texttt{<s> if no mistake you have made losing you are </s>}
	\end{center}
	A bigram model would likely assign a relatively high probability to this phrase, because each pair of consecutive words is entirely plausible. 
	In the above example, using a trigram model would probably yield better results, but at a considerable cost~--~even for a lexicon of 50 000 words (far too few for modelling any inflected language), switching from bigram to trigram introduces more than one hundred trillions potential parameters! In short, there is always a \mbox{trade-off} between the quality of predictions and the amount of data required to calculate the probability estimates. Training and storing \mbox{higher-order} word \mbox{$n$-grams} can be simply infeasible due to data sparsity and memory constraints.
	Another issue with word-based \mbox{$n$-grams} is that the truncation of word history may lead to unintuitive or inconsistent behaviour. This problem is especially pronounced in case of languages with weak constraints on word order~--~permutations of the same sequence may be assigned different probability estimates despite being grammatically and semantically equivalent. Furthermore, very similar word histories may lead to different predictions. Consider these two sequences:
	\begin{center}
		\texttt{she graduated from} \\
		\texttt{she graduated very recently from}.
	\end{center}
	From the point of view of predicting the next word, these two histories are very similar. However, they would be considered completely different by a trigram word model \cite{whittaker2000statistical}. In the second case, we would need at least a $5$-gram model to capture the intuition that the next word is probably a name of a university.

	Although \mbox{word-based} \mbox{$n$-gram} language models have been outperformed by deep neural networks in terms of perplexity and word error rate, they are still commonly used in \gls{asr} systems, as they strike a fine balance between effectiveness and simplicity. They are straightforward to train and can be easily used to guide the search through the lattice of word hypotheses. However, in case of inflected languages, the problem of data sparsity is significantly amplified and therefore using words as the modeling unit is not necessarily the best choice.
	\subsection{Class-based n-gram models}
	\label{subsection:class}
	Class-based models address the problem of data sparsity by reducing the number of parameters. The idea is to cluster words with similar statistical distributions or linguistic properties into groups. A class \mbox{$n$-gram} model with a vocabulary of size $V$ and $C$ classes has $V-C$ word emission parameters and $C^{n}-1$ independent \mbox{$n$-gram} parameters \cite{brown1992class}. Class-based models always have fewer parameters than analogous word-based models, and therefore their parameters can be more accurately estimated, as most classes will be well represented in the corpus. Class \mbox{$n$-gram} models are constructed in a similar fashion as word-based models, except that words are mapped to equivalence classes: 
	\begin{equation}
		\pi:w_{i}\mapsto \gls{mapping}.
		\label{equation:deterministic_class}
	\end{equation}
	The class mapping $\pi$ can be deterministic or probabilistic. An example of a deterministic mapping is the \texttt{<ign>} token described in Section \ref{subsection:mle}~--~a word either appears in the vocabulary or not, so the mapping is unambiguous. In contrast, clustering based on linguistic knowledge is often ambiguous, because the same word can belong to different grammatical categories, depending on the context. 
	In case of a deterministic mapping, the probability of a word given its history can be calculated as the product of the probability of a particular word given its class (word emission probability) and the probability of a certain class given a history of $n-1$ classes:
	\begin{equation}
		P_{\textsc{class}}(w_{i}|w_{i-n+1}^{i-1})=P(w_{i}|\pi(w_{i}))P(\pi(w_{i})|\pi(w_{i-n+1}), \dots, \pi(w_{i-1})).
		\label{equation:classngram}
	\end{equation}
	The class \mbox{$n$-gram} probabilities can be calculated from the corpus using \gls{mle}, similarly to word \mbox{$n$-gram} probabilities. The word emission probability is often dropped, but can be otherwise estimated as the relative frequency of the form:
	\begin{equation}
		P(w_{i}|c_{i})=\frac{C(w_{i})}{C(\pi(w_{i}))}.
		\label{equation:emission_probability}
	\end{equation}
	In case of a probabilistic mapping, there are multiple realisations of the same word history, as each word can belong to several classes. Therefore, the prediction of the current word requires a summation over the probabilities of all the possible realisations \cite{ney1994structuring}.
	The methods for finding the class mapping function fall roughly into two categories~--~\mbox{knowledge-based} and \mbox{data-driven}. The former approach takes advantage of prior linguistic knowledge. Examples include clustering based on \gls{pos} tags or semantic functions. This method usually requires additional resources such as tagged corpora, wordnets, or automatic morphological taggers. The \mbox{data-driven} clustering generally uses a greedy algorithm to automatically cluster words in such a way that the perplexity of the corpus is minimised. The \mbox{class-based} models used in this work are deterministic, \mbox{knowledge-based} morphosyntactic models.
	\section{Neural network models}
	\label{section:rnn}
	For decades, n-grams have been the standard approach to statistical language modelling. Although many alternative techniques were proposed, the improvements in performance usually came at the cost of computational complexity \cite{mikolov2011rnnlm}. The neural network based language models were introduced in \cite{bengio2003neural} and motivated by the observation that the \mbox{word-based} \mbox{n-gram} models have two major setbacks~--~they are unable to capture longer contexts and they ignore the similarity between words. 
	\subsection{The feedforward architecture}	
The neural models use a distributed representation to deal with the high dimensionality of the training data. The idea is to represent each word as a \mbox{real-valued} vector in $\mathbb{R}^{m}$ and then express the probability function of word sequences in terms of these vectors. The probability function is a smooth function of the word representations, so these kind of models generalize much better to unknown sequences. Because the word embedding $W$: $words \mapsto \mathbb{R}^{m}$ has a useful property of clustering together synonyms and words from the same class, it allows to deal with the data sparsity by generalizing every training sentence to a class of similar sentences. The embedding and the parameters of the probability function can be learned simultaneously. In~\cite{bengio2003neural}, a feedforward multi-layer neural network implementing this framework was shown to yield a much better perplexity than the \mbox{Kneser-Ney} back-off \mbox{n-gram} models, but at the cost of high computational complexity and nontrivial implementation.
	\subsection{The recurrent architecture}
	In \cite{mikolov2011extensions}, a recurrent network architecture was shown to outperform the feedforward one. The \glspl{rnn} have a simple implementation and a very useful ability to store information in the hidden layer. The architecture of neural networks used in the experiments is described in \cite{mikolov2010recurrent} and presented in Figure \ref{figure:recurrent}. The input layer is formed by concatenating $w(t)$, a vector representing the current word with $s(t-1)$, the output of the hidden layer from the previous time step:
	\begin{equation}
		x(t)=[w(t)^{T}s(t-1)^{T}]^{T}.
		\label{equation:word_vector}
	\end{equation}

	\begin{figure}[htbp]
		\centering
		\includesvg{rnn}
		\caption[A simple \gls{rnn}]{A simple \gls{rnn} called an Elman network, based on \cite{mikolov2011extensions}}.
		\label{figure:recurrent}
	\end{figure}
	The word vector $w(t)$ has the same size as the vocabulary. It uses \mbox{$1$-of-$N$} coding, meaning that the $i$-th word of the vocabulary is coded by setting the $i$-th element to one and all the other elements to zero \cite{schwenk2005training}. The output layer $y(t)$ has the same dimensionality and represents the probability distribution of the next word. The network is trained using the standard backpropagation algorithm, described in \cite{rumelhart1988learning}. The output of the hidden layer is computed by applying the sigmoid activation function \gls{sigmoid} on the product of the input and the weight matrix $U$:
	\begin{equation}
		s_{j}(t)=f(\sum_{i}x_{i}(t)u_{ji}),
		\label{equation:hidden}
	\end{equation}
where:
\begin{equation}
	f(z)=\frac{1}{1+e^{-z}}.
	\label{equation:sigmoid}
\end{equation}
The values of the output layer are calculated by multiplying the output of the hidden layer by the weight matrix $V$ and applying the softmax function:
\begin{equation}
	y_{k}(t)=g(\sum_{j}s_{j}(t)v_{kj}),
	\label{equation:output}
\end{equation}
The softmax function \gls{softmax} transforms a real-valued vector $z$ into a vector $\sigma$ of real values in the range (0, 1) adding up to one, so that it forms a valid probability distribution:
\begin{equation}
	\sigma(z)_{j}=\frac{e^{z_{j}}}{\sum_{k}e^{z_{k}}}.
	\label{equation:softmax}
\end{equation}

\subsection{Extensions to the RNN model}
The \glspl{rnn} used in the experimental part take advantage of two important techniques~--~\gls{bptt} and output layer factorisation. The former aims to improve the model performance, while the latter allows to speed up the training process. BPTT is an extension of the backpropagation algorithm on the recurrent networks. The network is unfolded by duplicating the recurrent weight for an arbitrary number of time steps $\tau$, and the error is propagated back through an unfolded network, as presented in Figure \ref{figure:unfolding}.

	\begin{figure}[htbp]
		\centering
		\includesvg{unfold}
		\caption[An example of \acrlong{bptt}]{An unfolded network for BPTT with $\tau$=2, based on \cite{boden2002guide}}.
		\label{figure:unfolding}
	\end{figure}

	To speed up the training, one could use a technique analogous to the one described in Section \ref{subsection:class}. Assuming that each word belongs to exactly one class, the probability of the word given its history can be calculated as in equation \ref{equation:classngram}. This reduces the computational complexity from $(1+H)H\tau+HV$ to $(1+H)H\tau+HC$, where $H$ is the size of the hidden layer, $V$ is the size of the vocabulary, $\tau$ is the number of \gls{bptt} steps, and $C$ is the number of classes. That can be a substantial improvement, as the $HV$ term is usually the computational bottleneck and of course $C$ is chosen so that $C\ll V$. However, the neural network architecture allows to extend this idea and assume that the emission probability depends on the hidden layer $s(t)$. Equation \ref{equation:classngram} becomes:
	\begin{equation}
		P_{\textsc{class}}(w_{i}|w_{i-n+1}^{i-1})=P_{0}(w_{i}|\pi(w_{i}), s(t))P_{1}(\pi(w_{i})|s(t)).
		\label{equation:classneural}
	\end{equation}
	Words are assigned to classes using the frequency binning with the number of classes as a parameter. The modified network architecture is presented in Figure \ref{figure:rnnclass}. 

	\begin{figure}[htbp]
		\centering
		\includesvg{rnnclass}
		\caption[A \acrlong{rnn} with a factorised output layer]{RNN with a factorised output layer, based on \cite{mikolov2011extensions}}.
		\label{figure:rnnclass}
	\end{figure}
	
	The probability distribution is first estimated over the classes and then over the words from the class containing the predicted word:
	\begin{equation}
		c_{l}=g(\sum_{j}s_{j}(t)w_{lj})
		\label{equation:classoutput}
	\end{equation}
	\begin{equation}
		y_{c}(t)=g(\sum_{j}s_{j}(t)v_{cj})
		\label{equation:wordoutput}
	\end{equation}

 	\section{Evaluation of language models}
	\label{section:evaluation}
	The best way to evaluate a language model in the context of automatic speech recognition is to simply incorporate it in an existing \gls{asr} system and measure the performance. This approach, known as extrinsic evaluation, is the only way to know whether a particular model will actually improve the recognition rates, but it can be time-consuming, as it requires running the entire system multiple times \cite{jurafsky2000speech}. Moreover, the recognition task is a very complex one, so there are a lot of factors that can affect the performance of the model. It is therefore more practical to use an intrinsic evaluation metric, such as perplexity, which enables to quickly and objectively measure the quality of a model in an application-agnostic manner. This section describes the evaluation methods used in the experimental part.
	\subsection{Entropy and perplexity}
	\label{subsection:perplexity}
	Perplexity is the most common intrinsic metric of language model quality. It can be derived from the notion of entropy. In information theory, an information source is defined as a device which emits symbols from a finite set $V$. Entropy of an information source can be intuitively understood as the measure of its unpredictability or information content, expressed in bits. For an information source independently emitting symbols $x$ with probability $P(x)$ it is defined as
	\begin{equation}
		H=-\sum_{x}P(x)\log_{2}P(x).
		\label{equation:entropy}
	\end{equation}
	Entropy is maximised when all emission probabilities are equal, that is when $P(x)=\frac{1}{|V|}$. This means that a source with entropy H contains the same amount of information as a source emitting symbols from a set of size $2^{H}$ with probability $2^{-H}$.

	Language can be treated as an information source producing sequences of modelling units $w_{1}, \dots, w_{n}$ with probability $P(w_{1}, \dots, w_{n})$, where each token $w_{i}$ is taken from a vocabulary $V$. Because the symbols are not emitted independently, the notion of per-word entropy is used:
	\begin{equation}
		H=-\lim_{n\to\infty}\frac{1}{n}\sum_{w_{1}, \dots, w_{n}}P(w_{1}, \dots, w_{n})\log_{2}P(w_{1}, \dots, w_{n}).
		\label{equation:genentropy}
	\end{equation}
	The Shannon-McMillan-Breiman theorem states that in case of a stationary and ergodic process, it is possible to use one long sequence in the equation \ref{equation:genentropy} instead of the summation over all possible sequences \cite{algoet1988sandwich}. The assumption here is that a very long sequence will contain most of the shorter sequences and their frequencies will correspond to their probabilities \cite{jurafsky2000speech}. The per-word entropy can therefore be approximated by:
	\begin{equation}
		\gls{entropy}\approx\lim_{n\to\infty}-\frac{1}{n}\log_{2}P(w_{1}, \dots, w_{n}).
		\label{equation:approxentropy}
	\end{equation}
	However, the exact probability of a word sequence is unknown, so it is more practical to use the notion of cross-entropy, which enables to replace the actual probability $P(w_{1}, \dots, w_{n})$ with the language model probability estimate $\hat{P}(w_{1}, \dots, w_{n})$. The sequences are generated according to the actual probability distribution, but the \gls{lm} probability estimates are used for the log summation:
	\begin{equation}
		\gls{crossentropy}=-\lim_{n\to\infty}\frac{1}{n}\sum_{w_{1}, \dots, w_{n}}P(w_{1}, \dots, w_{n})\log_{2}\hat{P}(w_{1}, \dots, w_{n}).
		\label{equation:crossentropy}
	\end{equation}
	\mbox{Cross-entropy} can be approximated similarly to \mbox{per-word} entropy, using the Shannon-McMillan-Breiman theorem:
	\begin{equation}
		\hat{H}=-\frac{1}{n}\log_{2}\hat{P}(w_{1}, \dots, w_{n}).
		\label{equation:entropymodel}
	\end{equation}
	The quantity in equation \ref{equation:entropymodel} can be thought of as the average amount of bits that are required to specify a word. The difference between the entropy $H$ and cross-entropy $\hat{H}$ is a measure of the accuracy of a model. Since the true entropy is always less than or equal to the cross-entropy, a model with a lower cross-entropy is a better representation of the language. Perplexity of the model $\hat{P}$ on a test text $W=w_{1}, \dots, w_{n}$ is formally defined as the exponentiation of the cross-entropy:
	\begin{equation}
		PP=\sqrt[n]{\frac{1}{P(w_{1}, \dots, w_{n})}}=\sqrt[n]{\prod_{i=1}^{n}\frac{1}{P(w_{i}|w_{1}, \dots, w_{i-1})}}
		\label{equation:perplexity}
	\end{equation}
	Perplexity is equivalent to the weighted average branching factor of a language. A model with a perplexity of $2^{H}$ is as confused while predicting the next word as if there were on average $2^{H}$ equally probable words.

	Despite its popularity as a measure of language model quality, perplexity is a rather poor method of estimating the actual performance of a language model in an \gls{asr} task. A model with lower perplexity is, in some sense, a better representation of the language, but there is no guarantee it will translate to high recognition accuracy. The main reason behind this is that perplexity does not take into account the acoustic similarities between words, which are crucial in the speech recognition process. A model able to accurately discriminate between acoustically similar words often performs better, even if its perplexity is relatively high. Some alternatives to perplexity have been proposed, such as low-level language model estimates, adversarial evaluation, artificial lattices, or classification of pseudo-negative sentences, but the word error rate is the only reliable method of measuring model performance in an \gls{asr} system \cite{pauls2012large}\cite{smith2012adversarial}.
	\subsection{WER}
	\label{subsection:wer}
	\glsfirst{wer} is a common performance metric for \gls{asr} or machine translation systems. It can be defined as the length normalised \mbox{word-level} Levenshtein distance. More intuitively, it is the minimal quantity of insertions, deletions, and substitutions of words required to convert a hypothesis phrase into the reference phrase, divided by the length of the reference phrase:
	\begin{equation*}
		WER=\frac{S+D+I}{N},
	\end{equation*}
	where $S$ is the number of substitutions, $D$ is the number of deletions, $I$ is the number of insertions and $N$ is the number of words in the reference phrase. All of these numbers are non-negative, so the minimal value of WER is zero precisely when the hypothesis and the reference are identical. However, there is no upper bound, as the number of insertions can theoretically be arbitrary. \gls{wer} is often expressed as a percentage. It is common to assign different weights to insertions, deletions, and substitutions, but for all \gls{wer} values in this paper they were weighted equally. Table \ref{table:werr} illustrates the WER calculation. 

\begin{table}[h!]
	\caption[An example of WER calculation]{An example of \gls{wer} calculation for a reference phrase \textit{let's recognize speech}}.
  \label{table:werr}
    \centering
    \begin{tabular*}{.6\linewidth}{@{\extracolsep{\fill}}lrrrr}
	        hypothesis                  & S & D & I & WER\\
		\midrule
		lets reckon a nice beach    & 3 & 2 & 0 & $\frac{5}{3}$\\
		lets wreck a nice speech    & 2 & 2 & 0 & $\frac{4}{3}$\\
		let us wreck a nice beach   & 3 & 3 & 0 & $\frac{6}{3}$\\
		let's recognize             & 0 & 0 & 1 & $\frac{1}{3}$\\
    \end{tabular*}
\end{table}

Word error rate is commonly used as a speech recognition accuracy metric, although it has been shown that in some applications it is not necessarily the best choice. For example, optimising the parameters of the \gls{asr} component of a \gls{st} system by translation metrics has been shown to lead to greater translation quality, despite a decrease in WER \cite{he2011word}. For the purpose of this thesis, the absolute \gls{wer} reduction (WERR) is used as an evaluation score for language models. WERR is defined as a reduction of \gls{wer} before and after applying the model.

	\subsection{N-best list rescoring}
	One way of calculating the \gls{werr} of a language model is the n-best list rescoring framework. An n-best list is simply a list of $n$ hypotheses that are most probable according to the acoustic model, along with their probabilities. The hypotheses are \mbox{re-ranked} by incorporating the language model information \cite{broman2005methods}. For the purpose of this work, the acoustic model probability ($P_{\textsc{am}}$) was combined with the language model probability ($P_{\textsc{lm}}$) using the log-linear interpolation:
	\begin{equation}
		P_{\textsc{re}}(h)\propto P_{\textsc{am}}^{(1-\alpha)}P_{\textsc{lm}}^{\alpha}.
	\end{equation}
	Table \ref{table:nbest} shows an example of n-best list rescoring. The \gls{werr} of the language model can be calculated as the difference between the \gls{wer} before and after rescoring. The \gls{wer} can be calculated as the \gls{wer} of the best hypothesis, as shown in equation \ref{equation:WERR}, or a weighted average over a set of hypothesis, with \gls{am} or \gls{lm} probabilities as weights, but in the context of an \gls{asr} system, the first method is a more natural evaluation metric.

\begin{table}[h!]
  \caption[N-best list rescoring]{N-best list rescoring with $\alpha=0.6$.}
  \label{table:nbest}
    \centering
    \begin{tabular*}{.6\linewidth}{@{\extracolsep{\fill}}lrrrr}
	    hypothesis & $P_{\textsc{am}}$ & $P_{\textsc{lm}}$ & $P_{\textsc{re}}$ & WER\\
	        \midrule
		lets reckon a nice beach    & 0.5 & 0.2 & 0.32 & $\frac{5}{3}$\\
		lets wreck a nice speech    & 0.4 & 0.1 & 0.22 & $\frac{4}{3}$\\
		let us wreck a nice beach   & 0.4 & 0.3 & 0.34 & $\frac{6}{3}$\\
		let's recognize             & 0.3 & 0.6 & 0.48 & $\frac{1}{3}$\\
    \end{tabular*}
\end{table}

	\begin{align*}
		\label{equation:WERR}
		&WER_{1}=WER(\argmax{P_{\textsc{am}}})=\frac{5}{3} \nonumber\\
		&WER_{2}=WER(\argmax{P_{\textsc{re}}})=\frac{1}{3} \nonumber\\
		&WERR =  WER_{1} - WER_{2}\approx133.33\%
	\end{align*}
