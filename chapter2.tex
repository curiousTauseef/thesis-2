\chapter{Statistical language models}
\label{chapter:lm}
Language modelling lies at the core of many natural language processing tasks, such as speech recognition and synthesis, document classification, grammar and spelling checking, parsing, machine translation, and information retrieval. In general, the task of statistical language models is to estimate the likelihood of a sequence of words. This information can then be used to reject invalid sentences or resolve ambiguities. In case of automatic speech recognition the language model guides and constrains the search among alternative word hypotheses \cite{glass2013automatic}.

In this chapter, a thorough introduction to statistical language modelling is given. The first section describes the word $n$-gram model, which is still one of the most important tools in speech and language processing. It also presents several smoothing techniques. Section \ref{section:ann} is dedicated to the connectionist models based on artificial neural networks. The final section contains an overview of evaluation methods for language models.
\section{N-gram models}
\label{section:ngrams}
In Section \ref{subsection:lm} we defined the chain rule of probability, which lets us decompose the joint probability of a sequence of $N$ words into a product of conditional probabilities:
\begin{equation}
	P(w_{1}, \dots, w_{n})=\prod_{i=1}^{n}P(w_{i}|w_{1},\dots,w_{i-1})=\prod_{i=1}^{n}P(w_{i}|h_{i})
\end{equation}
However, there is no way of computing the probability of a word given a long history of preceding words. Estimating it from relative frequency counts is infeasible, if only for data sparsity reason. We can deal with this problem by using an $n$-gram model. An $n$-gram is simply a sequence of $n$ words (or other modelling units) -- an $n$-gram of size 1 is called a unigram, size 2 is a bigram, and size 3 is called a trigram. The $n$-gram model approximates the probability of a word given all the previous words with the probability of the word given $n-1$ preceding words:
\begin{equation}
	P(w_{i}|w_{1},\dots,w_{i-1})\approx P(w_{i}|w_{i-n+1},\dots,w_{i-1})
\end{equation}
\subsection{Word n-grams}
The $n$-gram can be interpreted as a left-to-right Markov chain of order $n-1$, because the probability of the current state depends only on $n-1$ previous states. To put it differently, the $n$-gram models satisfies the $n-1$ order Markov assumption (see Section \ref{subsection:acoustic}). In contrast to \glspl{hmm}, the states of Markov chains are directly visible. In case of word $n$-gram models, states are simply words. For example, consider the following sequence:
\begin{center}
\texttt{open the pod bay doors}  
\end{center}
We would like to estimate the probability of next word being \texttt{hal}. Using a bigram model, for example, we have:
\begin{equation}
	P(\texttt{hal}|\texttt{open the pod bay doors}) \approx P(\texttt{hal}|\texttt{doors})
\end{equation}
Truncating the history to $n-1$ units drastically reduces the number of free parameters. Although this number is still enormous - for a bigram model and a vocabulary of 50000 words, there are potentially two and a half billion parameters -- it is at least feasible to estimate the probability of the entire sequence:
\begin{multline}
	P(\texttt{open the pod bay door hal}) \approx \\
	\approx P(\texttt{open}|\texttt{<s>})P(\texttt{the}|\texttt{open})P(\texttt{pod}|\texttt{the})P(\texttt{bay}|\texttt{pod})P(\texttt{door}|\texttt{bay})P(\texttt{hal}|\texttt{door})P(\texttt{</s>}|\texttt{hal}).
\end{multline}
where <s> and </s> are start-of sentence and end-of-sentence markers respectively.
There are several disadvantages of word-based $n$-gram models. The first one is the obviously unrealistic Markov's assumption. Language dependencies often span across far more than just three or four words, especially in inflected languages. Consequently, low-order word-based $n$-grams may do a poor job at disambiguating grammatically invalid sentences, because an incorrect sentence can be constructed from a sequence of valid $n$-grams, when the $n$ is small. Take this quote from Yoda as an example:
\begin{center}
	\texttt{that face you make look I so old to young eyes.}
\end{center}
		A bigram model would quite likely assign a relatively high probability to this phrase, because each pair of consecutive words is plausible. Using a trigram model would probably yield better results, but at a considerable cost - for a lexicon of 50 000 words, switching from bigram to trigram model introduces more than one hundred trillions potential parameters!
\section{Neural network models}
\label{section:ann}
\section{Evaluation of language models}
\label{section:evaluation}


