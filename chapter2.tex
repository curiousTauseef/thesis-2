\chapter{Statistical language models}
\label{chapter:lm}
Language modelling lies at the core of many natural language processing tasks, such as speech recognition and synthesis, document classification, grammar and spelling checking, parsing, machine translation, and information retrieval. In general, the task of \glspl{lm} is to estimate the likelihood of a sequence of words. This information can then be used to reject invalid sentences or resolve ambiguities. In case of automatic speech recognition the language model guides and constrains the search among alternative word hypotheses \cite{glass2013automatic}.

In this chapter, a thorough introduction to statistical language modelling is given. The first section describes the word $n$-gram model, which is still one of the most important tools in speech and language processing. It also presents several smoothing techniques. Section \ref{section:ann} is dedicated to the connectionist models based on artificial neural networks. The final section contains an overview of evaluation methods for language models.
\section{N-gram models}
\label{section:ngrams}
In Section \ref{subsection:lm} we defined the chain rule of probability, which lets us decompose the joint probability of a sequence of $N$ words into a product of conditional probabilities:
\begin{equation}
	\label{equation:chain}
	P(w_{1}, \dots, w_{n})=\prod_{i=1}^{n}P(w_{i}|w_{1},\dots,w_{i-1})=\prod_{i=1}^{n}P(w_{i}|h_{i})
\end{equation}
However, there is no way of computing the probability of a word given a long history of preceding words. Estimating it from relative frequency counts is infeasible, if only for data sparsity reason. We can deal with this problem by using an $n$-gram model. An $n$-gram is simply a sequence of $n$ words (or other modelling units) -- an $n$-gram of size 1 is called a unigram, size 2 is a bigram, and size 3 is called a trigram. The $n$-gram model approximates the probability of a word given all the previous words with the probability of the word given $n-1$ preceding words:
\begin{equation}
	P(w_{i}|w_{1},\dots,w_{i-1})\approx P(w_{i}|w_{i-(n-1)},\dots,w_{i-1})
\end{equation}
The $n$-gram can be interpreted as a left-to-right Markov chain of order $n-1$, because the probability of the current state depends only on $n-1$ previous states. To put it differently, the $n$-gram models satisfies the $n-1$ order Markov assumption (see Section \ref{subsection:acoustic}). In contrast to \glspl{hmm}, the states of Markov chains are directly visible.

Another way to look at the $n$-gram model is through the notion of history equivalence class, introduced in Section \ref{subsection:lm}. As indicated previously, it is impossible to obtain a probability estimate for every imaginable word history, but the number of parameters can be reduced by classifying word histories into equivalence classes. Indeed, in \cite{jelinek1997statistical}, language modelling is defined as the task of finding the best history classification function:
\begin{equation}
	\Phi:h_{i}\mapsto\varphi_{i}=\Phi(h_{i})=\Phi(w_{1}, \dots, w_{i-1})
\end{equation}
Finding an optimal history classification function is the matter of striking a fine balance between its predictive power and complexity. To be a useful predictor, an equivalence class has to maximise the information about next word, while minimising the dimensionality of the resulting model. In case of $n$-gram models, the history classification function $\Phi$ simply limits the word history to the last $n-1$ elements:
\begin{equation}
	\varphi_{i}=\Phi(h_{i})=\Phi(w_{1}, \dots, w_{i-1})=w_{i-(n-1)}, \dots, w_{i-1}
\end{equation}
Two word histories are therefore equivalent if their last $n-1$ elements are identical. The tradeoff between efficiency and complexity quickly becomes apparent when the value of $n$ is increased~--~higher order $n$-grams typically exhibit lower perplexity (see Section \ref{subsection:perplexity}) and better performance, but are more expensive in terms of time and memory.
\subsection{Maximum Likelihood Estimation}
\label{subsection:mle}
The most common technique of estimating the $n$-gram probabilities from a text corpus is \gls{mle}. The procedure is very simple -- the $n$-gram probability of a word $w_{i}$ given the truncated history $\varphi_{i}=w_{i-(n-1)}, \dots, w_{i-1}$ is the observed frequency of the $n$-gram $w_{i-(n-1)}, \dots, w_{i-1}, w_{i}$ normalised by the sum of observed frequencies of all $n$-grams sharing the same history:
\begin{equation}
	\label{equation:mle}
	p(w_{i}|w_{i-(n-1)}, \dots, w_{i-1})=\frac{C(w_{i-(n-1)}, \dots, w_{i-1}, w_{i})}{\sum_{w}C(w_{i-(n-1)}, \dots, w_{i-1}, w)}.
\end{equation}
Note that the sum in the denominator is equal simply to $C(\varphi_{i})$, so the formula in Equation \ref{equation:mle} can be more intuitively understood as the proportion of cases in which a particular equivalent word history $\varphi_{i}$ is followed by the word $w_{i}$:
\begin{equation}
	\label{equation:mle2}
	P(w_{i}|\varphi_{i})=\frac{C(\varphi_{i}, w_{i})}{C(\varphi_{i})}.
\end{equation}
This ratio is called a relative frequency. In the special case of unigram probability it can be calculated as the count of the particular word $w_{i}$ normalised by the size of the corpus:
\begin{equation}
	\label{equation:mle2}
	P(w_{i})=\frac{C(w_{i})}{\sum_{w}C(w)}.
\end{equation}
\begin{table}[h!]
	\caption{Calculating selected bigram and trigram probabilities using maximum likelihood estimation \cite{jurafsky2000speech}. The \texttt{<s>} and \texttt{</s>} tags denote the beginning and the end of the sentence.}
	\label{table:mle}
	\texttt{%
		\begin{tabular}{c}
			<s> egg bacon and spam </s> \\
			<s> egg bacon sausage and spam </s> \\
			<s> spam bacon sausage and spam </s> \\
			<s> spam egg spam spam bacon and spam </s> \\
			\\
		\end{tabular}}
		\centering
		\begin{tabular*}{.8\linewidth}{@{\extracolsep{\fill}}lll}
			$P(\texttt{bacon}|\texttt{spam})=\frac{2}{8}$ & $P(\texttt{spam}|\texttt{spam})=\frac{1}{8}$ & $P(\texttt{egg}|\texttt{<s> <s>})=\frac{1}{2}$  \\
			$P(\texttt{bacon}|\texttt{egg})=\frac{2}{3}$  & $P(\texttt{spam}|\texttt{<s>})=\frac{2}{4}$ & $P(\texttt{egg}|\texttt{bacon and})=0$ \\
			$P(\texttt{bacon}|\texttt{and})=0$            & $P(\texttt{spam}|\texttt{and})=1$ & $P(\texttt{</s>}|\texttt{and spam})=1$ \\
		\end{tabular*}
\end{table}
Table \ref{table:mle} shows an example of calculating $n$-gram probabilities using a corpus of four phrases from Monty Python's notorious spam sketch. Inspecting it reveals a major problem with the \gls{mle} approach. Note that some probability estimates are equal to one, so using the model to generate random sentences would in many cases result in phrases taken verbatim from the corpus. Even worse, there are also probability estimates equal to zero. This means that because of the chain rule, the model will asign a zero probability to any sequence containing a known word in a new context. This includes one of the previous lines of the sketch:
\begin{equation}
	P(\texttt{<s> egg and bacon </s>})=0 \Leftarrow P(\texttt{bacon}|\texttt{and})=0
\end{equation}
Both these artifacts are a consequence of data sparsity -- the corpus is simply too small for the model to be an accurate representation of the language. The probability of observed items is overestimated, while the probability of unobserved items is underestimated. Although the example is obviously exaggerated, the problem persists in case of full-sized corpora. There are many techniques of correcting this bias by shifting the probability mass from frequent to previously unseen items. This process, called smoothing or discounting, is described in Section \ref{subsection:smoothing}. 

Zero probability is one problem, but there is also an issue of words that do not appear in the corpus at all. In speech recognition, \gls{oov} tokens are inevitable and they need to be identified, because they contribute to recognition errors in surrounding words. More importantly, they are often information-rich nouns, such as proper names, domain-specific notions, or foreign words. Language models can be used to facilitate the process of \gls{oov} token detection by incorporating the information about unknown words into the training process. All words that appear in the \gls{lm} training data, but do not appear in the \gls{asr} vocabulary, are simply substituted by unknown word token \texttt{<ign>}. These pseudo-words are then treated like any other regular word in the corpus, similarly to \texttt{<s>} and \texttt{</s>}. 

Another conclusion that can be drawn from the example in Table \ref{table:mle} is that the model is only as representative as the corpus it is trained on. This is especially important in \gls{lvcsr}, where the model not only has to represent non-domain-specific vocabulary, but also focus on spoken language, which is often very different from writing. \gls{asr} systems are often trained on written texts, because this kind of data is usually readily available, but using speech transcripts can lead to better results with less training data \cite{dziadzio2015comparison}. This idea is further explored in the experimental part of the thesis.
\subsection{Smoothing}
\label{subsection:smoothing}
Smoothing is a way of dealing with the zero probability problem. In principle, it is the process of adjusting the \gls{mle} estimates to produce more accurate probabilities \cite{chen1996empirical}. High probabilities are adjusted downwards and low probabilities are adjusted upwards, so the resulting distribution is more uniform. The simplest method of smoothing is additive smoothing. The idea is to add $\delta$ to every $n$-gram count, where typically $0 < \delta \leq 1$:
\begin{equation}
	P_{add}(w_{i}|w_{i-(n-1)}, \dots, w_{i-1})=\frac{\delta+C(w_{i-(n-1)}, \dots, w_{i-1}, w_{i}}{\delta|V|+\sum_{w}C(w_{i-(n-1)}, \dots, w_{i-1}, w)}.
\end{equation}
This method is easy to implement, but has been shown in \cite{Gale94what'swrong} to generally perform poorly. That is why Katz back-off and Chen and Goodman's modified Knesser-Ney smoothing is used in the experimental part.

Katz smoothing builds on the Good-Turing estimate, based on the \textit{symmetry requirement} which states that two events which occur the same number of times in the sample must have equal probabilities \cite{whittaker2000statistical}. The idea is to adjust the count of $n$-grams that occur $c$ times using the counts of $n$-grams that occur $c+1$ times -- in particular, to estimate the probability of unseen $n$-grams using the singleton counts. Let $N_{c}$ denote the number of $n$-grams that occur $c$ times in the sample. For each count $c$, an adjusted count $\hat{c}$ is computed:
\begin{equation}
	\hat{c}=(c+1)\frac{N_{c+1}}{N_{c}}.
\end{equation}
The updated probability estimate can be calculated by normalising the updated count by the size of the sample. For an $n$-gram $\alpha$ with $c$ counts:
\begin{equation}
	P_{\textsc{good}}(\alpha:C(\alpha)=c)=\frac{\hat{c}}{N}, 
\end{equation}
where:
\begin{equation}
	N=\sum_{c}cN_{c}.
\end{equation}
The problem with using plain Good-Turing estimates for discounting is that $N_{c+1}$ quite often equals zero for high $c$. Katz smoothing extends the idea behind Good-Turing estimation by using lower-order distributions to better reallocate the count mass subtracted from nonzero counts. For example, while adjusting the counts of bigrams, the unigram distribution is used: 
\begin{equation}
	C_{\textsc{katz}}(w_{i-1}, w_{i})=
	\begin{cases}
		d_{c}c & \text{if } c>0 \\
		\alpha(w_{i-1})P_{\textsc{mle}}(w_{i}) & \text{if } c=0,
	\end{cases}
\end{equation}
where $d_{c}$ is the discount ratio. The back-off weight alpha is chosen so that $\sum_{w}C_{\textsc{katz}}(w_{i-1}, w)=\sum_{w}C_{\textsc{mle}}(w_{i-1}, w)$:
\begin{equation}
	\alpha(w_{i-1})=\frac{1-\sum_{w: C(w_{i-1}, w)>0}P_{\textsc{katz}}(w|w_{i-1})}{1-\sum_{w: C(w_{i-1}, w)>0}P_{\textsc{mle}}(w|w_{i-1})}
\end{equation}
\subsection{Word n-grams}
where $V$ is the vocabulary.
So far, while describing the $n$-gram models, we used the terms ``word'' and ``modeling unit'' interchangeably. However, depending on the application, $n$-grams can use sub-lexical units, such as phonemes, letters, and syllables, or supra-lexical units, such as \gls{pos} tags. In case of \gls{asr}, words are the most popular choice for the modeling units, because word-based models can be easily incorporated into the search process described in section \ref{subsection:lm}. For example, consider this famous quote:
\begin{center}
	\texttt{open the pod bay doors}
\end{center}
We would like to estimate the probability of next word being \texttt{hal}. Using a bigram model, we get:
\begin{equation}
	P(\texttt{hal}|\texttt{open the pod bay doors}) \approx P(\texttt{hal}|\texttt{doors})
\end{equation}
Truncating the history to $n-1$ units drastically reduces the number of free parameters. Although this number is still enormous - for a bigram model and a vocabulary of 50000 words, there are potentially two and a half billion parameters -- it is at least feasible to estimate the probability of the entire sequence using the chain rule from Equation \ref{equation:chain}:
\begin{multline}
	P(\texttt{<s> open the pod bay door hal </s>}) \approx \\
	\approx P(\texttt{open}|\texttt{<s>})P(\texttt{the}|\texttt{open})P(\texttt{pod}|\texttt{the})P(\texttt{bay}|\texttt{pod})P(\texttt{door}|\texttt{bay})P(\texttt{hal}|\texttt{door})P(\texttt{</s>}|\texttt{hal})
\end{multline}
There are several disadvantages of word-based $n$-gram models. The first one is the obviously unrealistic Markov's assumption. Language dependencies often span across far more than just three or four words, especially in inflected languages. Consequently, low-order word-based $n$-grams may do a poor job at disambiguating grammatically invalid sentences, because an incorrect sentence can be constructed from a sequence of valid short $n$-grams. Take this quote from Yoda as an example:
\begin{center}
	\texttt{<s> look I so old to young eyes </s>}
\end{center}
A bigram model would likely assign a relatively high probability to this phrase, because each pair of consecutive words is entirely plausible. 
In the above example, using a trigram model would probably yield better results, but at a considerable cost - even for a lexicon of 50 000 words (far too few for modeling any inflected language), switching from bigram to trigram introduces more than one hundred trillions potential parameters! In short, there is always a tradeof between the quality of predictions and the amount of data required to calculate the probability estimates. Training and storing higher-order word $n$-grams can be simply infeasible due to data sparsity and memory constraints.
Another issue with word-based $n$-grams is that the truncation of word history may lead to unintuitive or inconsistent behaviour. This problem is especially pronounced in case of languages with weak constraints on word order -- permutations of the same sequence may be assigned different probability estimates despite being grammatically and semantically equivalent. Furthermore, very similar word histories may lead to different predictions. Consider these two sequences:
\begin{center}
	\texttt{a brilliant american mathematician who graduated from} \\
	\texttt{a brilliant american mathematician who graduated recently from}.
\end{center}
From the point of view of predicting the next word, these two histories are very similar. However, they would be considered completely different by a trigram word model \cite{whittaker2000statistical}. In the second case, we would need at least a $5$-gram model to capture the intuition that the next word is probably a name of a university.

Although word-based $n$-gram language models have been outperformed by deep neural networks in terms of perplexity and word error reduction rate, they are still commonly used in \gls{asr} systems, as they strike a fine balance between effectiveness and simplicity. They are straightforward to train and can be easily used to guide the search through the lattice of word hypotheses. However, in case of inflected languages, the problem of data sparsity is significantly amplified because and therefore using words as the modeling unit is not necesarily the best choice.
\subsection{Class-based $n$-gram models}
\section{Neural network models}
\label{section:ann}
\section{Evaluation of language models}
\label{section:evaluation}
\subsection{Perplexity}
\label{subsection:perplexity}
\subsection{WERR}
\label{subsection:werr}
\subsection{Shannon Method}
