\chapter{Results}
\label{chapter:results}
\section{Perplexity}
The perplexity values presented in Tables \ref{table:ppl_plain}-\ref{table:ppl_gnc} were calculated on a test set of 20 000 utterances taken from the transcripted proceedings of the Polish parliament. The lowest perplexity achieved for a word n-gram model is 240.54. It should once again be noted that both the training and the test data come from a closed domain. This could significantly reduce the perplexity, as legal and parliamentary language is generally more predictable. For example, in \cite{bengio2003neural} it is reported that a Kneser-Ney smoothed back-off trigram model achieved a perplexity of 323 on the Brown Corpus (general English) and only 127 on the AP News Corpus. 

\begin{table}[!htbp]
	\centering
	\caption{Perplexity of the plain text language model with Chen-Goodman's modified Kneser-Ney smoothing.}
	\label{table:ppl_plain}
	\begin{tabular*}{.6\linewidth}{@{\extracolsep{\fill}}l*3r}
		{}        & \multicolumn{1}{c}{text} & \multicolumn{1}{c}{speech} & \multicolumn{1}{c}{full}  \\
		\midrule
                unigrams  & 4409.47  & 3510.78 & 3767.37\\
	        bigrams   & 1380.31  & 445.76  & 458.40\\
		trigrams  & 1231.23  & 240.54  & 242.31\\
	\end{tabular*}
\end{table}

\begin{table}[!htbp]
	\centering
	\caption{Perplexity of the lemma language model with Chen-Goodman's modified Kneser-Ney smoothing.}
	\label{table:ppl_lemma}
	\begin{tabular*}{.6\linewidth}{@{\extracolsep{\fill}}l*3r}
		{}        & \multicolumn{1}{c}{text} & \multicolumn{1}{c}{speech} & \multicolumn{1}{c}{full} \\
		\midrule
		unigrams  & 1844.97  & 1309.19 & 1654.56\\
	        bigrams   & 744.72   & 299.82  & 367.30\\
                trigrams  & 694.41   & 177.91  & 213.60\\
	\end{tabular*}
\end{table}
Because the models have different vocabulary sizes, the perplexity comparisons across different types of models are not meaningful. For every type of models, the perplexity of speech models is lower than that of text models. This could be attributed to a larger training set, but in case of the plain, lemma, and POS corpus, the perplexity of a model trained on the full corpus is consistently higher than that of a model trained on the speech corpus alone. This means that increasing the training set by incorporating the text data actually lead to a worse model, assuming that perplexity is a valid metric in this context. Another, rather unsuprising observation is the large and consistent decrease in perplexity with the n-gram order. Tables \ref{table:ppl_plain} and \ref{table:ppl_lemma} show that the drop in perplexity is more pronounced in case of models with a larger vocabulary. Interestingly, the difference in perplexity between trigram and unigram models is much greater in case of models trained on speech data. For example, the perplexity of the plain text trigram model is approximately 3.6 times lower than that of a plain text unigram model, while the perplexity of a plain speech trigram model is 14.6 times lower than that of its unigram counterpart.

\begin{table}[!htbp]
	\centering
	\caption{Perplexity of the POS language model with Witten-Bell smoothing.}
	\label{table:ppl_pos}
	\begin{tabular*}{.6\linewidth}{@{\extracolsep{\fill}}l*3r}
		{}        & \multicolumn{1}{c}{text} & \multicolumn{1}{c}{speech} & \multicolumn{1}{c}{full}  \\
		\midrule
		unigrams  & 11.25  & 10.40 & 10.37\\
	        bigrams   & 9.36   & 8.35  & 8.37\\
                trigrams  & 8.71   & 7.71  & 7.73\\
	\end{tabular*}
\end{table}

\begin{table}[!htbp]
	\centering
	\caption{Perplexity of the GNC language model with Good-Turing smoothing.}
	\label{table:ppl_gnc}
	\begin{tabular*}{.6\linewidth}{@{\extracolsep{\fill}}l*3r}
		{}        & \multicolumn{1}{c}{text} & \multicolumn{1}{c}{speech} & \multicolumn{1}{c}{full}  \\
		\midrule
		unigrams  & 90.04   & 82.16  & 81.38\\
	        bigrams   & 38.73   & 35.61  & 34.52\\
                trigrams  & 33.35   & 29.44  & 28.26\\
	\end{tabular*}
\end{table}

Tables \ref{table:ppl_pos} and \ref{table:ppl_gnc} present the perplexity results for the models based on morphosyntactic tags. The effect of the vocabulary size on perplexity is clearly visible. In case of the POS model, the perplexity of the model trained on the speech corpus is again lower than that of a model trained on the full corpus. However, the opposite is true for the GNC model. Generally, the perplexity scores are inconclusive as far as the difference between text-based and speech-based language models is concerned.

\begin{table}[!htbp]
	\centering
	\caption{Perplexity of small plain}
	\label{table:ppl_gnc}
	\begin{tabular*}{.6\linewidth}{@{\extracolsep{\fill}}l*2r}
		{}        & \multicolumn{1}{c}{text} & \multicolumn{1}{c}{speech}\\
		\midrule
		unigrams  & 4405.45   & 3078.70\\
	        bigrams   & 1376.68   & 476.25\\
                trigrams  & 1219.00   & 296.09\\
	\end{tabular*}
\end{table}

\section{WERR}
The n-gram models were 
\label{section:werr}
\begin{figure}[!htbp]
	  \centering
	  \includegraphics[height=10cm, width=15cm]{plain_werr_1.png}
	      \caption{Absolute word error reduction of the plain model (testing set 1)}
	      \label{figure:plain1}
\end{figure}

\begin{figure}[!htbp]
	  \centering
	  \includegraphics[height=10cm, width=15cm]{lemma_werr_1.png}
	      \caption{Absolute word error reduction of the lemma model (testing set 1)}
	      \label{figure:lemmy1}
\end{figure}

\begin{figure}[!htbp]
	  \centering
	  \includegraphics[height=10cm, width=15cm]{pos_werr_1.png}
	      \caption{Absolute word error reduction of the pos model (testing set 1)}
	      \label{figure:pos1}
\end{figure}

\begin{figure}[!htbp]
	  \centering
	  \includegraphics[height=10cm, width=15cm]{gnc_werr_1.png}
	      \caption{Absolute word error reduction of the gnc model (testing set 1)}
	      \label{figure:gnc1}
\end{figure}

\begin{figure}[!htbp]
	  \centering
	  \includegraphics[height=10cm, width=15cm]{plain_werr_5.png}
	      \caption{Absolute word error reduction of the plain model (testing set 5)}
	      \label{figure:plain5}
\end{figure}

\begin{figure}[!htbp]
	  \centering
	  \includegraphics[height=10cm, width=15cm]{lemma_werr_5.png}
	      \caption{Absolute word error reduction of the lemma model (testing set 5)}
	      \label{figure:lemmy5}
\end{figure}

\begin{figure}[!htbp]
	  \centering
	  \includegraphics[height=10cm, width=15cm]{pos_werr_5.png}
	      \caption{Absolute word error reduction of the pos model (testing set 5)}
	      \label{figure:pos5}
\end{figure}

\begin{figure}[!htbp]
	  \centering
	  \includegraphics[height=10cm, width=15cm]{gnc_werr_5.png}
	      \caption{Absolute word error reduction of the gnc model (testing set 5)}
	      \label{figure:gnc5}
\end{figure}
