\chapter{Results}
\label{chapter:results}
\section{Perplexity}
The perplexity values presented in Tables \ref{table:ppl_word}-\ref{table:ppl_gnc} were calculated on a test set of 20 000 utterances taken from the transcripted proceedings of the Polish parliament. The lowest perplexity achieved for a word n-gram model is 240.54. It should once again be noted that both the training and the test data come from a specific domain. This could significantly reduce the perplexity, as legal and parliamentary language is generally more predictable. For example, in \cite{bengio2003neural} it is reported that a Kneser-Ney smoothed back-off trigram model achieved a perplexity of 323 on the Brown Corpus (general English) and only 127 on the AP News Corpus. 

\begin{table}[!htbp]
	\centering
	\caption{Perplexity of the word text language models with Chen-Goodman's modified Kneser-Ney smoothing.}
	\label{table:ppl_word}
	\begin{tabular*}{.6\linewidth}{@{\extracolsep{\fill}}l*3r}
		{}        & \multicolumn{1}{c}{text} & \multicolumn{1}{c}{speech} & \multicolumn{1}{c}{full}  \\
		\midrule
                unigrams  & 4409.47  & 3510.78 & 3767.37\\
	        bigrams   & 1380.31  & 445.76  & 458.40\\
		trigrams  & 1231.23  & 240.54  & 242.31\\
	\end{tabular*}
\end{table}

\begin{table}[!htbp]
	\centering
	\caption{Perplexity of the lemma language models with Chen-Goodman's modified Kneser-Ney smoothing.}
	\label{table:ppl_lemma}
	\begin{tabular*}{.6\linewidth}{@{\extracolsep{\fill}}l*3r}
		{}        & \multicolumn{1}{c}{text} & \multicolumn{1}{c}{speech} & \multicolumn{1}{c}{full} \\
		\midrule
		unigrams  & 1844.97  & 1309.19 & 1654.56\\
	        bigrams   & 744.72   & 299.82  & 367.30\\
                trigrams  & 694.41   & 177.91  & 213.60\\
	\end{tabular*}
\end{table}

Because the models have different vocabulary sizes, the perplexity scores are not comparable across different types of models. For every type of models, the perplexity of speech models is lower than that of text models. This could be attributed to a larger training set, but in case of the word, lemma, and POS corpus, the perplexity of a model trained on the full corpus is consistently higher than that of a model trained on the speech corpus alone. This means that increasing the training set by incorporating the text data actually lead to a worse model, assuming that perplexity is a valid metric in this context. Another, rather unsuprising observation is the large and consistent decrease in perplexity with the n-gram order. Tables \ref{table:ppl_word} and \ref{table:ppl_lemma} show that the drop in perplexity is more pronounced in case of models with a larger vocabulary. Interestingly, the difference in perplexity between trigram and unigram models is much greater in case of models trained on speech data. For example, the perplexity of the word text trigram model is approximately 3.6 times lower than that of a word text unigram model, while the perplexity of a word speech trigram model is 14.6 times lower than that of its unigram counterpart.

\begin{table}[!htbp]
	\centering
	\caption{Perplexity of the POS language models with Witten-Bell smoothing.}
	\label{table:ppl_pos}
	\begin{tabular*}{.6\linewidth}{@{\extracolsep{\fill}}l*3r}
		{}        & \multicolumn{1}{c}{text} & \multicolumn{1}{c}{speech} & \multicolumn{1}{c}{full}  \\
		\midrule
		unigrams  & 11.25  & 10.40 & 10.37\\
	        bigrams   & 9.36   & 8.35  & 8.37\\
                trigrams  & 8.71   & 7.71  & 7.73\\
	\end{tabular*}
\end{table}

\begin{table}[!htbp]
	\centering
	\caption{Perplexity of the GNC language models with Good-Turing smoothing.}
	\label{table:ppl_gnc}
	\begin{tabular*}{.6\linewidth}{@{\extracolsep{\fill}}l*3r}
		{}        & \multicolumn{1}{c}{text} & \multicolumn{1}{c}{speech} & \multicolumn{1}{c}{full}  \\
		\midrule
		unigrams  & 90.04   & 82.16  & 81.38\\
	        bigrams   & 38.73   & 35.61  & 34.52\\
                trigrams  & 33.35   & 29.44  & 28.26\\
	\end{tabular*}
\end{table}

Tables \ref{table:ppl_pos} and \ref{table:ppl_gnc} present the perplexity results for the models based on morphosyntactic tags. The effect of the vocabulary size on perplexity is clearly visible. In case of the POS model, the perplexity of the model trained on the speech corpus is again lower than that of a model trained on the full corpus. However, the opposite is true for the GNC model. 

Generally, the perplexity scores hint that the models trained on speech transcripts are a better representation of the language for the purpose of \gls{asr}, as they tend to have a lower perplexity than models trained on written text, despite having a larger vocabulary. To further verify this hypothesis by eliminating the effect of the training set size, two equal sized training sets were built, one consisting of speech transcripts and the other of written texts. Then, four pairs of n-gram models were trained on these sets, this time ensuring that the vocabularies of corresponding models are identical. The perplexity of these models is presented in Tables \ref{table:ppl_word_small}-\ref{table:ppl_gnc_small}. The models based on speech data have a significantly lower perplexity for all four types of modeling units. The difference is more prominent in case of word and lemma models. For example, the perplexity of a speech-based word trigram model is more than four times lower than that of a text-based model. In case of POS and GNC models, the difference is still noticeable, although much less pronounced. Moreover, in case of speech models, perplexity still decreases more rapidly with the n-gram order. For example, using trigrams instead of unigrams results in a tenfold drop in perplexity of a speech-based model, and only a threefold decrease in case of a text-based model. 

\begin{table}[!htbp]
	\centering
	\caption{Comparison of perplexity of word text and speech models with equal vocabularies}
	\label{table:ppl_word_small}
	\begin{tabular*}{.4\linewidth}{@{\extracolsep{\fill}}l*2r}
		{}        & \multicolumn{1}{c}{text} & \multicolumn{1}{c}{speech}\\
		\midrule
		unigrams  & 3351.69   & 2287.98\\
	        bigrams   & 1051.36   & 367.79\\
                trigrams  & 942.74    & 226.06\\
	\end{tabular*}
\end{table}
\begin{table}[!htbp]
	\centering
	\caption{Comparison of perplexity of lemma text and speech models with equal vocabularies}
	\label{table:ppl_lemma_small}
	\begin{tabular*}{.4\linewidth}{@{\extracolsep{\fill}}l*2r}
		{}        & \multicolumn{1}{c}{text} & \multicolumn{1}{c}{speech}\\
		\midrule
		unigrams  & 1520.89  & 1149.99\\
	        bigrams   & 560.73   & 260.05\\
                trigrams  & 498.97   & 154.91\\
	\end{tabular*}
\end{table}
\begin{table}[!htbp]
	\centering
	\caption{Comparison of perplexity of POS text and speech models with equal vocabularies}
	\label{table:ppl_pos_small}
	\begin{tabular*}{.4\linewidth}{@{\extracolsep{\fill}}l*2r}
		{}        & \multicolumn{1}{c}{text} & \multicolumn{1}{c}{speech}\\
		\midrule
		unigrams  & 11.23  & 10.29\\
	        bigrams   & 9.27   & 8.23\\
                trigrams  & 8.62   & 7.56\\
	\end{tabular*}
\end{table}
\begin{table}[!htbp]
	\centering
	\caption{Comparison of perplexity of GNC text and speech models with equal vocabularies}
	\label{table:ppl_gnc_small}
	\begin{tabular*}{.4\linewidth}{@{\extracolsep{\fill}}l*2r}
		{}        & \multicolumn{1}{c}{text} & \multicolumn{1}{c}{speech}\\
		\midrule
		unigrams  & 159.14 & 156.54\\
	        bigrams   & 47.79  & 45.08\\
                trigrams  & 27.88  & 20.86\\
	\end{tabular*}
\end{table}

\begin{table}[!htbp]
	\centering
	\caption{Perplexity of neural models}
	\label{table:wer_neural}
	\begin{tabular*}{.4\linewidth}{@{\extracolsep{\fill}}lr}
		word   & 240.81\\
		lemma  & 216.64\\
		pos    & 7.12\\
		gnc    & 24.87\\
	\end{tabular*}
\end{table}

\section{Size}
\section{N-best list rescoring}
The WERR values presented in this section were calculated using the n-best list rescoring framework described in Subsection \ref{subsection:wer}. The n-best lists were actual recognition hypotheses from the Sarmata \gls{asr} system \cite{ziolko2011automatic}. The utterances come from Polish television news programmes. The language models were tested on a set of 84 lists, each containing about 240 hypotheses. Figures \ref{figure:word5}-\ref{figure:gnc5} present the absolute \gls{werr} of language models as a function of the $\alpha$ coefficient.

\label{section:nbest}
%\begin{figure}[!htbp]
%	  \centering
%	  \includegraphics[height=10cm, width=15cm]{word_werr_1.png}
%	      \caption{Absolute word error reduction of the word model (testing set 1)}
%	      \label{figure:word1}
%\end{figure}
%
%\begin{figure}[!htbp]
%	  \centering
%	  \includegraphics[height=10cm, width=15cm]{lemma_werr_1.png}
%	      \caption{Absolute word error reduction of the lemma model (testing set 1)}
%	      \label{figure:lemmy1}
%\end{figure}
%
%\begin{figure}[!htbp]
%	  \centering
%	  \includegraphics[height=10cm, width=15cm]{pos_werr_1.png}
%	      \caption{Absolute word error reduction of the pos model (testing set 1)}
%	      \label{figure:pos1}
%\end{figure}
%
%\begin{figure}[!htbp]
%	  \centering
%	  \includegraphics[height=10cm, width=15cm]{gnc_werr_1.png}
%	      \caption{Absolute word error reduction of the gnc model (testing set 1)}
%	      \label{figure:gnc1}
%\end{figure}
%
\begin{figure}[!htbp]
	  \centering
	  \includegraphics[height=10cm, width=15cm]{word_werr_5.png}
	      \caption{Absolute word error reduction of the word model (testing set 5)}
	      \label{figure:word5}
\end{figure}

\begin{figure}[!htbp]
	  \centering
	  \includegraphics[height=10cm, width=15cm]{lemma_werr_5.png}
	      \caption{Absolute word error reduction of the lemma model (testing set 5)}
	      \label{figure:lemmy5}
\end{figure}

\begin{figure}[!htbp]
	  \centering
	  \includegraphics[height=10cm, width=15cm]{pos_werr_5.png}
	      \caption{Absolute word error reduction of the pos model (testing set 5)}
	      \label{figure:pos5}
\end{figure}

\begin{figure}[!htbp]
	  \centering
	  \includegraphics[height=10cm, width=15cm]{gnc_werr_5.png}
	      \caption{Absolute word error reduction of the gnc model (testing set 5)}
	      \label{figure:gnc5}
\end{figure}

\FloatBarrier
\subsection{Mock n-best list rescoring}
The WERR results presented in Subsection \ref{section:nbest} were calculated on a real \gls{asr} system. However, the n-best lists were generated by an old version of Sarmata, when the performance of the acoustic model was rather poor, and it is therefore of limited value as a benchmark. As the recognition quality improves, it gets more important for a model to be able to correctly distinguish between very similar phrases. The generation of artificial n-best list is an implementation of the adversarial evaluation technique described in \cite{smith2012adversarial}. A simple algorithm was used to create a set of ten hypotheses for each phrase from the testing set used for perplexity calculation. To simulate the operation of the acoustic model, the hypotheses were generated by substituting random words by similar frequent words. Only words occuring more than 5000 times in the training data were used for substitution. The Levenshtein distance was used as a heuristic for finding similar words. Table \ref{table:mock} presents an example phrase along with the generated hypotheses.

\begin{table}[!htbp]
	\centering
	\caption{An example phrase and mock hypotheses}
	\label{table:mock}
	\begin{tabular*}{.6\linewidth}{@{\extracolsep{\fill}}l}
		lorem ipsum \\
		litwo ojczyzno \\
		moja ty jesteś \\
	\end{tabular*}
\end{table}

\begin{table}[!htbp]
	\centering
	\caption{WER of n-gram models}
	\label{table:wer_ngram}
	\begin{tabular*}{.6\linewidth}{@{\extracolsep{\fill}}l*3r}
		{}        & \multicolumn{1}{c}{text} & \multicolumn{1}{c}{speech} & \multicolumn{1}{c}{full} \\
		\midrule
		word   & 12.15  & 8.67  & 8.55\\
		lemma  & 16.95  & 11.81 & 13.34\\
		pos    & 21.10  & 19.38 & 19.58\\
		gnc    & 12.66  & 12.57 & 12.25\\
	\end{tabular*}
\end{table}

\begin{table}[!htbp]
	\centering
	\caption{WER of neural models}
	\label{table:wer_neural}
	\begin{tabular*}{.4\linewidth}{@{\extracolsep{\fill}}lr}
		{}        &  \multicolumn{1}{c}{full} \\
		\midrule
		word  & 8.21\\
		lemma  & 12.55\\
		pos    & 17.65\\
		gnc    & 11.61\\
	\end{tabular*}
\end{table}
