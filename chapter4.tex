\chapter{Results}
\label{chapter:results}
\section{Perplexity}
The perplexity values presented in Tables \ref{table:ppl_word}-\ref{table:ppl_gnc} were calculated on a test set of 20000 utterances taken from the transcripted proceedings of the Polish parliament. The lowest perplexity achieved for a word n-gram model is 240.54. It should once again be noted that both the training and the test data come from a specific domain. This could significantly reduce the perplexity, as legal and parliamentary language is generally more predictable. For example, in~\cite{bengio2003neural} it is reported that a Kneser-Ney smoothed back-off trigram model achieved a perplexity of~323 on the Brown Corpus (general English) and only~127 on the AP News Corpus. 

\begin{table}[!htbp]
	\centering
	\caption[Perplexity of the word language models]{Perplexity of the word language models with Chen-Goodman's modified Kneser-Ney smoothing.}
	\label{table:ppl_word}
	\begin{tabular*}{.6\linewidth}{@{\extracolsep{\fill}}l*3r}
		{}        & \multicolumn{1}{c}{text} & \multicolumn{1}{c}{speech} & \multicolumn{1}{c}{full}  \\
		\midrule
                unigrams  & 4409.47  & 3510.78 & 3767.37\\
	        bigrams   & 1380.31  & 445.76  & 458.40\\
		trigrams  & 1231.23  & 240.54  & 242.31\\
	\end{tabular*}
\end{table}

\begin{table}[!htbp]
	\centering
	\caption[Perplexity of the lemma language models]{Perplexity of the lemma language models with Chen-Goodman's modified Kneser-Ney smoothing.}
	\label{table:ppl_lemma}
	\begin{tabular*}{.6\linewidth}{@{\extracolsep{\fill}}l*3r}
		{}        & \multicolumn{1}{c}{text} & \multicolumn{1}{c}{speech} & \multicolumn{1}{c}{full} \\
		\midrule
		unigrams  & 1844.97  & 1309.19 & 1654.56\\
	        bigrams   & 744.72   & 299.82  & 367.30\\
                trigrams  & 694.41   & 177.91  & 213.60\\
	\end{tabular*}
\end{table}

Because the models have different vocabularies, the perplexity scores are not comparable across different types of models. However, for every type of models, the perplexity of speech models is lower than that of text models. This could be attributed to a larger training set, but in case of the word, lemma, and POS corpus, the perplexity of a model trained on the full corpus is consistently higher than that of a model trained on the speech corpus alone. This means that extending the training set by incorporating the text data actually lead to a worse model, assuming that perplexity is a valid metric in this context. Another, rather unsuprising observation is the large and consistent decrease in perplexity with the n-gram order. Tables \ref{table:ppl_word} and \ref{table:ppl_lemma} show that the drop in perplexity is more pronounced in case of models with a larger vocabulary. Interestingly, the difference in perplexity between trigram and unigram models is much greater in case of models trained on speech data. For example, the perplexity of the word text trigram model is approximately 3.6 times lower than that of a word text unigram model, while the perplexity of a word speech trigram model is 14.6 times lower than that of its unigram counterpart.

\begin{table}[!htbp]
	\centering
	\caption[Perplexity of the POS language models]{Perplexity of the POS language models with Witten-Bell smoothing.}
	\label{table:ppl_pos}
	\begin{tabular*}{.6\linewidth}{@{\extracolsep{\fill}}l*3r}
		{}        & \multicolumn{1}{c}{text} & \multicolumn{1}{c}{speech} & \multicolumn{1}{c}{full}  \\
		\midrule
		unigrams  & 11.25  & 10.40 & 10.37\\
	        bigrams   & 9.36   & 8.35  & 8.37\\
                trigrams  & 8.71   & 7.71  & 7.73\\
	\end{tabular*}
\end{table}

\begin{table}[!htbp]
	\centering
	\caption[Perplexity of the GNC language models]{Perplexity of the GNC language models with Good-Turing smoothing.}
	\label{table:ppl_gnc}
	\begin{tabular*}{.6\linewidth}{@{\extracolsep{\fill}}l*3r}
		{}        & \multicolumn{1}{c}{text} & \multicolumn{1}{c}{speech} & \multicolumn{1}{c}{full}  \\
		\midrule
		unigrams  & 90.04   & 82.16  & 81.38\\
	        bigrams   & 38.73   & 35.61  & 34.52\\
                trigrams  & 33.35   & 29.44  & 28.26\\
	\end{tabular*}
\end{table}

Tables \ref{table:ppl_pos} and \ref{table:ppl_gnc} present the perplexity results for the models based on morphosyntactic tags. The effect of the vocabulary size on perplexity is clearly visible. In case of the POS model, the perplexity of the model trained on the speech corpus is again lower than that of a model trained on the full corpus. However, the opposite is true for the GNC model. 

Generally, the perplexity scores hint that the models trained on speech transcripts are a better representation of the language for the purpose of \gls{asr}, as they tend to have a lower perplexity than models trained on written text, despite having a slightly larger vocabulary. To further verify this hypothesis by eliminating the effect of the training set size, two equal sized training sets were built, one consisting of speech transcripts and the other of written texts. Then, four pairs of \mbox{n-gram} models were trained on these sets, this time ensuring that the vocabularies of corresponding models are identical. The perplexity of these models is presented in Tables \mbox{\ref{table:ppl_word_small}-\ref{table:ppl_gnc_small}}. The models based on speech data have a significantly lower perplexity for all four types of modeling units. The difference is more prominent in case of word and lemma models. For example, the perplexity of a \mbox{speech-based} word trigram model is more than four times lower than that of a \mbox{text-based} model. In case of POS and GNC models, the difference is still noticeable, although much less pronounced. Moreover, in case of speech models, perplexity still decreases more rapidly with the n-gram order. For example, using trigrams instead of unigrams results in a tenfold drop in perplexity of a \mbox{speech-based} model, and only a threefold decrease in case of a \mbox{text-based} model. 

\begin{table}[!htbp]
	\centering
	\caption[Perplexity of word text and speech models with equal vocabularies]{Perplexity of word text and speech models with equal vocabularies.}
	\label{table:ppl_word_small}
	\begin{tabular*}{.4\linewidth}{@{\extracolsep{\fill}}l*2r}
		{}        & \multicolumn{1}{c}{text} & \multicolumn{1}{c}{speech}\\
		\midrule
		unigrams  & 3351.69   & 2287.98\\
	        bigrams   & 1051.36   & 367.79\\
                trigrams  & 942.74    & 226.06\\
	\end{tabular*}
\end{table}
\begin{table}[!htbp]
	\centering
	\caption[Perplexity of lemma text and speech models with equal vocabularies]{Perplexity of lemma text and speech models with equal vocabularies.}
	\label{table:ppl_lemma_small}
	\begin{tabular*}{.4\linewidth}{@{\extracolsep{\fill}}l*2r}
		{}        & \multicolumn{1}{c}{text} & \multicolumn{1}{c}{speech}\\
		\midrule
		unigrams  & 1520.89  & 1149.99\\
	        bigrams   & 560.73   & 260.05\\
                trigrams  & 498.97   & 154.91\\
	\end{tabular*}
\end{table}
\begin{table}[!htbp]
	\centering
	\caption[Perplexity of POS text and speech models with equal vocabularies]{Perplexity of POS text and speech models with equal vocabularies.}
	\label{table:ppl_pos_small}
	\begin{tabular*}{.4\linewidth}{@{\extracolsep{\fill}}l*2r}
		{}        & \multicolumn{1}{c}{text} & \multicolumn{1}{c}{speech}\\
		\midrule
		unigrams  & 11.23  & 10.29\\
	        bigrams   & 9.27   & 8.23\\
                trigrams  & 8.62   & 7.56\\
	\end{tabular*}
\end{table}
\begin{table}[!htbp]
	\centering
	\caption[Perplexity of GNC text and speech models with equal vocabularies]{Perplexity of GNC text and speech models with equal vocabularies.}
	\label{table:ppl_gnc_small}
	\begin{tabular*}{.4\linewidth}{@{\extracolsep{\fill}}l*2r}
		{}        & \multicolumn{1}{c}{text} & \multicolumn{1}{c}{speech}\\
		\midrule
		unigrams  & 159.14 & 156.54\\
	        bigrams   & 47.79  & 45.08\\
                trigrams  & 27.88  & 20.86\\
	\end{tabular*}
\end{table}

Table \ref{table:ppl_neural} presents the perplexity of neural models evaluated on the same testing set. When compared to corresponding \mbox{n-gram} models with the same vocabulary, the perplexity of neural models is slightly lower, with an exception of the lemma model.

\begin{table}[!htbp]
	\centering
	\caption[Perplexity of neural models]{Perplexity of neural models.}
	\label{table:ppl_neural}
	\begin{tabular*}{.4\linewidth}{@{\extracolsep{\fill}}lr}
		word   & 240.81\\
		lemma  & 216.64\\
		POS    & 7.12\\
		GNC    & 24.87\\
	\end{tabular*}
\end{table}

\section{Size}
The size of a text representation of the model can serve as a proxy for the amount of memory it will require in the final application. As shown in Table \ref{table:sizes}, the size of the model seems to be dependent mostly on the size of the vocabulary, with an interesting exception of the neural GNC model, which not only takes less space than the neural POS model, but is also 25 times smaller than its n-gram counterpart.

\begin{table}[!htbp]
	\centering
	\caption[Model size]{Model size in MB.}
	\label{table:sizes}
	\begin{tabular*}{.4\linewidth}{@{\extracolsep{\fill}}lr}
		neural word   & 899\\
		n-gram word   & 615\\
		n-gram lemma  & 432\\
		neural lemma  & 336\\
		n-gram GNC    & 36\\
		neural POS    & 3.7\\
		neural GNC    & 1.4\\
		n-gram POS    & 0.4\\
	\end{tabular*}
\end{table}

\section{N-best list rescoring}
\label{section:nbest}
The values of \gls{werr} presented in this section were calculated using the n-best list rescoring framework described in Subsection \ref{subsection:wer}. The n-best lists were actual recognition hypotheses from the Sarmata \gls{asr} system \cite{ziolko2011automatic}. The test utterances come from Polish television news programmes. The language models were tested on a set of 84 lists, each containing about 240 hypotheses. Figures \ref{figure:word}-\ref{figure:neural} present the absolute \gls{werr} of language models as a function of the $\alpha$ coefficient. The results should be treated with a certain dose of caution, as the quality of the acoustic hypotheses leaves a lot to be desired~--~the minimal WER a language model can achieve on this corpus is 66.35\%, and the maximal possible \gls{werr} is only 18.9\%. Note that all WER and WERR values in this and the following section are given as a percentage.

 Table \ref{table:max_werr} presents the maximum \gls{werr} values achieved by the respective models. Unsurprisingly, the models trained on words achieve the highest \gls{werr}, with the n-gram model having a slight advantage over its neural counterpart. As far as the trigram models are concerned, those trained on speech data perform slightly better than those trained on written text and In case of the lemma and GNC model, they even achieve higher \gls{werr} than the models trained on the full corpus.

\begin{table}[!htbp]
	\centering
	\caption[Maximal WERR achieved by respective models in \mbox{n-best} list rescoring]{Maximal \gls{werr} achieved by respective models in n-best list rescoring.}
	\label{table:max_werr}
	\begin{tabular*}{.4\linewidth}{@{\extracolsep{\fill}}lr}
		n-gram word full    & 15.67\\
		n-gram lemma speech  & 15.32\\
		neural word   & 15.28\\
		n-gram word speech  & 15.22\\
		n-gram word text    & 15.16\\
		n-gram lemma full    & 14.88\\
		n-gram lemma text    & 14.24\\
		neural lemma  & 14.10\\
		n-gram gnc speech & 13.75\\
		n-gram gnc full   & 13.54\\
		n-gram gnc text   & 13.29\\
		neural gnc    & 13.20\\
		n-gram pos text   & 9.89\\
		neural pos    & 9.58\\
		n-gram pos full   & 9.54\\
		n-gram pos speech & 9.47\\
	\end{tabular*}
\end{table}

\begin{figure}[!htbp]
	  \centering
	  \includegraphics[height=10cm, width=15cm]{word_werr.png}
	  \caption[Absolute WERR of the word trigram model]{Absolute \gls{werr} of the word trigram model.}
	      \label{figure:word}
\end{figure}

\begin{figure}[!htbp]
	  \centering
	  \includegraphics[height=10cm, width=15cm]{lemma_werr.png}
	  \caption[Absolute WERR of the lemma trigram model]{Absolute \gls{werr} of the lemma trigram model.}
	      \label{figure:lemma}
\end{figure}

\begin{figure}[!htbp]
	  \centering
	  \includegraphics[height=10cm, width=15cm]{pos_werr.png}
	  \caption[Absolute WERR of the POS trigram model]{Absolute \gls{werr} of the POS trigram model.}
	      \label{figure:pos}
\end{figure}

\begin{figure}[!htbp]
	  \centering
	  \includegraphics[height=10cm, width=15cm]{gnc_werr.png}
	  \caption[Absolute WERR of the GNC trigram model]{Absolute \gls{werr} of the GNC trigram model.}
	      \label{figure:gnc}
\end{figure}

\begin{figure}[!htbp]
	  \centering
	  \includegraphics[height=10cm, width=15cm]{ngram_full.png}
	  \caption[Absolute WERR of the trigram models]{Absolute \gls{werr} of the trigram models.}
	      \label{figure:neural}
\end{figure}

\begin{figure}[!htbp]
	  \centering
	  \includegraphics[height=10cm, width=15cm]{neural.png}
	  \caption[Absolute WERR of the neural models]{Absolute \gls{werr} of the neural models.}
	      \label{figure:neural}
\end{figure}

\FloatBarrier
\subsection{Mock n-best list rescoring}
The WERR results presented in Subsection \ref{section:nbest} were calculated on a real \gls{asr} system. However, the n-best lists were generated by an old version of Sarmata, when the performance of the acoustic model was rather poor, and they are therefore of limited value as a benchmark. As the recognition quality improves, it gets more important for a model to be able to correctly distinguish between very similar phrases. The generation of artificial \mbox{n-best} list is an implementation of the adversarial evaluation technique described in~\cite{smith2012adversarial}. A simple algorithm was used to create a set of ten hypotheses for each of the 5000 phrases randomly selected from the testing set used for perplexity evaluation. The hypotheses were generated by substituting random words with phonetically similar ones. Only words occuring more than 5000 times in the training data were used for substitution. Each word in the reference phrase had the same probability of being substituted, but that probability was different across the hypotheses. Insertion and deletion errors were neglected. Apart from simplicity, this decision was made to avoid the \mbox{n-gram} models' bias for shorter phrases and to simulate the operation of an advanced acoustic model. Because the orthography of Polish is highly phonemic, it was assumed that graphemic similarity is a good proxy of phonemic similarity. Therefore, the grapheme-level Levenshtein distance served as a heuristic for finding similar words. Inspecting the mock acoustic hypotheses showed that there is indeed a large degree of acoustic similarity between them. Table \ref{table:mock} presents an example phrase along with the automatically generated hypotheses.

\begin{table}[!htbp]
	\centering
	\caption[An example phrase and generated mock hypotheses]{An example phrase and generated mock hypotheses.}
	\label{table:mock}
	\texttt{%
	\begin{tabular*}{.6\linewidth}{@{\extracolsep{\fill}}l}
		coraz więcej polaków wyjeżdża odwiedza kraje europejskie\\
\\
		coraz więc polaków wyjeżdża odwiedza kraju europejskie\\
		wraz więcej polaków wyjeżdża odwiedza kraju europejskiej\\
		oraz więcej polaków wyjeżdża odwiedza kraje europejskiej\\
		raz więc polaków wyjeżdża zwiedza kraju europejskie\\
	\end{tabular*}}
\end{table}

Table \ref{table:wer_ngram} presents the mean WER of the hypotheses chosen by the trigram models. The lowest WER achieved is 8.55 for the full word model. Generally, the word-based model has the lowest error rate. Interestingly, the full GNC model not only significantly outperforms the \gls{pos} model, but also the lemma model, despite a much smaller vocabulary. Moreover, in case of the text-based corpus, the GNC model has a very similar error rate as the word-based model. This result proves that the morphosyntactic tags can be a viable alternative to words in case of highly inflected language. The models based on speech transcript outperform their text-based equivalents in all cases, but the difference is less pronounced in case of models with a smaller vocabulary. This may suggest that the dissimilarities between spoken and written language lie more in semantics than in syntax. 

\begin{table}[!htbp]
	\centering
	\caption[WER of trigram models]{WER of trigram models.}
	\label{table:wer_ngram}
	\begin{tabular*}{.6\linewidth}{@{\extracolsep{\fill}}l*3r}
		{}        & \multicolumn{1}{c}{text} & \multicolumn{1}{c}{speech} & \multicolumn{1}{c}{full} \\
		\midrule
		word   & 12.15  & 8.67  & 8.55\\
		lemma  & 16.95  & 11.81 & 13.34\\
		pos    & 20.96  & 19.38 & 19.54\\
		gnc    & 12.58  & 12.27 & 11.89\\
	\end{tabular*}
\end{table}

Table \ref{table:wer_neural} presents the mean \gls{wer} of the neural models. Regardless of the type of the modelling unit, they outperform the n-gram models trained on the same corpus. In particular, the neural GNC model achieves a lower WER than all n-gram models except for the speech and full word-based trigram model. This result suggests that \gls{rnn} morphosyntactic models can be a promising area of research as far as speech recognition is concerned, as they combine the compactness of class-based models and the ability to capture relationship spanning across multiple words, which is especially important in case of languages with a large degree of inflection.

\begin{table}[!htbp]
	\centering
	\caption[WER of neural models]{WER of neural models.}
	\label{table:wer_neural}
	\begin{tabular*}{.4\linewidth}{@{\extracolsep{\fill}}lr}
		word  & 8.21\\
		lemma  & 12.55\\
		pos    & 17.65\\
		gnc    & 11.61\\
	\end{tabular*}
\end{table}

Table \ref{table:max_werr_mock} presents the results of the mock n-best list rescoring. The neural network models outperform their \mbox{$n$-gram} equivalents for every modelling unit. Interestingly, the neural GNC model achieves a better WERR than the word text trigram and all the lemma models, despite a much smaller vocabulary.
\begin{table}[!htbp]
	\centering
	\caption[WERR achieved by respective models in mock \mbox{n-best} list rescoring]{WERR achieved by respective models in mock \mbox{n-best} list rescoring.}
	\label{table:max_werr_mock}
	\begin{tabular*}{.4\linewidth}{@{\extracolsep{\fill}}lr}
		neural word   & 21.43\\
		n-gram word full  & 21.09\\
		n-gram word speech  & 20.97\\
		neural gnc    & 18.03\\
		n-gram lemma speech  & 17.83\\
		n-gram gnc full  & 17.75\\
		n-gram word text  & 17.49\\
		n-gram gnc speech  & 17.37\\
		neural lemma  & 17.09\\
		n-gram gnc text  & 17.06\\
		n-gram lemma full  & 16.30\\
		n-gram lemma text  & 12.69\\
		neural pos    & 11.99\\
		n-gram pos speech  & 10.26\\
		n-gram pos full  & 10.10\\
		n-gram pos text  & 8.68\\
	\end{tabular*}
\end{table}
